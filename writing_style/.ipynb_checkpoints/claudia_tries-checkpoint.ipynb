{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97e6fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from collections import Counter\n",
    "# import httplib2\n",
    "import itertools\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd \n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import urllib.request\n",
    "from urllib.request import urlopen, Request\n",
    "import random\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc45d509-90f8-4db6-a063-13e17fad1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70f63a",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2790af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_json('../data/reviewSelected100.json', encoding='ISO-8859-1', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e12df",
   "metadata": {},
   "source": [
    "## 3.2 Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14711fd",
   "metadata": {},
   "source": [
    "### Tokenisation and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78f7f863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>18wW_ZjWCkmCpIGSYlym4A</td>\n",
       "      <td>B19N0vr_Ff7hb5AyPxW_Qw</td>\n",
       "      <td>Rii85bzYKGC9P0zOyAem6A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>While the waitress we had was great, the resta...</td>\n",
       "      <td>2017-06-06 21:50:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>CWVLFI0EblAcikZK2tCnvA</td>\n",
       "      <td>ypKZT3cHMUKv15JfKmxnkQ</td>\n",
       "      <td>Rii85bzYKGC9P0zOyAem6A</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simple chain steakhouse. Not a knock on their ...</td>\n",
       "      <td>2017-03-27 12:26:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>I-7XrxlGpagxjBn1AOK9Og</td>\n",
       "      <td>sFXfLbJjxVag-ZW1pzjpvg</td>\n",
       "      <td>Rii85bzYKGC9P0zOyAem6A</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Very good steaks! Place is always busy but the...</td>\n",
       "      <td>2015-08-19 18:19:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3155</th>\n",
       "      <td>WBTgTv6RD3ZZaKFUdan3pg</td>\n",
       "      <td>IgiigXHxvmpCcVwWA0kbHA</td>\n",
       "      <td>Rii85bzYKGC9P0zOyAem6A</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Was told 15 to 25 minutes for a table. Was sea...</td>\n",
       "      <td>2017-07-08 22:26:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>Tp-59xgWhoyPE1xe9iffgA</td>\n",
       "      <td>6tDfetjsLRNS_RzgN74Qkg</td>\n",
       "      <td>Rii85bzYKGC9P0zOyAem6A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This was an above average restaurant experienc...</td>\n",
       "      <td>2017-09-09 22:36:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   review_id                 user_id             business_id  \\\n",
       "3120  18wW_ZjWCkmCpIGSYlym4A  B19N0vr_Ff7hb5AyPxW_Qw  Rii85bzYKGC9P0zOyAem6A   \n",
       "3129  CWVLFI0EblAcikZK2tCnvA  ypKZT3cHMUKv15JfKmxnkQ  Rii85bzYKGC9P0zOyAem6A   \n",
       "3133  I-7XrxlGpagxjBn1AOK9Og  sFXfLbJjxVag-ZW1pzjpvg  Rii85bzYKGC9P0zOyAem6A   \n",
       "3155  WBTgTv6RD3ZZaKFUdan3pg  IgiigXHxvmpCcVwWA0kbHA  Rii85bzYKGC9P0zOyAem6A   \n",
       "3163  Tp-59xgWhoyPE1xe9iffgA  6tDfetjsLRNS_RzgN74Qkg  Rii85bzYKGC9P0zOyAem6A   \n",
       "\n",
       "      stars  useful  funny  cool  \\\n",
       "3120      2       0      0     0   \n",
       "3129      4       1      1     1   \n",
       "3133      4       0      0     0   \n",
       "3155      5       0      0     0   \n",
       "3163      3       0      0     0   \n",
       "\n",
       "                                                   text                date  \n",
       "3120  While the waitress we had was great, the resta... 2017-06-06 21:50:57  \n",
       "3129  Simple chain steakhouse. Not a knock on their ... 2017-03-27 12:26:18  \n",
       "3133  Very good steaks! Place is always busy but the... 2015-08-19 18:19:12  \n",
       "3155  Was told 15 to 25 minutes for a table. Was sea... 2017-07-08 22:26:28  \n",
       "3163  This was an above average restaurant experienc... 2017-09-09 22:36:53  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get reviews for a random business \n",
    "random_business = reviews.sample()\n",
    "random_business_id = random_business.iloc[0]['business_id']\n",
    "small_business_dataset = reviews.loc[reviews['business_id'] == random_business_id]\n",
    "small_business_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c91d93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_business_dataset_reviews = list(small_business_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0669b8f6-714b-4708-840b-2c65c03f97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the reviews into a concatenated string \n",
    "b1_review = ''.join(small_business_dataset_reviews)\n",
    "clean_review = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", b1_review)\n",
    "b1_review = nlp(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "deaa0847-5dec-4317-b9f5-ece17562bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 420), ('and', 394), ('a', 287), ('I', 249), ('to', 244), ('was', 231), ('for', 144), ('of', 134), ('is', 115), ('we', 114)]\n"
     ]
    }
   ],
   "source": [
    "# removed punctuation and get the top 10 most common words (including stopwords)\n",
    "b1_review_words = [token.text for token in b1_review if token.is_alpha == True] \n",
    "b1_word_freq = Counter(b1_review_words)\n",
    "common_words = b1_word_freq.most_common(10)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ede74a1b-4e6c-4436-8c3b-df2f479aad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nt', 75), ('steak', 72), ('food', 66), ('great', 61), ('good', 60), ('service', 48), ('place', 44), ('time', 38), ('got', 37), ('Texas', 35)]\n"
     ]
    }
   ],
   "source": [
    "# removed punctuation and get the top 10 most common words (excluding stopwords)\n",
    "b1_review_words = [token.text for token in b1_review if token.is_stop != True and token.is_alpha == True] \n",
    "b1_word_freq = Counter(b1_review_words)\n",
    "common_words = b1_word_freq.most_common(10)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82e4306f-2536-450b-ba7d-cc0f073dfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot log graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eeb35a0e-7b64-4e9a-9d53-e89a299e51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do some stemming after removing the stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "porter_st = PorterStemmer()\n",
    "lancaster_st = LancasterStemmer()\n",
    "snow_st = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfe90886-30ac-4282-978f-f66739952d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('steak', 105), ('nt', 75), ('great', 71), ('food', 70), ('order', 62), ('good', 62), ('servic', 56), ('place', 50), ('time', 47), ('seat', 44)]\n"
     ]
    }
   ],
   "source": [
    "# Using Porter Stemmer\n",
    "porter_stemmed_words = [porter_st.stem(word) for word in b1_review_words]\n",
    "porter_freq = Counter(porter_stemmed_words)\n",
    "porter_common = porter_freq.most_common(10)\n",
    "print(porter_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "993edaec-02a4-4bef-9a21-c61302974a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('steak', 105), ('serv', 97), ('nt', 75), ('gre', 71), ('food', 71), ('ord', 62), ('good', 62), ('plac', 51), ('wait', 50), ('tim', 47)]\n"
     ]
    }
   ],
   "source": [
    "# Using Lancaster Stemmer\n",
    "lancaster_stemmed_words = [lancaster_st.stem(word) for word in b1_review_words]\n",
    "lancaster_freq = Counter(lancaster_stemmed_words)\n",
    "lancaster_common = lancaster_freq.most_common(10)\n",
    "print(lancaster_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32ce69f4-074a-4ab0-ae2d-d6e466205781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('steak', 105), ('nt', 75), ('great', 71), ('food', 70), ('order', 62), ('good', 62), ('servic', 56), ('place', 50), ('time', 47), ('seat', 44)]\n"
     ]
    }
   ],
   "source": [
    "# Using Snowball Stemmer\n",
    "snow_stemmed_words = [snow_st.stem(word) for word in b1_review_words]\n",
    "snow_freq = Counter(snow_stemmed_words)\n",
    "snow_common = snow_freq.most_common(10)\n",
    "print(snow_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d6c54",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c24448",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sentences = reviews.sample(5, random_state=42)\n",
    "random_sentences = list(random_sentences['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c81cc594-a486-47fa-91bf-0f4043a6cd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Que ce soit pour leurs délicieux bubbles tea/smooties, leurs ''Bánh mì'' , leurs petits snacks (viennoiseries, tapioca, ...), on adore Vua et aussi leurs prix très abordables. On y retourne lorsqu'on est dans le Quartier Latin !\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8155e95-94cb-490a-bbff-2db99e5b7cf0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Que', 'NNP'),\n",
       "  ('ce', 'NN'),\n",
       "  ('soit', 'VBD'),\n",
       "  ('pour', 'JJ'),\n",
       "  ('leurs', 'NNS'),\n",
       "  ('délicieux', 'VBP'),\n",
       "  ('bubbles', 'NNS'),\n",
       "  ('tea/smooties', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('leurs', 'VBZ'),\n",
       "  ('``', '``'),\n",
       "  ('Bánh', 'NNP'),\n",
       "  ('mì', 'NN'),\n",
       "  (\"''\", \"''\"),\n",
       "  (',', ','),\n",
       "  ('leurs', 'VBZ'),\n",
       "  ('petits', 'NNS'),\n",
       "  ('snacks', 'NNS'),\n",
       "  ('(', '('),\n",
       "  ('viennoiseries', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('tapioca', 'NN'),\n",
       "  (',', ','),\n",
       "  ('...', ':'),\n",
       "  (')', ')'),\n",
       "  (',', ','),\n",
       "  ('on', 'IN'),\n",
       "  ('adore', 'IN'),\n",
       "  ('Vua', 'NNP'),\n",
       "  ('et', 'CC'),\n",
       "  ('aussi', 'JJ'),\n",
       "  ('leurs', 'NNS'),\n",
       "  ('prix', 'VBP'),\n",
       "  ('très', 'JJ'),\n",
       "  ('abordables', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('On', 'IN'),\n",
       "  ('y', 'JJ'),\n",
       "  ('retourne', 'JJ'),\n",
       "  (\"lorsqu'on\", 'NN'),\n",
       "  ('est', 'JJS'),\n",
       "  ('dans', 'NNS'),\n",
       "  ('le', 'VBP'),\n",
       "  ('Quartier', 'NNP'),\n",
       "  ('Latin', 'NNP'),\n",
       "  ('!', '.')],\n",
       " [('As', 'IN'),\n",
       "  ('I', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('said', 'VBD'),\n",
       "  ('previously', 'RB'),\n",
       "  ('...', ':'),\n",
       "  ('we', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('been', 'VBN'),\n",
       "  ('coming', 'VBG'),\n",
       "  ('to', 'TO'),\n",
       "  ('LMAH', 'NNP'),\n",
       "  ('for', 'IN'),\n",
       "  ('over', 'IN'),\n",
       "  ('10', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('At', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('15', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('And', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('ALWAYS', 'NNP'),\n",
       "  ('seen', 'VBN'),\n",
       "  ('Dr.', 'NNP'),\n",
       "  ('White', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('bring', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('to', 'TO'),\n",
       "  ('have', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('urine', 'JJ'),\n",
       "  ('sample', 'NN'),\n",
       "  ('done', 'VBN'),\n",
       "  ('.', '.'),\n",
       "  ('While', 'IN'),\n",
       "  ('being', 'VBG'),\n",
       "  ('done', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('pee', 'NN'),\n",
       "  (\"'d\", 'MD'),\n",
       "  ('on', 'IN'),\n",
       "  ('himself', 'PRP'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('vet', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('staff', 'NN'),\n",
       "  ('only', 'RB'),\n",
       "  ('tried', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('clean', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('off', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('him', 'PRP'),\n",
       "  ('with', 'IN'),\n",
       "  ('alcohol', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('try', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('actually', 'RB'),\n",
       "  ('get', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('off', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('him', 'PRP'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('TELL', 'VB'),\n",
       "  ('ME', 'NNP'),\n",
       "  ('that', 'IN'),\n",
       "  ('he', 'PRP'),\n",
       "  ('pee', 'VBZ'),\n",
       "  (\"'d\", 'MD'),\n",
       "  ('all', 'DT'),\n",
       "  ('over', 'IN'),\n",
       "  ('his', 'PRP$'),\n",
       "  ('stomach', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('legs', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('So', 'RB'),\n",
       "  ('now', 'RB'),\n",
       "  ('I', 'PRP'),\n",
       "  ('e', 'VBP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('give', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('a', 'DT'),\n",
       "  ('bath', 'NN'),\n",
       "  ('because', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('ca', 'MD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('take', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('urine', 'JJ'),\n",
       "  ('sample', 'NN'),\n",
       "  ('properly', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('LIVID', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('Pretty', 'NNP'),\n",
       "  ('decent', 'NN'),\n",
       "  ('but', 'CC'),\n",
       "  ('so', 'RB'),\n",
       "  ('spacious', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Very', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('for', 'IN'),\n",
       "  ('kids', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('order', 'NN'),\n",
       "  ('dim', 'VB'),\n",
       "  ('sum', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  (\"'re\", 'VBP'),\n",
       "  ('open', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('Free', 'JJ'),\n",
       "  ('wifi', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('awesome', 'JJ'),\n",
       "  (':', ':'),\n",
       "  (')', ')')],\n",
       " [('Dumpling', 'VBG'),\n",
       "  ('Village', 'NNP'),\n",
       "  ('really', 'RB'),\n",
       "  ('suffering', 'VBG'),\n",
       "  ('from', 'IN'),\n",
       "  ('identity', 'NN'),\n",
       "  ('crisis', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('Is', 'VBZ'),\n",
       "  ('it', 'PRP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('Restaurant', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('mostly', 'RB'),\n",
       "  ('Northern', 'NNP'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('items', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('menu', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('Is', 'VBZ'),\n",
       "  ('it', 'PRP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Northern', 'NNP'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('restaurant', 'NN'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('dishes', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('menu', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('pork', 'NN'),\n",
       "  ('bone', 'NN'),\n",
       "  ('soup', 'NN'),\n",
       "  (',', ','),\n",
       "  ('cold', 'JJ'),\n",
       "  ('wheat', 'NN'),\n",
       "  ('bucknoodle', 'NN'),\n",
       "  ('soup', 'NN'),\n",
       "  (',', ','),\n",
       "  ('kimchi', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bean', 'NN'),\n",
       "  ('sprout', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('appetitizers', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('only', 'JJ'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('characters', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('3', 'CD'),\n",
       "  ('on', 'IN'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('business', 'NN'),\n",
       "  ('sign', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('front', 'NN'),\n",
       "  ('..', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('rest', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('in', 'IN'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('English', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('All', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('wait', 'NN'),\n",
       "  ('staffs', 'NNS'),\n",
       "  ('here', 'RB'),\n",
       "  ('speak', 'VBP'),\n",
       "  ('Mandarian', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('even', 'RB'),\n",
       "  ('most', 'JJS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('patrons', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('Chinese', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('It', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('located', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('really', 'RB'),\n",
       "  ('tiny', 'JJ'),\n",
       "  ('strip', 'NN'),\n",
       "  ('mall', 'NN'),\n",
       "  ('..', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('limited', 'JJ'),\n",
       "  ('parkings', 'NNS'),\n",
       "  ('!', '.'),\n",
       "  ('To', 'TO'),\n",
       "  ('tell', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('the', 'DT'),\n",
       "  ('truth', 'NN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('area', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('over', 'IN'),\n",
       "  ('10', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('I', 'PRP'),\n",
       "  ('never', 'RB'),\n",
       "  ('notice', 'VBP'),\n",
       "  ('it', 'PRP'),\n",
       "  ('until', 'IN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('read', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('reviews', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('yelp', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('around', 'RB'),\n",
       "  ('6', 'CD'),\n",
       "  ('pm', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('Saturday', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('restaurant', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('almost', 'RB'),\n",
       "  ('full', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('tables', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('packed', 'VBN'),\n",
       "  ('together', 'RB'),\n",
       "  (',', ','),\n",
       "  ('so', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('still', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('certain', 'JJ'),\n",
       "  ('degree', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('privacy', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('tell', 'VB'),\n",
       "  ('what', 'WP'),\n",
       "  ('the', 'DT'),\n",
       "  ('table', 'NN'),\n",
       "  ('next', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('me', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('eating', 'VBG'),\n",
       "  ('.', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('decided', 'VBD'),\n",
       "  ('on', 'IN'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('favourite', 'JJ'),\n",
       "  ('chives', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('pork', 'NN'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('kimchi', 'NNS'),\n",
       "  ('fried', 'VBD'),\n",
       "  ('rice', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('Korean', 'NNP'),\n",
       "  ('cold', 'VBD'),\n",
       "  ('bucknoodle', 'JJ'),\n",
       "  ('soup', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('$', '$'),\n",
       "  ('4.99', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('noodle', 'JJ'),\n",
       "  ('soup', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('so', 'RB'),\n",
       "  ('so', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Nice', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('refreshing', 'VBG'),\n",
       "  ('!', '.'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('based', 'VBN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('sour', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('really', 'RB'),\n",
       "  ('stimulate', 'VB'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('tastebuds', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('just', 'RB'),\n",
       "  ('make', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('want', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('eat', 'VB'),\n",
       "  ('more', 'JJR'),\n",
       "  ('!', '.'),\n",
       "  ('(', '('),\n",
       "  ('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('finish', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('myself', 'PRP'),\n",
       "  ('!', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('want', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('share', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('want', 'VBP'),\n",
       "  ('one', 'CD'),\n",
       "  ('all', 'DT'),\n",
       "  ('for', 'IN'),\n",
       "  ('myself', 'PRP'),\n",
       "  ('next', 'JJ'),\n",
       "  ('time', 'NN'),\n",
       "  ('!', '.'),\n",
       "  (')', ')'),\n",
       "  ('The', 'DT'),\n",
       "  ('kimchi', 'NN'),\n",
       "  ('fried', 'VBD'),\n",
       "  ('rice', 'NN'),\n",
       "  ('..', 'NNP'),\n",
       "  ('hmmmmm', 'NN'),\n",
       "  ('not', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('A', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('oily', 'JJ'),\n",
       "  ('side', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('much', 'JJ'),\n",
       "  ('taste', 'NN'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('spicy', 'VB'),\n",
       "  ('enough', 'RB'),\n",
       "  ('?', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('could', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('finish', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('and', 'CC'),\n",
       "  ('end', 'VB'),\n",
       "  ('up', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('pack', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('to', 'TO'),\n",
       "  ('go', 'VB'),\n",
       "  ('...', ':'),\n",
       "  ('(', '('),\n",
       "  ('not', 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('sure', 'JJ'),\n",
       "  ('I', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('eat', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  (',', ','),\n",
       "  ('but', 'CC'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('just', 'RB'),\n",
       "  ('let', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('go', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('waste', 'NN'),\n",
       "  (')', ')'),\n",
       "  ('The', 'DT'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('last', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('really', 'RB'),\n",
       "  ('worth', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('wait', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('14', 'CD'),\n",
       "  ('big', 'JJ'),\n",
       "  ('fat', 'NN'),\n",
       "  ('hot', 'JJ'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('filled', 'VBN'),\n",
       "  ('up', 'RP'),\n",
       "  ('the', 'DT'),\n",
       "  ('whole', 'JJ'),\n",
       "  ('steamer', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('Yes', 'UH'),\n",
       "  ('I', 'PRP'),\n",
       "  ('counted', 'VBD'),\n",
       "  ('them', 'PRP'),\n",
       "  ('14', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('them', 'PRP'),\n",
       "  ('for', 'IN'),\n",
       "  ('$', '$'),\n",
       "  ('4.99', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('freshly', 'RB'),\n",
       "  ('made', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('freshly', 'RB'),\n",
       "  ('steamed', 'VBN'),\n",
       "  ('!', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('skin', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('nice', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('thin', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('chewy', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('thick', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Oh', 'NNP'),\n",
       "  ('so', 'RB'),\n",
       "  ('delicious', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('One', 'CD'),\n",
       "  ('bite', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('ozzing', 'VBG'),\n",
       "  ('out', 'RP'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('if', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('burning', 'VBG'),\n",
       "  ('hot', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('just', 'RB'),\n",
       "  ('stuff', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('whole', 'JJ'),\n",
       "  ('thing', 'NN'),\n",
       "  ('into', 'IN'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('mouth', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('let', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('favour', 'NN'),\n",
       "  ('just', 'RB'),\n",
       "  ('explosed', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('mouth', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('Just', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('warning', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('If', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('going', 'VBG'),\n",
       "  ('out', 'RP'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('date', 'NN'),\n",
       "  (',', ','),\n",
       "  ('please', 'VB'),\n",
       "  ('do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('come', 'VB'),\n",
       "  ('here', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('Even', 'RB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('good', 'JJ'),\n",
       "  ('place', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('cheap', 'JJ'),\n",
       "  ('date', 'NN'),\n",
       "  (',', ','),\n",
       "  ('our', 'PRP$'),\n",
       "  ('bill', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('$', '$'),\n",
       "  ('18', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('2', 'CD'),\n",
       "  ('and', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('full', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('place', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('noisy', 'RB'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('food', 'NN'),\n",
       "  ('too', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('date', 'NN'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('too', 'RB'),\n",
       "  ('busy', 'JJ'),\n",
       "  ('eating', 'VBG'),\n",
       "  ('and', 'CC'),\n",
       "  ('just', 'RB'),\n",
       "  ('ignore', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('!', '.'),\n",
       "  ('Or', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('keep', 'VB'),\n",
       "  ('eating', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('eating', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('freak', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('friends', 'NNS'),\n",
       "  ('out', 'RP'),\n",
       "  ('!', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('services', 'NNS'),\n",
       "  ('here', 'RB'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('but', 'CC'),\n",
       "  ('efficient', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('given', 'VBN'),\n",
       "  ('extra', 'JJ'),\n",
       "  ('napkins', 'NNS'),\n",
       "  ('without', 'IN'),\n",
       "  ('requesting', 'VBG'),\n",
       "  ('...', ':'),\n",
       "  ('Maybe', 'RB'),\n",
       "  ('we', 'PRP'),\n",
       "  ('looked', 'VBD'),\n",
       "  ('really', 'RB'),\n",
       "  ('messy', 'VBN'),\n",
       "  ('?', '.'),\n",
       "  ('?', '.'),\n",
       "  ('Cash', 'NNP'),\n",
       "  ('only', 'RB'),\n",
       "  ('!', '.')],\n",
       " [('best', 'JJS'),\n",
       "  ('pizza', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('south', 'JJ'),\n",
       "  ('side', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('huge', 'JJ'),\n",
       "  ('drink', 'NN'),\n",
       "  ('selection', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('awesome', 'VB'),\n",
       "  ('atmosphere', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('$', '$'),\n",
       "  ('4', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('slice', 'NN'),\n",
       "  ('that', 'WDT'),\n",
       "  ('could', 'MD'),\n",
       "  ('easily', 'RB'),\n",
       "  ('feed', 'VB'),\n",
       "  ('2', 'CD'),\n",
       "  ('people', 'NNS'),\n",
       "  ('!', '.')]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tagged = []\n",
    "for sentence in random_sentences:\n",
    "    nltk_tagged.append((nltk.pos_tag(word_tokenize(sentence))))\n",
    "nltk_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d874c6c-1f0f-4232-b819-693f49a971a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que      PROPN , NNP\n",
      "ce       PROPN , NNP\n",
      "soit     ADJ   , JJ\n",
      "pour     NOUN  , NN\n",
      "leurs    VERB  , VBZ\n",
      "délicieux NOUN  , NN\n",
      "bubbles  NOUN  , NNS\n",
      "tea      NOUN  , NN\n",
      "/        SYM   , SYM\n",
      "smooties NOUN  , NNS\n",
      ",        PUNCT , ,\n",
      "leurs    VERB  , VBZ\n",
      "''       PUNCT , ''\n",
      "Bánh     PROPN , NNP\n",
      "mì       INTJ  , UH\n",
      "''       PUNCT , ''\n",
      ",        PUNCT , ,\n",
      "leurs    NOUN  , NNS\n",
      "petits   VERB  , VBZ\n",
      "snacks   NOUN  , NNS\n",
      "(        PUNCT , -LRB-\n",
      "viennoiseries NOUN  , NNS\n",
      ",        PUNCT , ,\n",
      "tapioca  INTJ  , UH\n",
      ",        PUNCT , ,\n",
      "...      PUNCT , NFP\n",
      ")        PUNCT , -RRB-\n",
      ",        PUNCT , ,\n",
      "on       ADP   , IN\n",
      "adore    PROPN , NNP\n",
      "Vua      PROPN , NNP\n",
      "et       PROPN , NNP\n",
      "aussi    PROPN , NNP\n",
      "leurs    VERB  , VBZ\n",
      "prix     NOUN  , NN\n",
      "très     ADJ   , JJ\n",
      "abordables NOUN  , NNS\n",
      ".        PUNCT , .\n",
      "On       ADP   , IN\n",
      "y        PROPN , NNP\n",
      "retourne VERB  , VBN\n",
      "lorsqu'on PROPN , NNP\n",
      "est      PROPN , NNP\n",
      "dans     PROPN , NNPS\n",
      "le       X     , FW\n",
      "Quartier PROPN , NNP\n",
      "Latin    PROPN , NNP\n",
      "!        PUNCT , .\n",
      "As       ADP   , IN\n",
      "I        PRON  , PRP\n",
      "'ve      AUX   , VBP\n",
      "said     VERB  , VBN\n",
      "previously ADV   , RB\n",
      "...      PUNCT , .\n",
      "we've    PROPN , NNP\n",
      "been     AUX   , VBN\n",
      "coming   VERB  , VBG\n",
      "to       ADP   , IN\n",
      "LMAH     PROPN , NNP\n",
      "for      ADP   , IN\n",
      "over     ADP   , IN\n",
      "10       NUM   , CD\n",
      "years    NOUN  , NNS\n",
      ".        PUNCT , .\n",
      "At       ADV   , RB\n",
      "least    ADJ   , JJS\n",
      "15       NUM   , CD\n",
      ".        PUNCT , .\n",
      "And      CCONJ , CC\n",
      "we       PRON  , PRP\n",
      "'ve      AUX   , VBP\n",
      "ALWAYS   ADV   , RB\n",
      "seen     VERB  , VBN\n",
      "Dr.      PROPN , NNP\n",
      "White    PROPN , NNP\n",
      ".        PUNCT , .\n",
      "\n",
      "        SPACE , _SP\n",
      "I        PRON  , PRP\n",
      "had      VERB  , VBD\n",
      "to       PART  , TO\n",
      "bring    VERB  , VB\n",
      "my       PRON  , PRP$\n",
      "cat      NOUN  , NN\n",
      "in       ADP   , RP\n",
      "to       PART  , TO\n",
      "have     VERB  , VB\n",
      "a        DET   , DT\n",
      "urine    NOUN  , NN\n",
      "sample   NOUN  , NN\n",
      "done     VERB  , VBN\n",
      ".        PUNCT , .\n",
      "While    SCONJ , IN\n",
      "being    AUX   , VBG\n",
      "done     VERB  , VBN\n",
      ",        PUNCT , ,\n",
      "my       PRON  , PRP$\n",
      "cat      NOUN  , NN\n",
      "pee'd    ADV   , RB\n",
      "on       ADP   , IN\n",
      "himself  PRON  , PRP\n",
      ".        PUNCT , .\n",
      "The      DET   , DT\n",
      "vet      NOUN  , NN\n",
      "or       CCONJ , CC\n",
      "staff    NOUN  , NN\n",
      "only     ADV   , RB\n",
      "tried    VERB  , VBD\n",
      "to       PART  , TO\n",
      "clean    VERB  , VB\n",
      "it       PRON  , PRP\n",
      "off      ADP   , IN\n",
      "of       ADP   , IN\n",
      "him      PRON  , PRP\n",
      "with     ADP   , IN\n",
      "alcohol  NOUN  , NN\n",
      ".        PUNCT , .\n",
      "They     PRON  , PRP\n",
      "did      AUX   , VBD\n",
      "n't      PART  , RB\n",
      "try      VERB  , VB\n",
      "to       PART  , TO\n",
      "actually ADV   , RB\n",
      "get      VERB  , VB\n",
      "it       PRON  , PRP\n",
      "off      ADP   , IN\n",
      "of       ADP   , IN\n",
      "him      PRON  , PRP\n",
      ".        PUNCT , .\n",
      "They     PRON  , PRP\n",
      "did      AUX   , VBD\n",
      "n't      PART  , RB\n",
      "even     ADV   , RB\n",
      "TELL     VERB  , VB\n",
      "ME       PRON  , PRP\n",
      "that     SCONJ , IN\n",
      "he       PRON  , PRP\n",
      "pee'd    VERB  , VBZ\n",
      "all      ADV   , RB\n",
      "over     ADP   , IN\n",
      "his      PRON  , PRP$\n",
      "stomach  NOUN  , NN\n",
      "and      CCONJ , CC\n",
      "legs     NOUN  , NNS\n",
      ".        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "So       ADV   , RB\n",
      "now      ADV   , RB\n",
      "I        PRON  , PRP\n",
      "e        VERB  , VBP\n",
      "had      VERB  , VBD\n",
      "to       PART  , TO\n",
      "give     VERB  , VB\n",
      "my       PRON  , PRP$\n",
      "cat      NOUN  , NN\n",
      "a        DET   , DT\n",
      "bath     NOUN  , NN\n",
      "because  SCONJ , IN\n",
      "they     PRON  , PRP\n",
      "ca       AUX   , MD\n",
      "n't      PART  , RB\n",
      "take     VERB  , VB\n",
      "a        DET   , DT\n",
      "urine    NOUN  , NN\n",
      "sample   NOUN  , NN\n",
      "properly ADV   , RB\n",
      ".        PUNCT , .\n",
      "I        PRON  , PRP\n",
      "am       AUX   , VBP\n",
      "LIVID    PROPN , NNP\n",
      ".        PUNCT , .\n",
      "Pretty   ADV   , RB\n",
      "decent   ADJ   , JJ\n",
      "but      CCONJ , CC\n",
      "so       ADV   , RB\n",
      "spacious ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "         SPACE , _SP\n",
      "Very     ADV   , RB\n",
      "good     ADJ   , JJ\n",
      "for      ADP   , IN\n",
      "kids     NOUN  , NNS\n",
      "and      CCONJ , CC\n",
      "you      PRON  , PRP\n",
      "can      AUX   , MD\n",
      "order    VERB  , VB\n",
      "dim      NOUN  , NN\n",
      "sum      NOUN  , NN\n",
      "!        PUNCT , .\n",
      "They     PRON  , PRP\n",
      "'re      VERB  , VBP\n",
      "open     ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "!        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "Free     ADJ   , JJ\n",
      "wifi     NOUN  , NN\n",
      "and      CCONJ , CC\n",
      "awesome  ADJ   , JJ\n",
      ":)       PUNCT , .\n",
      "Dumpling PROPN , NNP\n",
      "Village  PROPN , NNP\n",
      "really   ADV   , RB\n",
      "suffering VERB  , VBG\n",
      "from     ADP   , IN\n",
      "identity NOUN  , NN\n",
      "crisis   NOUN  , NN\n",
      "!        PUNCT , .\n",
      "!        PUNCT , .\n",
      "Is       AUX   , VBZ\n",
      "it       PRON  , PRP\n",
      "a        DET   , DT\n",
      "Korean   PROPN , NNP\n",
      "Restaurant PROPN , NNP\n",
      ",        PUNCT , ,\n",
      "not      PART  , RB\n",
      "really   ADV   , RB\n",
      ".        PUNCT , .\n",
      "There    PRON  , EX\n",
      "are      AUX   , VBP\n",
      "mostly   ADV   , RB\n",
      "Northern ADJ   , JJ\n",
      "Chinese  ADJ   , JJ\n",
      "items    NOUN  , NNS\n",
      "on       ADP   , IN\n",
      "the      DET   , DT\n",
      "menu     NOUN  , NN\n",
      "like     ADP   , IN\n",
      "dumplings NOUN  , NNS\n",
      ".        PUNCT , .\n",
      "Is       AUX   , VBZ\n",
      "it       PRON  , PRP\n",
      "a        DET   , DT\n",
      "Northern ADJ   , JJ\n",
      "Chinese  ADJ   , JJ\n",
      "restaurant NOUN  , NN\n",
      ",        PUNCT , ,\n",
      "not      PART  , RB\n",
      "really   ADV   , RB\n",
      "There    PRON  , EX\n",
      "are      AUX   , VBP\n",
      "a        DET   , DT\n",
      "lot      NOUN  , NN\n",
      "of       ADP   , IN\n",
      "Korean   ADJ   , JJ\n",
      "dishes   NOUN  , NNS\n",
      "         SPACE , _SP\n",
      "in       ADP   , IN\n",
      "the      DET   , DT\n",
      "menu     NOUN  , NN\n",
      "like     ADP   , IN\n",
      "pork     NOUN  , NN\n",
      "bone     NOUN  , NN\n",
      "soup     NOUN  , NN\n",
      ",        PUNCT , ,\n",
      "cold     ADJ   , JJ\n",
      "wheat    NOUN  , NN\n",
      "bucknoodle NOUN  , NN\n",
      "soup     NOUN  , NN\n",
      ",        PUNCT , ,\n",
      "kimchi   PROPN , NNP\n",
      "and      CCONJ , CC\n",
      "bean     NOUN  , NN\n",
      "sprout   NOUN  , NN\n",
      "as       ADP   , IN\n",
      "appetitizers NOUN  , NNS\n",
      ".        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "The      DET   , DT\n",
      "only     ADJ   , JJ\n",
      "Korean   ADJ   , JJ\n",
      "characters NOUN  , NNS\n",
      "are      AUX   , VBP\n",
      "the      DET   , DT\n",
      "3        NUM   , CD\n",
      "on       ADP   , IN\n",
      "their    PRON  , PRP$\n",
      "business NOUN  , NN\n",
      "sign     NOUN  , NN\n",
      "in       ADP   , IN\n",
      "the      DET   , DT\n",
      "front    NOUN  , NN\n",
      "..       PUNCT , NFP\n",
      "and      CCONJ , CC\n",
      "all      DET   , PDT\n",
      "the      DET   , DT\n",
      "rest     NOUN  , NN\n",
      "are      VERB  , VBP\n",
      "in       ADP   , IN\n",
      "Chinese  PROPN , NNP\n",
      "and      CCONJ , CC\n",
      "English  PROPN , NNP\n",
      ".        PUNCT , .\n",
      "All      DET   , PDT\n",
      "the      DET   , DT\n",
      "wait     ADJ   , JJ\n",
      "staffs   NOUN  , NNS\n",
      "here     ADV   , RB\n",
      "speak    VERB  , VBP\n",
      "Mandarian PROPN , NNP\n",
      "and      CCONJ , CC\n",
      "even     ADV   , RB\n",
      "most     ADJ   , JJS\n",
      "of       ADP   , IN\n",
      "the      DET   , DT\n",
      "patrons  NOUN  , NNS\n",
      "are      VERB  , VBP\n",
      "Chinese  ADJ   , JJ\n",
      ".        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "It       PRON  , PRP\n",
      "is       AUX   , VBZ\n",
      "located  VERB  , VBN\n",
      "in       ADP   , IN\n",
      "a        DET   , DT\n",
      "really   ADV   , RB\n",
      "tiny     ADJ   , JJ\n",
      "strip    NOUN  , NN\n",
      "mall     NOUN  , NN\n",
      "..       PUNCT , NFP\n",
      "with     ADP   , IN\n",
      "limited  ADJ   , JJ\n",
      "parkings NOUN  , NNS\n",
      "!        PUNCT , .\n",
      "To       PART  , TO\n",
      "tell     VERB  , VB\n",
      "you      PRON  , PRP\n",
      "the      DET   , DT\n",
      "truth    NOUN  , NN\n",
      "I        PRON  , PRP\n",
      "am       VERB  , VBP\n",
      "in       ADP   , IN\n",
      "the      DET   , DT\n",
      "area     NOUN  , NN\n",
      "for      ADP   , IN\n",
      "over     ADP   , IN\n",
      "10       NUM   , CD\n",
      "years    NOUN  , NNS\n",
      "I        PRON  , PRP\n",
      "never    ADV   , RB\n",
      "notice   VERB  , VBP\n",
      "it       PRON  , PRP\n",
      "until    ADP   , IN\n",
      "I        PRON  , PRP\n",
      "read     VERB  , VBP\n",
      "the      DET   , DT\n",
      "reviews  NOUN  , NNS\n",
      "on       ADP   , IN\n",
      "yelp     NOUN  , NN\n",
      "!        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "We       PRON  , PRP\n",
      "arrived  VERB  , VBD\n",
      "around   ADV   , RB\n",
      "6        NUM   , CD\n",
      "pm       NOUN  , NN\n",
      "on       ADP   , IN\n",
      "a        DET   , DT\n",
      "Saturday PROPN , NNP\n",
      ".        PUNCT , .\n",
      "The      DET   , DT\n",
      "restaurant NOUN  , NN\n",
      "was      AUX   , VBD\n",
      "almost   ADV   , RB\n",
      "full     ADJ   , JJ\n",
      ".        PUNCT , .\n",
      "         SPACE , _SP\n",
      "The      DET   , DT\n",
      "tables   NOUN  , NNS\n",
      "are      AUX   , VBP\n",
      "not      PART  , RB\n",
      "really   ADV   , RB\n",
      "packed   VERB  , VBN\n",
      "together ADV   , RB\n",
      ",        PUNCT , ,\n",
      "so       ADV   , RB\n",
      "you      PRON  , PRP\n",
      "still    ADV   , RB\n",
      "have     VERB  , VBP\n",
      "a        DET   , DT\n",
      "certain  ADJ   , JJ\n",
      "degree   NOUN  , NN\n",
      "of       ADP   , IN\n",
      "privacy  NOUN  , NN\n",
      "at       ADP   , IN\n",
      "least    ADJ   , JJS\n",
      "I        PRON  , PRP\n",
      "can      AUX   , MD\n",
      "not      PART  , RB\n",
      "tell     VERB  , VB\n",
      "what     PRON  , WP\n",
      "the      DET   , DT\n",
      "table    NOUN  , NN\n",
      "next     ADV   , RB\n",
      "to       ADP   , IN\n",
      "me       PRON  , PRP\n",
      "were     AUX   , VBD\n",
      "eating   VERB  , VBG\n",
      ".        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "We       PRON  , PRP\n",
      "decided  VERB  , VBD\n",
      "on       ADP   , IN\n",
      "my       PRON  , PRP$\n",
      "favourite ADJ   , JJ\n",
      "chives   NOUN  , NNS\n",
      "and      CCONJ , CC\n",
      "pork     NOUN  , NN\n",
      "dumplings NOUN  , NNS\n",
      ",        PUNCT , ,\n",
      "kimchi   PROPN , NNP\n",
      "fried    VERB  , VBD\n",
      "rice     NOUN  , NN\n",
      "and      CCONJ , CC\n",
      "Korean   ADJ   , JJ\n",
      "cold     ADJ   , JJ\n",
      "bucknoodle NOUN  , NN\n",
      "soup     NOUN  , NN\n",
      ".        PUNCT , .\n",
      "They     PRON  , PRP\n",
      "are      AUX   , VBP\n",
      "all      ADV   , RB\n",
      "$        SYM   , $\n",
      "4.99     NUM   , CD\n",
      ".        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "The      DET   , DT\n",
      "cold     ADJ   , JJ\n",
      "noodle   NOUN  , NN\n",
      "soup     NOUN  , NN\n",
      "is       AUX   , VBZ\n",
      "so       ADV   , RB\n",
      "so       ADV   , RB\n",
      "good     ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "Nice     ADJ   , JJ\n",
      "and      CCONJ , CC\n",
      "refreshing ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "and      CCONJ , CC\n",
      "the      DET   , DT\n",
      "soup     NOUN  , NN\n",
      "based    VERB  , VBN\n",
      "is       AUX   , VBZ\n",
      "a        DET   , DT\n",
      "little   ADJ   , JJ\n",
      "sour     ADJ   , JJ\n",
      "and      CCONJ , CC\n",
      "it       PRON  , PRP\n",
      "really   ADV   , RB\n",
      "stimulate VERB  , VBP\n",
      "your     PRON  , PRP$\n",
      "tastebuds NOUN  , NNS\n",
      "and      CCONJ , CC\n",
      "just     ADV   , RB\n",
      "make     VERB  , VB\n",
      "you      PRON  , PRP\n",
      "want     VERB  , VB\n",
      "to       PART  , TO\n",
      "eat      VERB  , VB\n",
      "more     ADJ   , JJR\n",
      "!        PUNCT , .\n",
      "(        PUNCT , -LRB-\n",
      "I        PRON  , PRP\n",
      "can      AUX   , MD\n",
      "finish   VERB  , VB\n",
      "it       PRON  , PRP\n",
      "all      DET   , DT\n",
      "myself   PRON  , PRP\n",
      "!        PUNCT , .\n",
      "I        PRON  , PRP\n",
      "do       AUX   , VBP\n",
      "n't      PART  , RB\n",
      "want     VERB  , VB\n",
      "to       PART  , TO\n",
      "share    VERB  , VB\n",
      "!        PUNCT , .\n",
      "I        PRON  , PRP\n",
      "want     VERB  , VBP\n",
      "one      NUM   , CD\n",
      "all      DET   , DT\n",
      "for      ADP   , IN\n",
      "myself   PRON  , PRP\n",
      "next     ADJ   , JJ\n",
      "time     NOUN  , NN\n",
      "!        PUNCT , .\n",
      ")        PUNCT , -RRB-\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "The      DET   , DT\n",
      "kimchi   PROPN , NNP\n",
      "fried    VERB  , VBD\n",
      "rice     NOUN  , NN\n",
      "..       PUNCT , NFP\n",
      "hmmmmm   ADV   , RB\n",
      "not      PART  , RB\n",
      "good     ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "A        DET   , DT\n",
      "little   ADJ   , JJ\n",
      "on       ADP   , IN\n",
      "the      DET   , DT\n",
      "oily     ADJ   , JJ\n",
      "side     NOUN  , NN\n",
      ".        PUNCT , .\n",
      "There    PRON  , EX\n",
      "are      AUX   , VBP\n",
      "not      PART  , RB\n",
      "much     ADJ   , JJ\n",
      "taste    NOUN  , NN\n",
      ",        PUNCT , ,\n",
      "not      PART  , RB\n",
      "spicy    ADJ   , JJ\n",
      "enough   ADV   , RB\n",
      "?        PUNCT , .\n",
      "We       PRON  , PRP\n",
      "could    AUX   , MD\n",
      "not      PART  , RB\n",
      "finish   VERB  , VB\n",
      "it       PRON  , PRP\n",
      "and      CCONJ , CC\n",
      "end      VERB  , VB\n",
      "up       ADP   , RP\n",
      "have     VERB  , VBP\n",
      "to       PART  , TO\n",
      "pack     VERB  , VB\n",
      "it       PRON  , PRP\n",
      "all      DET   , DT\n",
      "to       PART  , TO\n",
      "go       VERB  , VB\n",
      "...      PUNCT , NFP\n",
      "(not     INTJ  , UH\n",
      "even     ADV   , RB\n",
      "sure     ADJ   , JJ\n",
      "I        PRON  , PRP\n",
      "will     AUX   , MD\n",
      "eat      VERB  , VB\n",
      "it       PRON  , PRP\n",
      ",        PUNCT , ,\n",
      "but      CCONJ , CC\n",
      "can      AUX   , MD\n",
      "not      PART  , RB\n",
      "just     ADV   , RB\n",
      "let      VERB  , VB\n",
      "it       PRON  , PRP\n",
      "all      DET   , DT\n",
      "go       VERB  , VB\n",
      "to       AUX   , IN\n",
      "waste    VERB  , VB\n",
      ")        PUNCT , -RRB-\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "The      DET   , DT\n",
      "dumplings NOUN  , NNS\n",
      "arrived  VERB  , VBD\n",
      "last     ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "But      CCONJ , CC\n",
      "it       PRON  , PRP\n",
      "really   ADV   , RB\n",
      "worth    ADJ   , JJ\n",
      "the      DET   , DT\n",
      "wait     NOUN  , NN\n",
      "!        PUNCT , .\n",
      "14       NUM   , CD\n",
      "big      ADJ   , JJ\n",
      "fat      ADJ   , JJ\n",
      "hot      ADJ   , JJ\n",
      "dumplings NOUN  , NNS\n",
      "filled   VERB  , VBD\n",
      "up       ADP   , RP\n",
      "the      DET   , DT\n",
      "whole    ADJ   , JJ\n",
      "steamer  NOUN  , NN\n",
      "!        PUNCT , .\n",
      "Yes      INTJ  , UH\n",
      "I        PRON  , PRP\n",
      "counted  VERB  , VBD\n",
      "them     PRON  , PRP\n",
      "14       NUM   , CD\n",
      "of       ADP   , IN\n",
      "them     PRON  , PRP\n",
      "for      ADP   , IN\n",
      "$        SYM   , $\n",
      "4.99     NUM   , CD\n",
      ".        PUNCT , .\n",
      "They     PRON  , PRP\n",
      "are      AUX   , VBP\n",
      "all      DET   , DT\n",
      "freshly  ADV   , RB\n",
      "made     VERB  , VBN\n",
      ",        PUNCT , ,\n",
      "freshly  ADV   , RB\n",
      "steamed  ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "The      DET   , DT\n",
      "skin     NOUN  , NN\n",
      "is       AUX   , VBZ\n",
      "nice     ADJ   , JJ\n",
      "and      CCONJ , CC\n",
      "thin     ADJ   , JJ\n",
      ",        PUNCT , ,\n",
      "not      PART  , RB\n",
      "chewy    NOUN  , NN\n",
      "and      CCONJ , CC\n",
      "thick    ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "Oh       INTJ  , UH\n",
      "so       ADV   , RB\n",
      "delicious ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "One      NUM   , CD\n",
      "bite     NOUN  , NN\n",
      "and      CCONJ , CC\n",
      "all      DET   , PDT\n",
      "the      DET   , DT\n",
      "soup     NOUN  , NN\n",
      "are      AUX   , VBP\n",
      "ozzing   VERB  , VBG\n",
      "out      ADP   , RP\n",
      "!        PUNCT , .\n",
      "But      CCONJ , CC\n",
      "if       SCONJ , IN\n",
      "they     PRON  , PRP\n",
      "are      AUX   , VBP\n",
      "not      PART  , RB\n",
      "burning  VERB  , VBG\n",
      "hot      ADJ   , JJ\n",
      ",        PUNCT , ,\n",
      "you      PRON  , PRP\n",
      "can      AUX   , MD\n",
      "just     ADV   , RB\n",
      "stuff    VERB  , VB\n",
      "the      DET   , DT\n",
      "whole    ADJ   , JJ\n",
      "thing    NOUN  , NN\n",
      "into     ADP   , IN\n",
      "your     PRON  , PRP$\n",
      "mouth    NOUN  , NN\n",
      "and      CCONJ , CC\n",
      "let      VERB  , VB\n",
      "the      DET   , DT\n",
      "soup     NOUN  , NN\n",
      "and      CCONJ , CC\n",
      "favour   NOUN  , NN\n",
      "just     ADV   , RB\n",
      "explosed VERB  , VBN\n",
      "in       ADP   , IN\n",
      "your     PRON  , PRP$\n",
      "mouth    NOUN  , NN\n",
      ".        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "Just     ADV   , RB\n",
      "a        DET   , DT\n",
      "warning  NOUN  , NN\n",
      ".        PUNCT , .\n",
      "If       SCONJ , IN\n",
      "you      PRON  , PRP\n",
      "going    VERB  , VBG\n",
      "out      ADP   , RP\n",
      "on       ADP   , IN\n",
      "a        DET   , DT\n",
      "date     NOUN  , NN\n",
      ",        PUNCT , ,\n",
      "please   INTJ  , UH\n",
      "do       AUX   , VBP\n",
      "n't      PART  , RB\n",
      "come     VERB  , VB\n",
      "here     ADV   , RB\n",
      ".        PUNCT , .\n",
      "Even     ADV   , RB\n",
      "it       PRON  , PRP\n",
      "is       VERB  , VBZ\n",
      "a        DET   , DT\n",
      "good     ADJ   , JJ\n",
      "place    NOUN  , NN\n",
      "for      ADP   , IN\n",
      "a        DET   , DT\n",
      "cheap    ADJ   , JJ\n",
      "date     NOUN  , NN\n",
      ",        PUNCT , ,\n",
      "our      PRON  , PRP$\n",
      "bill     NOUN  , NN\n",
      "was      AUX   , VBD\n",
      "$        SYM   , $\n",
      "18       NUM   , CD\n",
      "for      ADP   , IN\n",
      "2        NUM   , CD\n",
      "and      CCONJ , CC\n",
      "we       PRON  , PRP\n",
      "were     VERB  , VBD\n",
      "full     ADJ   , JJ\n",
      "!        PUNCT , .\n",
      "But      CCONJ , CC\n",
      "the      DET   , DT\n",
      "place    NOUN  , NN\n",
      "can      AUX   , MD\n",
      "be       VERB  , VB\n",
      "noisy    ADJ   , JJ\n",
      ",        PUNCT , ,\n",
      "the      DET   , DT\n",
      "food     NOUN  , NN\n",
      "too      ADV   , RB\n",
      "good     ADJ   , JJ\n",
      "and      CCONJ , CC\n",
      "your     PRON  , PRP$\n",
      "date     NOUN  , NN\n",
      "will     AUX   , MD\n",
      "be       VERB  , VB\n",
      "too      ADV   , RB\n",
      "busy     ADJ   , JJ\n",
      "eating   VERB  , VBG\n",
      "and      CCONJ , CC\n",
      "just     ADV   , RB\n",
      "ignore   VERB  , VB\n",
      "you      PRON  , PRP\n",
      "!        PUNCT , .\n",
      "Or       CCONJ , CC\n",
      "you      PRON  , PRP\n",
      "will     AUX   , MD\n",
      "keep     VERB  , VB\n",
      "eating   VERB  , VBG\n",
      "and      CCONJ , CC\n",
      "eating   VERB  , VBG\n",
      "and      CCONJ , CC\n",
      "you      PRON  , PRP\n",
      "freak    VERB  , VBP\n",
      "all      DET   , PDT\n",
      "your     PRON  , PRP$\n",
      "friends  NOUN  , NNS\n",
      "out      ADP   , RP\n",
      "!        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "The      DET   , DT\n",
      "services NOUN  , NNS\n",
      "here     ADV   , RB\n",
      "is       AUX   , VBZ\n",
      "cold     ADJ   , JJ\n",
      "but      CCONJ , CC\n",
      "efficient ADJ   , JJ\n",
      ".        PUNCT , .\n",
      "We       PRON  , PRP\n",
      "were     AUX   , VBD\n",
      "given    VERB  , VBN\n",
      "extra    ADJ   , JJ\n",
      "napkins  NOUN  , NNS\n",
      "without  ADP   , IN\n",
      "requesting VERB  , VBG\n",
      "...      PUNCT , :\n",
      "Maybe    ADV   , RB\n",
      "we       PRON  , PRP\n",
      "looked   VERB  , VBD\n",
      "really   ADV   , RB\n",
      "messy    ADJ   , JJ\n",
      "?        PUNCT , .\n",
      "?        PUNCT , .\n",
      "\n",
      "\n",
      "       SPACE , _SP\n",
      "Cash     VERB  , VB\n",
      "only     ADV   , RB\n",
      "!        PUNCT , .\n",
      "best     ADJ   , JJS\n",
      "pizza    NOUN  , NN\n",
      "in       ADP   , IN\n",
      "south    ADJ   , JJ\n",
      "side     NOUN  , NN\n",
      ".        PUNCT , .\n",
      "huge     ADJ   , JJ\n",
      "drink    NOUN  , NN\n",
      "selection NOUN  , NN\n",
      ".        PUNCT , .\n",
      "awesome  ADJ   , JJ\n",
      "atmosphere NOUN  , NN\n",
      ".        PUNCT , .\n",
      "$        SYM   , $\n",
      "4        NUM   , CD\n",
      "for      ADP   , IN\n",
      "a        DET   , DT\n",
      "slice    NOUN  , NN\n",
      "that     DET   , WDT\n",
      "could    AUX   , MD\n",
      "easily   ADV   , RB\n",
      "feed     VERB  , VB\n",
      "2        NUM   , CD\n",
      "people   NOUN  , NNS\n",
      "!        PUNCT , .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tagged = []\n",
    "for sentence in random_sentences:\n",
    "    spacy_tagged.append(nlp(sentence))\n",
    "for tagged in spacy_tagged:\n",
    "    for token in tagged:\n",
    "        print(f'{token.text:{8}} {token.pos_:{6}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627839e",
   "metadata": {},
   "source": [
    "# WORK COMPLETED UP TILL HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f44ab",
   "metadata": {},
   "source": [
    "### Writing Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57662d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSOFurl():\n",
    "    while(True):\n",
    "        ip0 = 'stackoverflow'\n",
    "        ip1 = 'com'\n",
    "        ip2 = 'questions'\n",
    "        ip3 = str(random.randint(0, 100000000))\n",
    "        url = 'https://' + ip0 + '.' + ip1 + '/'+ ip2 + '/'+ ip3\n",
    "        try:\n",
    "            urlContent = urlopen(url).read()\n",
    "            if urlContent:\n",
    "                break\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    print(\"Found URL: \" + url)\n",
    "    page1 = requests.get(url)\n",
    "    soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "    text = list(soup1.find_all(\"p\"))\n",
    "    sof_text = [txt.get_text() for txt in text]\n",
    "   \n",
    "    return sof_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ba56e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHWZurl():\n",
    "    url = 'https://www.hardwarezone.com.sg/home'\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "    urls = []\n",
    "    for link in soup.find_all('a'):\n",
    "        urls.append(link.get('href'))\n",
    "    random.shuffle(urls)\n",
    "    while(True):\n",
    "        for url in urls:\n",
    "            print(url)\n",
    "            try:\n",
    "                url = 'https://www.hardwarezone.com.sg' + url\n",
    "                urlContent = urlopen(url).read()\n",
    "                page1 = requests.get(url)\n",
    "                soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "                text = list(soup1.find_all(\"p\"))\n",
    "                hwz_text = [txt.get_text() for txt in text]\n",
    "                if urlContent and len(hwz_text)>10:\n",
    "                    break\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "        break\n",
    "    print(\"Found URL: \" + url)\n",
    "    return hwz_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "380f0b1c-e1d2-4e2e-a744-d92781fa7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "def getCNAurl():\n",
    "    url = 'https://www.channelnewsasia.com/'\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "    linked_urls = set()\n",
    "\n",
    "    def get_all_links(url):\n",
    "        urls = set()\n",
    "        domain_name = urlparse(url).netloc\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "        for a_tag in soup.findAll(\"a\"):\n",
    "            href = a_tag.attrs.get(\"href\")\n",
    "            if href == \"\" or href is None:\n",
    "                # href empty tag\n",
    "                continue\n",
    "            href = urljoin(url, href)\n",
    "            parsed_href = urlparse(href)\n",
    "            # remove URL GET parameters, URL fragments, etc.\n",
    "            href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "            if href in urls:\n",
    "                # already in the set\n",
    "                continue\n",
    "            if domain_name not in href:\n",
    "                # external link\n",
    "                continue\n",
    "            if len(href) > 60:\n",
    "                urls.add(href)\n",
    "        return urls\n",
    "    \n",
    "    urls_cna = get_all_links('https://www.channelnewsasia.com/')\n",
    "    url = random.sample(urls_cna, 1)\n",
    "    print(\"Found URL: \", url[0])\n",
    "    \n",
    "    page1 = requests.get(url[0])\n",
    "    soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "    text = list(soup1.find_all(\"p\"))\n",
    "    cna_text = [txt.get_text() for txt in text]\n",
    "   \n",
    "    return cna_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb97780",
   "metadata": {},
   "source": [
    "#### Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf0b4c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URL: https://stackoverflow.com/questions/67452202\n",
      "Found URL: https://stackoverflow.com/questions/43866776\n"
     ]
    }
   ],
   "source": [
    "sof_text1 = getSOFurl()\n",
    "sof_text2 = getSOFurl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2104397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javascript:openDkSideNav()\n",
      "nonnumeric port: 'openDkSideNav()'\n",
      "/tech-news-tom-hanks-apple-tv-plus-apocalyptic-finch\n",
      "Found URL: https://www.hardwarezone.com.sg/tech-news-tom-hanks-apple-tv-plus-apocalyptic-finch\n",
      "javascript:void(0)\n",
      "nonnumeric port: 'void(0)'\n",
      "/product-guide/211-coolers/home\n",
      "/product-guide/404-internet-cloud-ecommerce/home\n",
      "/product-guide/403-apps-software/home\n",
      "/home\n",
      "/feature-nia-dacostas-candyman-beautifully-presented-falls-just-short-perfect-score\n",
      "Found URL: https://www.hardwarezone.com.sg/feature-nia-dacostas-candyman-beautifully-presented-falls-just-short-perfect-score\n"
     ]
    }
   ],
   "source": [
    "hwz_text1 = getHWZurl()\n",
    "hwz_text2 = getHWZurl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bfeee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colot\\AppData\\Local\\Temp/ipykernel_17340/690541370.py:33: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  url = random.sample(urls_cna, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URL:  https://www.channelnewsasia.com/business/exclusive-india-likely-block-chinese-investment-insurance-giant-lics-ipo-sources-2194451\n",
      "Found URL:  https://www.channelnewsasia.com/business/eu-court-backs-altices-million-euro-fine-gun-jumping-merger-deal-2194441\n"
     ]
    }
   ],
   "source": [
    "cna_text1 = getCNAurl()\n",
    "cna_text2 = getCNAurl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee8a1e",
   "metadata": {},
   "source": [
    "##### Cleaning SOF text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cabf88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_text1 = sof_text1[4:]\n",
    "sof_text1 = sof_text1[:-10]\n",
    "temp_list = []\n",
    "for line in sof_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "sof_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5be9617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have a method which returns pointer of one of three relative classes:',\n",
       " 'And I need to create an object exactly of the class, which class` pointer method returns',\n",
       " 'I tryed the line typeid(root->search_by_name(process_object)) myobject;But it`s obviously silly.',\n",
       " 'Could you advise something?',\n",
       " \"It's not possible to derive the static type of the polymorphic object so as to make a static declaration of type as in:\",\n",
       " 'There are a couple of things you could do.',\n",
       " 'One is to put a clone() or create() style function in your polymorphic classes to return correctly typed dynamic objects.',\n",
       " 'The other would be to manually query the possible types, which rather defeats the point of polymorphism:',\n",
       " 'NOTE: Pointers used for exposition only, use std::unique_ptr etc.. in real code.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sof_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc0f4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_text2 = sof_text2[4:]\n",
    "sof_text2 = sof_text2[:-10]\n",
    "temp_list = []\n",
    "for line in sof_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "sof_text2 = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fb7f9",
   "metadata": {},
   "source": [
    "##### Cleaning CNA text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6186128c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n      Business\\n  \\n',\n",
       " '\\n\\n      Business\\n  \\n',\n",
       " 'NEW DELHI :     New Delhi wants to block Chinese investors from buying shares in Indian insurance giant Life Insurance Corp (LIC) which is due to go public, four senior government officials and a banker told Reuters, underscoring tensions between the two nations.',\n",
       " \"State-owned LIC is considered a strategic asset, commanding more than 60per cent of India's life insurance market with assets of more than US$500 billion. While the government is planning to allow foreign investors to participate in what is likely to be the country's biggest-ever IPO worth a potential US$12.2 billion, it is leery of Chinese ownership, the sources said.\",\n",
       " 'Political tensions between the countries rocketed last year after their soldiers clashed on the disputed Himalayan border and since then, India has sought to limit Chinese investment in sensitive companies and sectors, banned a raft of Chinese mobile apps and subjected imports of Chinese goods to extra scrutiny.',\n",
       " '\"With China after the border clashes it cannot be business as usual. The trust deficit has significantly widen(ed),\" said one of the government officials, adding that Chinese investment in companies like LIC could pose risks.',\n",
       " 'The sources declined to be identified as discussions on how Chinese investment might be blocked are ongoing and as no final decisions have been made.',\n",
       " \"India's finance ministry and LIC did not respond to Reuters emailed requests for comment. China's foreign ministry and commerce ministry did not immediately respond to requests for comment.\",\n",
       " \"Aiming to solve budget constraints, Prime Minister Narendra Modi's administration is hoping to raise 900 billion rupees through selling 5per cent to 10per cent of LIC this financial year which ends in March. The government has yet to decide on whether it will sell one tranche of shares seeking to raise the full amount or choose to seek the funds in two tranches, sources have said.\",\n",
       " \"Under current law, no overseas investors can invest in LIC but the government is considering allowing foreign institutional investors to buy up to 20per cent of LIC's offering.\",\n",
       " 'Options to prevent Chinese investment in LIC include amending the current law on foreign direct investment with a clause that relates to LIC or creating a new law specific to LIC, two of the government officials said.',\n",
       " \"They added that the government was conscious of the difficulty in checking on Chinese investments that could come indirectly and would attempt to craft a policy that would protect India's security but not deter overseas investors.\",\n",
       " 'A third option being explored is barring Chinese investors from becoming cornerstone investors in the IPO, said one government official and the banker, although that would not prevent Chinese investors from buying shares in the secondary market.',\n",
       " 'Ten investment banks including Goldman Sachs, Citigroup and SBI Capital Market have been chosen to handle the offering.',\n",
       " '(US$1 = 73.8200 Indian rupees)',\n",
       " '',\n",
       " ' (Reporting by Aftab Ahmed and Manoj Kumar in New Delhi, Nupur Anand in Mumbai; Additional reporting by Beijing Newsroom; Editing by Sanjeev Miglani and Edwina Gibbs)',\n",
       " \"\\n      This service is not intended for persons residing in the E.U. By clicking subscribe, I agree to receive news updates and promotional material from Mediacorp and Mediacorp's partners. \\n  \",\n",
       " ' Copyright© Mediacorp 2021. Mediacorp Pte Ltd. All rights reserved. ',\n",
       " \"We know it's a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be.\",\n",
       " 'To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.',\n",
       " 'Upgraded but still having issues? Contact us']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cna_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d0987",
   "metadata": {},
   "source": [
    "##### Cleaning HWZ text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c822fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_text1 = hwz_text1[:-5]\n",
    "temp_list = []\n",
    "for line in hwz_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "hwz_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d96f688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_text2 = hwz_text2[:-5]\n",
    "temp_list = []\n",
    "for line in hwz_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "hwz_text2 = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11179763",
   "metadata": {},
   "source": [
    "##### First word in sentence capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93702e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for sof_text1:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I have a method which returns pointer of one of three relative classes:',\n",
       " 'And I need to create an object exactly of the class, which class` pointer method returns',\n",
       " 'I tryed the line typeid(root->search_by_name(process_object)) myobject;But it`s obviously silly.',\n",
       " 'Could you advise something?',\n",
       " \"It's not possible to derive the static type of the polymorphic object so as to make a static declaration of type as in:\",\n",
       " 'There are a couple of things you could do.',\n",
       " 'One is to put a clone() or create() style function in your polymorphic classes to return correctly typed dynamic objects.',\n",
       " 'The other would be to manually query the possible types, which rather defeats the point of polymorphism:',\n",
       " 'NOTE: Pointers used for exposition only, use std::unique_ptr etc.. in real code.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in sof_text1:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for sof_text1: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7ebe5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for sof_text2:  0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I found on this site solution about move legend in fieldset from border to inside fieldset.',\n",
       " 'http://codepen.io/vkjgr/pen/oFdBa When I want to do this i should use float left style on legend, but I should use clear both after legend ?',\n",
       " 'This solution works on all browsers ?',\n",
       " 'When I want clean I must use only clear both ?',\n",
       " 'If you want to follow your own approach, it also works as shown below.',\n",
       " 'Otherwise, width:100% on legend will also do.',\n",
       " 'fieldset {\\r\\n  padding: 2em;\\r\\n}\\r\\n\\r\\nlegend {\\r\\n  float: left;\\r\\n}\\r\\n\\r\\n.clear {\\r\\n  clear: both;\\r\\n}\\n<fieldset>\\r\\n  <legend>\\r\\n    Hello\\r\\n  </legend>\\r\\n  <div class=\"clear\">\\r\\n\\r\\n  </div>\\r\\n  <p>\\r\\n    Lorem ipsum dolor sit amet, consectetur adipisicing elit.',\n",
       " 'Neque sunt ad facilis fugiat perferendis et fugit quas, accusantium, quod atque provident natus facere animi sed accusamus qui doloribus illo nesciunt.',\n",
       " '</p>\\r\\n</fieldset>']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in sof_text2:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for sof_text2: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "913e6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for hwz_text1:  1.0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in hwz_text1:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for hwz_text1: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d80943eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for hwz_text2:  1.0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in hwz_text2:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for hwz_text2: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20b0680a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17340/1562548034.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0muppercount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcna_text1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0muppercount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in cna_text1:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for cna_text1: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4bcc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9e31e9b",
   "metadata": {},
   "source": [
    "##### Length of articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1137d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentences in sof_text1:  9\n",
      "No of sentences in sof_text2:  9\n",
      "No of sentences in hwz_text1:  9\n",
      "No of sentences in hwz_text2:  50\n"
     ]
    }
   ],
   "source": [
    "print(\"No of sentences in sof_text1: \", len(sof_text1))\n",
    "print(\"No of sentences in sof_text2: \", len(sof_text2))\n",
    "print(\"No of sentences in hwz_text1: \", len(hwz_text1))\n",
    "print(\"No of sentences in hwz_text2: \", len(hwz_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90754d4",
   "metadata": {},
   "source": [
    "##### Proper nouns capitalised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a5c0f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in sof_text1:  0.0\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in sof_text1:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in sof_text1: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa0d3f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in sof_text2:  0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in sof_text2:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in sof_text2: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66a68814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in hwz_text1:  1.0\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in hwz_text1:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in hwz_text1: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adda956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in hwz_text2:  0.958904109589041\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in hwz_text2:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in hwz_text2: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7828fc9",
   "metadata": {},
   "source": [
    "###### What kind of proper nouns used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9214571",
   "metadata": {},
   "source": [
    "1. Stack Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d27c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legend 6\n",
      "method 2\n",
      "pointer 2\n",
      "classes 2\n",
      "object 2\n",
      "class 2\n",
      "type 2\n",
      "style 2\n",
      "solution 2\n",
      "fieldset 2\n",
      "float 2\n",
      "line 1\n",
      "typeid(root->search_by_name(process_object 1\n",
      "myobject;But 1\n",
      "it`s 1\n",
      "declaration 1\n",
      "couple 1\n",
      "things 1\n",
      "clone 1\n",
      "function 1\n",
      "objects 1\n",
      "types 1\n",
      "point 1\n",
      "polymorphism 1\n",
      "NOTE 1\n",
      "Pointers 1\n",
      "exposition 1\n",
      "std::unique_ptr 1\n",
      "code 1\n",
      "site 1\n",
      "move 1\n",
      "border 1\n",
      "browsers 1\n",
      "approach 1\n",
      "width:100 1\n",
      "% 1\n",
      "padding 1\n",
      "2em 1\n",
      "Hello 1\n",
      "div 1\n",
      "class=\"clear 1\n",
      "/div 1\n",
      "p 1\n",
      "Lorem 1\n",
      "ipsum 1\n",
      "dolor 1\n",
      "amet 1\n",
      "consectetur 1\n",
      "adipisicing 1\n",
      "elit 1\n",
      "Neque 1\n",
      "sunt 1\n",
      "ad 1\n",
      "facilis 1\n",
      "fugiat 1\n",
      "et 1\n",
      "fugit 1\n",
      "quas 1\n",
      "accusantium 1\n",
      "quod 1\n",
      "atque 1\n",
      "natus 1\n",
      "facere 1\n",
      "animi 1\n",
      "accusamus 1\n",
      "qui 1\n",
      "doloribus 1\n",
      "illo 1\n",
      "nesciunt 1\n",
      "/p 1\n"
     ]
    }
   ],
   "source": [
    "sof_tagged = []\n",
    "sof_noun_dict = {}\n",
    "for sentence in sof_text1:\n",
    "    sof_tagged.append(nlp(sentence))\n",
    "for sentence in sof_text2:\n",
    "    sof_tagged.append(nlp(sentence))\n",
    "for tagged in sof_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in sof_noun_dict.keys():\n",
    "                sof_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                sof_noun_dict[token.text] = 1\n",
    "\n",
    "sof_noun_dict_sorted = sorted(sof_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sof_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbbdc5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candyman 23\n",
      "movie 16\n",
      "film 9\n",
      "Anthony 9\n",
      "time 6\n",
      "story 6\n",
      "presentation 5\n",
      "man 5\n",
      "Finch 4\n",
      "slasher 4\n",
      "legend 4\n",
      "hook 4\n",
      "narrative 4\n",
      "Brianna 4\n",
      "exhibition 4\n",
      "Tom 3\n",
      "Goodyear 3\n",
      "horror 3\n",
      "DaCosta 3\n",
      "injustice 3\n",
      "day 3\n",
      "hand 3\n",
      "things 3\n",
      "show 3\n",
      "pacing 3\n",
      "environments 3\n",
      "Apple 2\n",
      "content 2\n",
      "Hanks 2\n",
      "Jeff 2\n",
      "death 2\n",
      "trailer 2\n",
      "world 2\n",
      "Aftershock 2\n",
      "custom 2\n",
      "PC 2\n",
      "curve 2\n",
      "way 2\n",
      "sequel 2\n",
      "fact 2\n",
      "flick 2\n",
      "point 2\n",
      "mirrors 2\n",
      "gore 2\n",
      "series 2\n",
      "discrimination 2\n",
      "artist 2\n",
      "Cabrini 2\n",
      "Green 2\n",
      "art 2\n",
      "ritual 2\n",
      "origins 2\n",
      "fans 2\n",
      "background 2\n",
      "twist 2\n",
      "one 2\n",
      "information 2\n",
      "head 2\n",
      "end 2\n",
      "flow 2\n",
      "example 2\n",
      "Netflix 1\n",
      "strategy 1\n",
      "laser 1\n",
      "award 1\n",
      "titles 1\n",
      "scale 1\n",
      "service 1\n",
      "Greyhound 1\n",
      "Sony 1\n",
      "year 1\n",
      "robotics 1\n",
      "engineer 1\n",
      "Earth 1\n",
      "surface 1\n",
      "climate 1\n",
      "flare 1\n",
      "sun 1\n",
      "companion 1\n",
      "DIY 1\n",
      "robot 1\n",
      "heart 1\n",
      "determination 1\n",
      "caretaker 1\n",
      "trio 1\n",
      "journey 1\n",
      "storm 1\n",
      "safety 1\n",
      "road 1\n",
      "trip 1\n",
      "challenges 1\n",
      "humor 1\n",
      "dangers 1\n",
      "streaming 1\n",
      "TV+ 1\n",
      "November 1\n",
      "record 1\n",
      "films 1\n",
      "prejudices 1\n",
      "Nia 1\n",
      "matter 1\n",
      "component 1\n",
      "view 1\n",
      "hooks 1\n",
      "theme 1\n",
      "use 1\n",
      "shadow 1\n",
      "puppetry 1\n",
      "audiences 1\n",
      "concept 1\n",
      "backstory 1\n",
      "method 1\n",
      "core 1\n",
      "Bernard 1\n",
      "Rose 1\n",
      "Clive 1\n",
      "Barker 1\n",
      "Forbidden 1\n",
      "title 1\n",
      "centrepiece 1\n",
      "instalments 1\n",
      "apparition 1\n",
      "form 1\n",
      "gaunt 1\n",
      "coat 1\n",
      "place 1\n",
      "Bloody 1\n",
      "Mary 1\n",
      "folks 1\n",
      "name 1\n",
      "times 1\n",
      "front 1\n",
      "mirror 1\n",
      "person 1\n",
      "centers 1\n",
      "McCoy 1\n",
      "Yahya 1\n",
      "Abdul 1\n",
      "Mateen 1\n",
      "II 1\n",
      "apartment 1\n",
      "estate 1\n",
      "girlfriend 1\n",
      "gallery 1\n",
      "director 1\n",
      "Cartwright 1\n",
      "Teyonah 1\n",
      "Parris 1\n",
      "trouble 1\n",
      "inspiration 1\n",
      "brother 1\n",
      "Troy 1\n",
      "Nathan 1\n",
      "Stewart 1\n",
      "Jarrett 1\n",
      "dinner 1\n",
      "party 1\n",
      "chance 1\n",
      "encounter 1\n",
      "interest 1\n",
      "district 1\n",
      "shares 1\n",
      "experience 1\n",
      "knowledge 1\n",
      "resolves 1\n",
      "representation 1\n",
      "anguish 1\n",
      "guests 1\n",
      "wives 1\n",
      "tale 1\n",
      "turn 1\n",
      "people 1\n",
      "fashion 1\n",
      "number 1\n",
      "victims 1\n",
      "truth 1\n",
      "role 1\n",
      "consequences 1\n",
      "parts 1\n",
      "spoilers 1\n",
      "review 1\n",
      "purpose 1\n",
      "context 1\n",
      "twists 1\n",
      "list 1\n",
      "entity 1\n",
      "hive 1\n",
      "relevance 1\n",
      "bees 1\n",
      "audience 1\n",
      "term 1\n",
      "paper 1\n",
      "while 1\n",
      "beginning 1\n",
      "revelation 1\n",
      "spirit 1\n",
      "Sherman 1\n",
      "Fields 1\n",
      "police 1\n",
      "crime 1\n",
      "jigsaw 1\n",
      "puzzle 1\n",
      "pieces 1\n",
      "fuel 1\n",
      "fire 1\n",
      "Daniel 1\n",
      "Robitaille 1\n",
      "treat 1\n",
      "bit 1\n",
      "shows 1\n",
      "genre 1\n",
      "scare 1\n",
      "premise 1\n",
      "payload 1\n",
      "importance 1\n",
      "cards 1\n",
      "chest 1\n",
      "stream 1\n",
      "significance 1\n",
      "murders 1\n",
      "sub 1\n",
      "job 1\n",
      "mood 1\n",
      "state 1\n",
      "start 1\n",
      "colours 1\n",
      "sequences 1\n",
      "reign 1\n",
      "fear 1\n",
      "grips 1\n",
      "cast 1\n",
      "tones 1\n",
      "themes 1\n",
      "artworks 1\n",
      "stage 1\n",
      "shift 1\n",
      "points 1\n",
      "whole 1\n",
      "changes 1\n",
      "settings 1\n",
      "reflections 1\n",
      "days 1\n",
      "coherence 1\n",
      "part 1\n",
      "idea 1\n",
      "aspect 1\n",
      "order 1\n",
      "gamer 1\n",
      "% 1\n",
      "lick 1\n",
      "sense 1\n",
      "Goat 1\n",
      "Simulator 1\n",
      "gaps 1\n",
      "token 1\n",
      "condition 1\n",
      "questions 1\n",
      "rug 1\n",
      "bits 1\n",
      "building 1\n",
      "price 1\n",
      "exchange 1\n",
      "board 1\n",
      "universe 1\n",
      "round 1\n",
      "applause 1\n",
      "viewers 1\n",
      "lore 1\n",
      "specks 1\n",
      "rust 1\n",
      "none 1\n",
      "boxes 1\n",
      "concepts 1\n",
      "boot 1\n",
      "today 1\n",
      "course 1\n",
      "amount 1\n",
      "feedback 1\n",
      "article 1\n",
      "team 1\n"
     ]
    }
   ],
   "source": [
    "hwz_tagged = []\n",
    "hwz_noun_dict = {}\n",
    "for sentence in hwz_text1:\n",
    "    hwz_tagged.append(nlp(sentence))\n",
    "for sentence in hwz_text2:\n",
    "    hwz_tagged.append(nlp(sentence))\n",
    "for tagged in hwz_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in hwz_noun_dict.keys():\n",
    "                hwz_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                hwz_noun_dict[token.text] = 1\n",
    "\n",
    "hwz_noun_dict_sorted = sorted(hwz_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in hwz_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac262786",
   "metadata": {},
   "source": [
    "##### Good grammar?\n",
    "1. Subject-verb agreement\n",
    "2. Tense matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91989b",
   "metadata": {},
   "source": [
    "#### Subject-verb agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b53a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3ebd07c8375e47af8da6df3c3ed96925-0\" class=\"displacy\" width=\"1800\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">There</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">lot</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Korean</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">dishes</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">menu</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">expl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-6\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3ebd07c8375e47af8da6df3c3ed96925-0-8\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3ebd07c8375e47af8da6df3c3ed96925-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,266.5 L1628.0,254.5 1612.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"There are a lot of Korean dishes in the menu\")\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a097237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "def is_passive(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dict1 = {'DEP': 'nsubjpass'}\n",
    "    dict2 = {'DEP': 'aux', 'OP': '*'}\n",
    "    dict3 = {'DEP': 'auxpass'}\n",
    "    dict4 = {'TAG': 'VBN'}\n",
    "    passive_rule = [dict1, dict2, dict3, dict4]\n",
    "    matcher.add(\"Passive\", [passive_rule])\n",
    "    matches = matcher(doc)\n",
    "    if matches:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8894a",
   "metadata": {},
   "source": [
    "https://github.com/armsp/active_or_passive/blob/master/spacy_voices.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55e652",
   "metadata": {},
   "source": [
    "### Short break to count the % of passive sentences in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "503a3e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of passive sentences in sof_text1:  0.0\n",
      "% of passive sentences in sof_text2:  0.0\n"
     ]
    }
   ],
   "source": [
    "sof_text1_passivecounts = 0\n",
    "sof_text2_passivecounts = 0 \n",
    "\n",
    "for sent in sof_text1:\n",
    "    if is_passive(sent):\n",
    "        sof_text1_passivecounts += 1\n",
    "for sent in sof_text2:\n",
    "    if is_passive(sent):\n",
    "        sof_text2_passivecounts += 1\n",
    "\n",
    "print(\"% of passive sentences in sof_text1: \", sof_text1_passivecounts/len(sof_text1))\n",
    "print(\"% of passive sentences in sof_text2: \", sof_text2_passivecounts/len(sof_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d474e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of passive sentences in hwz_text1:  0.1111111111111111\n",
      "% of passive sentences in hwz_text2:  0.1\n"
     ]
    }
   ],
   "source": [
    "hwz_text1_passivecounts = 0\n",
    "hwz_text2_passivecounts = 0 \n",
    "\n",
    "for sent in hwz_text1:\n",
    "    if is_passive(sent):\n",
    "        hwz_text1_passivecounts += 1\n",
    "for sent in hwz_text2:\n",
    "    if is_passive(sent):\n",
    "        hwz_text2_passivecounts += 1\n",
    "\n",
    "print(\"% of passive sentences in hwz_text1: \", hwz_text1_passivecounts/len(hwz_text1))\n",
    "print(\"% of passive sentences in hwz_text2: \", hwz_text2_passivecounts/len(hwz_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043af22c",
   "metadata": {},
   "source": [
    "# Ok back to business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sof_text1:\n",
    "    if not is_passive(sent):\n",
    "        sent_tagged = nlp(sent)\n",
    "        for word, tag in sent_tagged:\n",
    "            if tag == 'NNS' or tag == 'AUX':\n",
    "                \n",
    "        \n",
    "for sent in sof_text2:\n",
    "    if is_passive(sent):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ffa9de",
   "metadata": {},
   "source": [
    "# HELP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab469c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2fd79",
   "metadata": {},
   "source": [
    "# So i learnt that this way is not good. i will attempt to try with spacy dependency tracker and then combine with nltk pos tagger to check if there is SVA. thx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2f2ac",
   "metadata": {},
   "source": [
    "# this is what i will do\n",
    "# 1. choose 6 good urls (2 from each source) so that there can be comparisons\n",
    "# 2. do the CNA preprocessing part\n",
    "# 3. continue with the two parts for checking for good grammar (any more ideas?)\n",
    "# 4. think n implement more things to analyse for writing style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27625a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
