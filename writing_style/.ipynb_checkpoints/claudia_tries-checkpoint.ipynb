{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "97e6fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from collections import Counter\n",
    "# import httplib2\n",
    "import itertools\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd \n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import urllib.request\n",
    "from urllib.request import urlopen, Request\n",
    "import random\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bc45d509-90f8-4db6-a063-13e17fad1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70f63a",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c2790af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_json('../data/reviewSelected100.json', encoding='ISO-8859-1', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e12df",
   "metadata": {},
   "source": [
    "## 3.2 Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14711fd",
   "metadata": {},
   "source": [
    "### Tokenisation and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "78f7f863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>1WhT1Fp4YcQqc9pOOnd2Iw</td>\n",
       "      <td>akL4MRM2jytzX07Ut2HDyA</td>\n",
       "      <td>NdpvGGF4cLrdnA6ydSZz3g</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Service was horrible. Simply way overcharging ...</td>\n",
       "      <td>2017-12-29 15:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>l8s3iaWZUEsp1Y7E8aVlDA</td>\n",
       "      <td>i0cM1FAC3dy3t5tReCm98Q</td>\n",
       "      <td>NdpvGGF4cLrdnA6ydSZz3g</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-Each 'order' of meat is literally 5 slices, s...</td>\n",
       "      <td>2016-02-23 23:30:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>AOV9NUzFiXxQOKcHKflN_w</td>\n",
       "      <td>bjy0OVaWhdDh3vynEyNNtg</td>\n",
       "      <td>NdpvGGF4cLrdnA6ydSZz3g</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This restaurant is very good. I love the servi...</td>\n",
       "      <td>2017-01-28 21:57:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>BxD9tgEvdE7pk3FVh_pFHg</td>\n",
       "      <td>4fdrCA2ahO2LA4G39hVdlw</td>\n",
       "      <td>NdpvGGF4cLrdnA6ydSZz3g</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Very good service and variety of foods you can...</td>\n",
       "      <td>2017-05-14 16:24:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>ljgoOYRr3MC7kLNUMACTLw</td>\n",
       "      <td>i0cM1FAC3dy3t5tReCm98Q</td>\n",
       "      <td>NdpvGGF4cLrdnA6ydSZz3g</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>good quality ingredients, horrible wait time.\\...</td>\n",
       "      <td>2016-12-20 05:33:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   review_id                 user_id             business_id  \\\n",
       "3112  1WhT1Fp4YcQqc9pOOnd2Iw  akL4MRM2jytzX07Ut2HDyA  NdpvGGF4cLrdnA6ydSZz3g   \n",
       "3159  l8s3iaWZUEsp1Y7E8aVlDA  i0cM1FAC3dy3t5tReCm98Q  NdpvGGF4cLrdnA6ydSZz3g   \n",
       "3181  AOV9NUzFiXxQOKcHKflN_w  bjy0OVaWhdDh3vynEyNNtg  NdpvGGF4cLrdnA6ydSZz3g   \n",
       "3192  BxD9tgEvdE7pk3FVh_pFHg  4fdrCA2ahO2LA4G39hVdlw  NdpvGGF4cLrdnA6ydSZz3g   \n",
       "3193  ljgoOYRr3MC7kLNUMACTLw  i0cM1FAC3dy3t5tReCm98Q  NdpvGGF4cLrdnA6ydSZz3g   \n",
       "\n",
       "      stars  useful  funny  cool  \\\n",
       "3112      1       1      0     0   \n",
       "3159      3       2      0     0   \n",
       "3181      5       0      0     0   \n",
       "3192      4       0      0     0   \n",
       "3193      3       0      0     0   \n",
       "\n",
       "                                                   text                date  \n",
       "3112  Service was horrible. Simply way overcharging ... 2017-12-29 15:58:00  \n",
       "3159  -Each 'order' of meat is literally 5 slices, s... 2016-02-23 23:30:05  \n",
       "3181  This restaurant is very good. I love the servi... 2017-01-28 21:57:40  \n",
       "3192  Very good service and variety of foods you can... 2017-05-14 16:24:44  \n",
       "3193  good quality ingredients, horrible wait time.\\... 2016-12-20 05:33:51  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get reviews for a random business \n",
    "random_business = reviews.sample()\n",
    "random_business_id = random_business.iloc[0]['business_id']\n",
    "small_business_dataset = reviews.loc[reviews['business_id'] == random_business_id]\n",
    "small_business_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c91d93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_business_dataset_reviews = list(small_business_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0669b8f6-714b-4708-840b-2c65c03f97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the reviews into a concatenated string \n",
    "b1_review = ''.join(small_business_dataset_reviews)\n",
    "clean_review = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", b1_review)\n",
    "b1_review = nlp(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "deaa0847-5dec-4317-b9f5-ece17562bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 619), ('and', 425), ('to', 404), ('I', 327), ('a', 311), ('was', 293), ('of', 272), ('for', 221), ('is', 205), ('we', 169)]\n"
     ]
    }
   ],
   "source": [
    "# removed punctuation and get the top 10 most common words (including stopwords)\n",
    "b1_review_words = [token.text for token in b1_review if token.is_alpha == True] \n",
    "b1_word_freq = Counter(b1_review_words)\n",
    "common_words = b1_word_freq.most_common(10)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ede74a1b-4e6c-4436-8c3b-df2f479aad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nt', 129), ('food', 115), ('pot', 89), ('hot', 88), ('service', 87), ('soup', 87), ('place', 84), ('good', 74), ('table', 57), ('came', 56)]\n"
     ]
    }
   ],
   "source": [
    "# removed punctuation and get the top 10 most common words (excluding stopwords)\n",
    "b1_review_words = [token.text for token in b1_review if token.is_stop != True and token.is_alpha == True] \n",
    "b1_word_freq = Counter(b1_review_words)\n",
    "common_words = b1_word_freq.most_common(10)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "82e4306f-2536-450b-ba7d-cc0f073dfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot log graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "eeb35a0e-7b64-4e9a-9d53-e89a299e51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do some stemming after removing the stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "porter_st = PorterStemmer()\n",
    "lancaster_st = LancasterStemmer()\n",
    "snow_st = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bfe90886-30ac-4282-978f-f66739952d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 132), ('nt', 129), ('place', 120), ('servic', 116), ('pot', 109), ('order', 102), ('soup', 97), ('hot', 93), ('good', 79), ('tabl', 69)]\n"
     ]
    }
   ],
   "source": [
    "# Using Porter Stemmer\n",
    "porter_stemmed_words = [porter_st.stem(word) for word in b1_review_words]\n",
    "porter_freq = Counter(porter_stemmed_words)\n",
    "porter_common = porter_freq.most_common(10)\n",
    "print(porter_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "993edaec-02a4-4bef-9a21-c61302974a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('serv', 194), ('food', 132), ('nt', 129), ('plac', 120), ('pot', 112), ('ord', 102), ('soup', 97), ('hot', 93), ('wait', 80), ('good', 79)]\n"
     ]
    }
   ],
   "source": [
    "# Using Lancaster Stemmer\n",
    "lancaster_stemmed_words = [lancaster_st.stem(word) for word in b1_review_words]\n",
    "lancaster_freq = Counter(lancaster_stemmed_words)\n",
    "lancaster_common = lancaster_freq.most_common(10)\n",
    "print(lancaster_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "32ce69f4-074a-4ab0-ae2d-d6e466205781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 132), ('nt', 129), ('place', 120), ('servic', 116), ('pot', 109), ('order', 102), ('soup', 97), ('hot', 93), ('good', 79), ('tabl', 69)]\n"
     ]
    }
   ],
   "source": [
    "# Using Snowball Stemmer\n",
    "snow_stemmed_words = [snow_st.stem(word) for word in b1_review_words]\n",
    "snow_freq = Counter(snow_stemmed_words)\n",
    "snow_common = snow_freq.most_common(10)\n",
    "print(snow_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d6c54",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d4c24448",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sentences = reviews.sample(5, random_state=42)\n",
    "random_sentences = list(random_sentences['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c81cc594-a486-47fa-91bf-0f4043a6cd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Que ce soit pour leurs délicieux bubbles tea/smooties, leurs ''Bánh mì'' , leurs petits snacks (viennoiseries, tapioca, ...), on adore Vua et aussi leurs prix très abordables. On y retourne lorsqu'on est dans le Quartier Latin !\""
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c8155e95-94cb-490a-bbff-2db99e5b7cf0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Que', 'NNP'),\n",
       "  ('ce', 'NN'),\n",
       "  ('soit', 'VBD'),\n",
       "  ('pour', 'JJ'),\n",
       "  ('leurs', 'NNS'),\n",
       "  ('délicieux', 'VBP'),\n",
       "  ('bubbles', 'NNS'),\n",
       "  ('tea/smooties', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('leurs', 'VBZ'),\n",
       "  ('``', '``'),\n",
       "  ('Bánh', 'NNP'),\n",
       "  ('mì', 'NN'),\n",
       "  (\"''\", \"''\"),\n",
       "  (',', ','),\n",
       "  ('leurs', 'VBZ'),\n",
       "  ('petits', 'NNS'),\n",
       "  ('snacks', 'NNS'),\n",
       "  ('(', '('),\n",
       "  ('viennoiseries', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('tapioca', 'NN'),\n",
       "  (',', ','),\n",
       "  ('...', ':'),\n",
       "  (')', ')'),\n",
       "  (',', ','),\n",
       "  ('on', 'IN'),\n",
       "  ('adore', 'IN'),\n",
       "  ('Vua', 'NNP'),\n",
       "  ('et', 'CC'),\n",
       "  ('aussi', 'JJ'),\n",
       "  ('leurs', 'NNS'),\n",
       "  ('prix', 'VBP'),\n",
       "  ('très', 'JJ'),\n",
       "  ('abordables', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('On', 'IN'),\n",
       "  ('y', 'JJ'),\n",
       "  ('retourne', 'JJ'),\n",
       "  (\"lorsqu'on\", 'NN'),\n",
       "  ('est', 'JJS'),\n",
       "  ('dans', 'NNS'),\n",
       "  ('le', 'VBP'),\n",
       "  ('Quartier', 'NNP'),\n",
       "  ('Latin', 'NNP'),\n",
       "  ('!', '.')],\n",
       " [('As', 'IN'),\n",
       "  ('I', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('said', 'VBD'),\n",
       "  ('previously', 'RB'),\n",
       "  ('...', ':'),\n",
       "  ('we', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('been', 'VBN'),\n",
       "  ('coming', 'VBG'),\n",
       "  ('to', 'TO'),\n",
       "  ('LMAH', 'NNP'),\n",
       "  ('for', 'IN'),\n",
       "  ('over', 'IN'),\n",
       "  ('10', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('At', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('15', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('And', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('ALWAYS', 'NNP'),\n",
       "  ('seen', 'VBN'),\n",
       "  ('Dr.', 'NNP'),\n",
       "  ('White', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('bring', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('to', 'TO'),\n",
       "  ('have', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('urine', 'JJ'),\n",
       "  ('sample', 'NN'),\n",
       "  ('done', 'VBN'),\n",
       "  ('.', '.'),\n",
       "  ('While', 'IN'),\n",
       "  ('being', 'VBG'),\n",
       "  ('done', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('pee', 'NN'),\n",
       "  (\"'d\", 'MD'),\n",
       "  ('on', 'IN'),\n",
       "  ('himself', 'PRP'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('vet', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('staff', 'NN'),\n",
       "  ('only', 'RB'),\n",
       "  ('tried', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('clean', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('off', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('him', 'PRP'),\n",
       "  ('with', 'IN'),\n",
       "  ('alcohol', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('try', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('actually', 'RB'),\n",
       "  ('get', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('off', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('him', 'PRP'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('TELL', 'VB'),\n",
       "  ('ME', 'NNP'),\n",
       "  ('that', 'IN'),\n",
       "  ('he', 'PRP'),\n",
       "  ('pee', 'VBZ'),\n",
       "  (\"'d\", 'MD'),\n",
       "  ('all', 'DT'),\n",
       "  ('over', 'IN'),\n",
       "  ('his', 'PRP$'),\n",
       "  ('stomach', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('legs', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('So', 'RB'),\n",
       "  ('now', 'RB'),\n",
       "  ('I', 'PRP'),\n",
       "  ('e', 'VBP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('give', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('a', 'DT'),\n",
       "  ('bath', 'NN'),\n",
       "  ('because', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('ca', 'MD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('take', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('urine', 'JJ'),\n",
       "  ('sample', 'NN'),\n",
       "  ('properly', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('LIVID', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('Pretty', 'NNP'),\n",
       "  ('decent', 'NN'),\n",
       "  ('but', 'CC'),\n",
       "  ('so', 'RB'),\n",
       "  ('spacious', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Very', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('for', 'IN'),\n",
       "  ('kids', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('order', 'NN'),\n",
       "  ('dim', 'VB'),\n",
       "  ('sum', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  (\"'re\", 'VBP'),\n",
       "  ('open', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('Free', 'JJ'),\n",
       "  ('wifi', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('awesome', 'JJ'),\n",
       "  (':', ':'),\n",
       "  (')', ')')],\n",
       " [('Dumpling', 'VBG'),\n",
       "  ('Village', 'NNP'),\n",
       "  ('really', 'RB'),\n",
       "  ('suffering', 'VBG'),\n",
       "  ('from', 'IN'),\n",
       "  ('identity', 'NN'),\n",
       "  ('crisis', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('Is', 'VBZ'),\n",
       "  ('it', 'PRP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('Restaurant', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('mostly', 'RB'),\n",
       "  ('Northern', 'NNP'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('items', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('menu', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('Is', 'VBZ'),\n",
       "  ('it', 'PRP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Northern', 'NNP'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('restaurant', 'NN'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('dishes', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('menu', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('pork', 'NN'),\n",
       "  ('bone', 'NN'),\n",
       "  ('soup', 'NN'),\n",
       "  (',', ','),\n",
       "  ('cold', 'JJ'),\n",
       "  ('wheat', 'NN'),\n",
       "  ('bucknoodle', 'NN'),\n",
       "  ('soup', 'NN'),\n",
       "  (',', ','),\n",
       "  ('kimchi', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bean', 'NN'),\n",
       "  ('sprout', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('appetitizers', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('only', 'JJ'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('characters', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('3', 'CD'),\n",
       "  ('on', 'IN'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('business', 'NN'),\n",
       "  ('sign', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('front', 'NN'),\n",
       "  ('..', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('rest', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('in', 'IN'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('English', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('All', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('wait', 'NN'),\n",
       "  ('staffs', 'NNS'),\n",
       "  ('here', 'RB'),\n",
       "  ('speak', 'VBP'),\n",
       "  ('Mandarian', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('even', 'RB'),\n",
       "  ('most', 'JJS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('patrons', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('Chinese', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('It', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('located', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('really', 'RB'),\n",
       "  ('tiny', 'JJ'),\n",
       "  ('strip', 'NN'),\n",
       "  ('mall', 'NN'),\n",
       "  ('..', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('limited', 'JJ'),\n",
       "  ('parkings', 'NNS'),\n",
       "  ('!', '.'),\n",
       "  ('To', 'TO'),\n",
       "  ('tell', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('the', 'DT'),\n",
       "  ('truth', 'NN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('area', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('over', 'IN'),\n",
       "  ('10', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('I', 'PRP'),\n",
       "  ('never', 'RB'),\n",
       "  ('notice', 'VBP'),\n",
       "  ('it', 'PRP'),\n",
       "  ('until', 'IN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('read', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('reviews', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('yelp', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('around', 'RB'),\n",
       "  ('6', 'CD'),\n",
       "  ('pm', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('Saturday', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('restaurant', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('almost', 'RB'),\n",
       "  ('full', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('tables', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('packed', 'VBN'),\n",
       "  ('together', 'RB'),\n",
       "  (',', ','),\n",
       "  ('so', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('still', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('certain', 'JJ'),\n",
       "  ('degree', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('privacy', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('tell', 'VB'),\n",
       "  ('what', 'WP'),\n",
       "  ('the', 'DT'),\n",
       "  ('table', 'NN'),\n",
       "  ('next', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('me', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('eating', 'VBG'),\n",
       "  ('.', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('decided', 'VBD'),\n",
       "  ('on', 'IN'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('favourite', 'JJ'),\n",
       "  ('chives', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('pork', 'NN'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('kimchi', 'NNS'),\n",
       "  ('fried', 'VBD'),\n",
       "  ('rice', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('Korean', 'NNP'),\n",
       "  ('cold', 'VBD'),\n",
       "  ('bucknoodle', 'JJ'),\n",
       "  ('soup', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('$', '$'),\n",
       "  ('4.99', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('noodle', 'JJ'),\n",
       "  ('soup', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('so', 'RB'),\n",
       "  ('so', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Nice', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('refreshing', 'VBG'),\n",
       "  ('!', '.'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('based', 'VBN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('sour', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('really', 'RB'),\n",
       "  ('stimulate', 'VB'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('tastebuds', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('just', 'RB'),\n",
       "  ('make', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('want', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('eat', 'VB'),\n",
       "  ('more', 'JJR'),\n",
       "  ('!', '.'),\n",
       "  ('(', '('),\n",
       "  ('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('finish', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('myself', 'PRP'),\n",
       "  ('!', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('want', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('share', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('want', 'VBP'),\n",
       "  ('one', 'CD'),\n",
       "  ('all', 'DT'),\n",
       "  ('for', 'IN'),\n",
       "  ('myself', 'PRP'),\n",
       "  ('next', 'JJ'),\n",
       "  ('time', 'NN'),\n",
       "  ('!', '.'),\n",
       "  (')', ')'),\n",
       "  ('The', 'DT'),\n",
       "  ('kimchi', 'NN'),\n",
       "  ('fried', 'VBD'),\n",
       "  ('rice', 'NN'),\n",
       "  ('..', 'NNP'),\n",
       "  ('hmmmmm', 'NN'),\n",
       "  ('not', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('A', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('oily', 'JJ'),\n",
       "  ('side', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('much', 'JJ'),\n",
       "  ('taste', 'NN'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('spicy', 'VB'),\n",
       "  ('enough', 'RB'),\n",
       "  ('?', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('could', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('finish', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('and', 'CC'),\n",
       "  ('end', 'VB'),\n",
       "  ('up', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('pack', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('to', 'TO'),\n",
       "  ('go', 'VB'),\n",
       "  ('...', ':'),\n",
       "  ('(', '('),\n",
       "  ('not', 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('sure', 'JJ'),\n",
       "  ('I', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('eat', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  (',', ','),\n",
       "  ('but', 'CC'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('just', 'RB'),\n",
       "  ('let', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('go', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('waste', 'NN'),\n",
       "  (')', ')'),\n",
       "  ('The', 'DT'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('last', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('really', 'RB'),\n",
       "  ('worth', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('wait', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('14', 'CD'),\n",
       "  ('big', 'JJ'),\n",
       "  ('fat', 'NN'),\n",
       "  ('hot', 'JJ'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('filled', 'VBN'),\n",
       "  ('up', 'RP'),\n",
       "  ('the', 'DT'),\n",
       "  ('whole', 'JJ'),\n",
       "  ('steamer', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('Yes', 'UH'),\n",
       "  ('I', 'PRP'),\n",
       "  ('counted', 'VBD'),\n",
       "  ('them', 'PRP'),\n",
       "  ('14', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('them', 'PRP'),\n",
       "  ('for', 'IN'),\n",
       "  ('$', '$'),\n",
       "  ('4.99', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('freshly', 'RB'),\n",
       "  ('made', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('freshly', 'RB'),\n",
       "  ('steamed', 'VBN'),\n",
       "  ('!', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('skin', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('nice', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('thin', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('chewy', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('thick', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Oh', 'NNP'),\n",
       "  ('so', 'RB'),\n",
       "  ('delicious', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('One', 'CD'),\n",
       "  ('bite', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('ozzing', 'VBG'),\n",
       "  ('out', 'RP'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('if', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('burning', 'VBG'),\n",
       "  ('hot', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('just', 'RB'),\n",
       "  ('stuff', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('whole', 'JJ'),\n",
       "  ('thing', 'NN'),\n",
       "  ('into', 'IN'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('mouth', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('let', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('favour', 'NN'),\n",
       "  ('just', 'RB'),\n",
       "  ('explosed', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('mouth', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('Just', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('warning', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('If', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('going', 'VBG'),\n",
       "  ('out', 'RP'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('date', 'NN'),\n",
       "  (',', ','),\n",
       "  ('please', 'VB'),\n",
       "  ('do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('come', 'VB'),\n",
       "  ('here', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('Even', 'RB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('good', 'JJ'),\n",
       "  ('place', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('cheap', 'JJ'),\n",
       "  ('date', 'NN'),\n",
       "  (',', ','),\n",
       "  ('our', 'PRP$'),\n",
       "  ('bill', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('$', '$'),\n",
       "  ('18', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('2', 'CD'),\n",
       "  ('and', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('full', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('place', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('noisy', 'RB'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('food', 'NN'),\n",
       "  ('too', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('date', 'NN'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('too', 'RB'),\n",
       "  ('busy', 'JJ'),\n",
       "  ('eating', 'VBG'),\n",
       "  ('and', 'CC'),\n",
       "  ('just', 'RB'),\n",
       "  ('ignore', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('!', '.'),\n",
       "  ('Or', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('keep', 'VB'),\n",
       "  ('eating', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('eating', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('freak', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('friends', 'NNS'),\n",
       "  ('out', 'RP'),\n",
       "  ('!', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('services', 'NNS'),\n",
       "  ('here', 'RB'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('but', 'CC'),\n",
       "  ('efficient', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('given', 'VBN'),\n",
       "  ('extra', 'JJ'),\n",
       "  ('napkins', 'NNS'),\n",
       "  ('without', 'IN'),\n",
       "  ('requesting', 'VBG'),\n",
       "  ('...', ':'),\n",
       "  ('Maybe', 'RB'),\n",
       "  ('we', 'PRP'),\n",
       "  ('looked', 'VBD'),\n",
       "  ('really', 'RB'),\n",
       "  ('messy', 'VBN'),\n",
       "  ('?', '.'),\n",
       "  ('?', '.'),\n",
       "  ('Cash', 'NNP'),\n",
       "  ('only', 'RB'),\n",
       "  ('!', '.')],\n",
       " [('best', 'JJS'),\n",
       "  ('pizza', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('south', 'JJ'),\n",
       "  ('side', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('huge', 'JJ'),\n",
       "  ('drink', 'NN'),\n",
       "  ('selection', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('awesome', 'VB'),\n",
       "  ('atmosphere', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('$', '$'),\n",
       "  ('4', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('slice', 'NN'),\n",
       "  ('that', 'WDT'),\n",
       "  ('could', 'MD'),\n",
       "  ('easily', 'RB'),\n",
       "  ('feed', 'VB'),\n",
       "  ('2', 'CD'),\n",
       "  ('people', 'NNS'),\n",
       "  ('!', '.')]]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tagged = []\n",
    "for sentence in random_sentences:\n",
    "    nltk_tagged.append((nltk.pos_tag(word_tokenize(sentence))))\n",
    "nltk_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5d874c6c-1f0f-4232-b819-693f49a971a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que      PROPN \n",
      "ce       PROPN \n",
      "soit     ADJ   \n",
      "pour     NOUN  \n",
      "leurs    VERB  \n",
      "délicieux NOUN  \n",
      "bubbles  NOUN  \n",
      "tea      NOUN  \n",
      "/        SYM   \n",
      "smooties NOUN  \n",
      ",        PUNCT \n",
      "leurs    VERB  \n",
      "''       PUNCT \n",
      "Bánh     PROPN \n",
      "mì       INTJ  \n",
      "''       PUNCT \n",
      ",        PUNCT \n",
      "leurs    NOUN  \n",
      "petits   VERB  \n",
      "snacks   NOUN  \n",
      "(        PUNCT \n",
      "viennoiseries NOUN  \n",
      ",        PUNCT \n",
      "tapioca  INTJ  \n",
      ",        PUNCT \n",
      "...      PUNCT \n",
      ")        PUNCT \n",
      ",        PUNCT \n",
      "on       ADP   \n",
      "adore    PROPN \n",
      "Vua      PROPN \n",
      "et       PROPN \n",
      "aussi    PROPN \n",
      "leurs    VERB  \n",
      "prix     NOUN  \n",
      "très     ADJ   \n",
      "abordables NOUN  \n",
      ".        PUNCT \n",
      "On       ADP   \n",
      "y        PROPN \n",
      "retourne VERB  \n",
      "lorsqu'on PROPN \n",
      "est      PROPN \n",
      "dans     PROPN \n",
      "le       X     \n",
      "Quartier PROPN \n",
      "Latin    PROPN \n",
      "!        PUNCT \n",
      "As       ADP   \n",
      "I        PRON  \n",
      "'ve      AUX   \n",
      "said     VERB  \n",
      "previously ADV   \n",
      "...      PUNCT \n",
      "we've    PROPN \n",
      "been     AUX   \n",
      "coming   VERB  \n",
      "to       ADP   \n",
      "LMAH     PROPN \n",
      "for      ADP   \n",
      "over     ADP   \n",
      "10       NUM   \n",
      "years    NOUN  \n",
      ".        PUNCT \n",
      "At       ADV   \n",
      "least    ADJ   \n",
      "15       NUM   \n",
      ".        PUNCT \n",
      "And      CCONJ \n",
      "we       PRON  \n",
      "'ve      AUX   \n",
      "ALWAYS   ADV   \n",
      "seen     VERB  \n",
      "Dr.      PROPN \n",
      "White    PROPN \n",
      ".        PUNCT \n",
      "\n",
      "        SPACE \n",
      "I        PRON  \n",
      "had      VERB  \n",
      "to       PART  \n",
      "bring    VERB  \n",
      "my       PRON  \n",
      "cat      NOUN  \n",
      "in       ADP   \n",
      "to       PART  \n",
      "have     VERB  \n",
      "a        DET   \n",
      "urine    NOUN  \n",
      "sample   NOUN  \n",
      "done     VERB  \n",
      ".        PUNCT \n",
      "While    SCONJ \n",
      "being    AUX   \n",
      "done     VERB  \n",
      ",        PUNCT \n",
      "my       PRON  \n",
      "cat      NOUN  \n",
      "pee'd    ADV   \n",
      "on       ADP   \n",
      "himself  PRON  \n",
      ".        PUNCT \n",
      "The      DET   \n",
      "vet      NOUN  \n",
      "or       CCONJ \n",
      "staff    NOUN  \n",
      "only     ADV   \n",
      "tried    VERB  \n",
      "to       PART  \n",
      "clean    VERB  \n",
      "it       PRON  \n",
      "off      ADP   \n",
      "of       ADP   \n",
      "him      PRON  \n",
      "with     ADP   \n",
      "alcohol  NOUN  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "did      AUX   \n",
      "n't      PART  \n",
      "try      VERB  \n",
      "to       PART  \n",
      "actually ADV   \n",
      "get      VERB  \n",
      "it       PRON  \n",
      "off      ADP   \n",
      "of       ADP   \n",
      "him      PRON  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "did      AUX   \n",
      "n't      PART  \n",
      "even     ADV   \n",
      "TELL     VERB  \n",
      "ME       PRON  \n",
      "that     SCONJ \n",
      "he       PRON  \n",
      "pee'd    VERB  \n",
      "all      ADV   \n",
      "over     ADP   \n",
      "his      PRON  \n",
      "stomach  NOUN  \n",
      "and      CCONJ \n",
      "legs     NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "So       ADV   \n",
      "now      ADV   \n",
      "I        PRON  \n",
      "e        VERB  \n",
      "had      VERB  \n",
      "to       PART  \n",
      "give     VERB  \n",
      "my       PRON  \n",
      "cat      NOUN  \n",
      "a        DET   \n",
      "bath     NOUN  \n",
      "because  SCONJ \n",
      "they     PRON  \n",
      "ca       AUX   \n",
      "n't      PART  \n",
      "take     VERB  \n",
      "a        DET   \n",
      "urine    NOUN  \n",
      "sample   NOUN  \n",
      "properly ADV   \n",
      ".        PUNCT \n",
      "I        PRON  \n",
      "am       AUX   \n",
      "LIVID    PROPN \n",
      ".        PUNCT \n",
      "Pretty   ADV   \n",
      "decent   ADJ   \n",
      "but      CCONJ \n",
      "so       ADV   \n",
      "spacious ADJ   \n",
      "!        PUNCT \n",
      "         SPACE \n",
      "Very     ADV   \n",
      "good     ADJ   \n",
      "for      ADP   \n",
      "kids     NOUN  \n",
      "and      CCONJ \n",
      "you      PRON  \n",
      "can      AUX   \n",
      "order    VERB  \n",
      "dim      NOUN  \n",
      "sum      NOUN  \n",
      "!        PUNCT \n",
      "They     PRON  \n",
      "'re      VERB  \n",
      "open     ADJ   \n",
      "!        PUNCT \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Free     ADJ   \n",
      "wifi     NOUN  \n",
      "and      CCONJ \n",
      "awesome  ADJ   \n",
      ":)       PUNCT \n",
      "Dumpling PROPN \n",
      "Village  PROPN \n",
      "really   ADV   \n",
      "suffering VERB  \n",
      "from     ADP   \n",
      "identity NOUN  \n",
      "crisis   NOUN  \n",
      "!        PUNCT \n",
      "!        PUNCT \n",
      "Is       AUX   \n",
      "it       PRON  \n",
      "a        DET   \n",
      "Korean   PROPN \n",
      "Restaurant PROPN \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "really   ADV   \n",
      ".        PUNCT \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "mostly   ADV   \n",
      "Northern ADJ   \n",
      "Chinese  ADJ   \n",
      "items    NOUN  \n",
      "on       ADP   \n",
      "the      DET   \n",
      "menu     NOUN  \n",
      "like     ADP   \n",
      "dumplings NOUN  \n",
      ".        PUNCT \n",
      "Is       AUX   \n",
      "it       PRON  \n",
      "a        DET   \n",
      "Northern ADJ   \n",
      "Chinese  ADJ   \n",
      "restaurant NOUN  \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "really   ADV   \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "a        DET   \n",
      "lot      NOUN  \n",
      "of       ADP   \n",
      "Korean   ADJ   \n",
      "dishes   NOUN  \n",
      "         SPACE \n",
      "in       ADP   \n",
      "the      DET   \n",
      "menu     NOUN  \n",
      "like     ADP   \n",
      "pork     NOUN  \n",
      "bone     NOUN  \n",
      "soup     NOUN  \n",
      ",        PUNCT \n",
      "cold     ADJ   \n",
      "wheat    NOUN  \n",
      "bucknoodle NOUN  \n",
      "soup     NOUN  \n",
      ",        PUNCT \n",
      "kimchi   PROPN \n",
      "and      CCONJ \n",
      "bean     NOUN  \n",
      "sprout   NOUN  \n",
      "as       ADP   \n",
      "appetitizers NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "only     ADJ   \n",
      "Korean   ADJ   \n",
      "characters NOUN  \n",
      "are      AUX   \n",
      "the      DET   \n",
      "3        NUM   \n",
      "on       ADP   \n",
      "their    PRON  \n",
      "business NOUN  \n",
      "sign     NOUN  \n",
      "in       ADP   \n",
      "the      DET   \n",
      "front    NOUN  \n",
      "..       PUNCT \n",
      "and      CCONJ \n",
      "all      DET   \n",
      "the      DET   \n",
      "rest     NOUN  \n",
      "are      VERB  \n",
      "in       ADP   \n",
      "Chinese  PROPN \n",
      "and      CCONJ \n",
      "English  PROPN \n",
      ".        PUNCT \n",
      "All      DET   \n",
      "the      DET   \n",
      "wait     ADJ   \n",
      "staffs   NOUN  \n",
      "here     ADV   \n",
      "speak    VERB  \n",
      "Mandarian PROPN \n",
      "and      CCONJ \n",
      "even     ADV   \n",
      "most     ADJ   \n",
      "of       ADP   \n",
      "the      DET   \n",
      "patrons  NOUN  \n",
      "are      VERB  \n",
      "Chinese  ADJ   \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "It       PRON  \n",
      "is       AUX   \n",
      "located  VERB  \n",
      "in       ADP   \n",
      "a        DET   \n",
      "really   ADV   \n",
      "tiny     ADJ   \n",
      "strip    NOUN  \n",
      "mall     NOUN  \n",
      "..       PUNCT \n",
      "with     ADP   \n",
      "limited  ADJ   \n",
      "parkings NOUN  \n",
      "!        PUNCT \n",
      "To       PART  \n",
      "tell     VERB  \n",
      "you      PRON  \n",
      "the      DET   \n",
      "truth    NOUN  \n",
      "I        PRON  \n",
      "am       VERB  \n",
      "in       ADP   \n",
      "the      DET   \n",
      "area     NOUN  \n",
      "for      ADP   \n",
      "over     ADP   \n",
      "10       NUM   \n",
      "years    NOUN  \n",
      "I        PRON  \n",
      "never    ADV   \n",
      "notice   VERB  \n",
      "it       PRON  \n",
      "until    ADP   \n",
      "I        PRON  \n",
      "read     VERB  \n",
      "the      DET   \n",
      "reviews  NOUN  \n",
      "on       ADP   \n",
      "yelp     NOUN  \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "We       PRON  \n",
      "arrived  VERB  \n",
      "around   ADV   \n",
      "6        NUM   \n",
      "pm       NOUN  \n",
      "on       ADP   \n",
      "a        DET   \n",
      "Saturday PROPN \n",
      ".        PUNCT \n",
      "The      DET   \n",
      "restaurant NOUN  \n",
      "was      AUX   \n",
      "almost   ADV   \n",
      "full     ADJ   \n",
      ".        PUNCT \n",
      "         SPACE \n",
      "The      DET   \n",
      "tables   NOUN  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "really   ADV   \n",
      "packed   VERB  \n",
      "together ADV   \n",
      ",        PUNCT \n",
      "so       ADV   \n",
      "you      PRON  \n",
      "still    ADV   \n",
      "have     VERB  \n",
      "a        DET   \n",
      "certain  ADJ   \n",
      "degree   NOUN  \n",
      "of       ADP   \n",
      "privacy  NOUN  \n",
      "at       ADP   \n",
      "least    ADJ   \n",
      "I        PRON  \n",
      "can      AUX   \n",
      "not      PART  \n",
      "tell     VERB  \n",
      "what     PRON  \n",
      "the      DET   \n",
      "table    NOUN  \n",
      "next     ADV   \n",
      "to       ADP   \n",
      "me       PRON  \n",
      "were     AUX   \n",
      "eating   VERB  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "We       PRON  \n",
      "decided  VERB  \n",
      "on       ADP   \n",
      "my       PRON  \n",
      "favourite ADJ   \n",
      "chives   NOUN  \n",
      "and      CCONJ \n",
      "pork     NOUN  \n",
      "dumplings NOUN  \n",
      ",        PUNCT \n",
      "kimchi   PROPN \n",
      "fried    VERB  \n",
      "rice     NOUN  \n",
      "and      CCONJ \n",
      "Korean   ADJ   \n",
      "cold     ADJ   \n",
      "bucknoodle NOUN  \n",
      "soup     NOUN  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "are      AUX   \n",
      "all      ADV   \n",
      "$        SYM   \n",
      "4.99     NUM   \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "cold     ADJ   \n",
      "noodle   NOUN  \n",
      "soup     NOUN  \n",
      "is       AUX   \n",
      "so       ADV   \n",
      "so       ADV   \n",
      "good     ADJ   \n",
      "!        PUNCT \n",
      "Nice     ADJ   \n",
      "and      CCONJ \n",
      "refreshing ADJ   \n",
      "!        PUNCT \n",
      "and      CCONJ \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "based    VERB  \n",
      "is       AUX   \n",
      "a        DET   \n",
      "little   ADJ   \n",
      "sour     ADJ   \n",
      "and      CCONJ \n",
      "it       PRON  \n",
      "really   ADV   \n",
      "stimulate VERB  \n",
      "your     PRON  \n",
      "tastebuds NOUN  \n",
      "and      CCONJ \n",
      "just     ADV   \n",
      "make     VERB  \n",
      "you      PRON  \n",
      "want     VERB  \n",
      "to       PART  \n",
      "eat      VERB  \n",
      "more     ADJ   \n",
      "!        PUNCT \n",
      "(        PUNCT \n",
      "I        PRON  \n",
      "can      AUX   \n",
      "finish   VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "myself   PRON  \n",
      "!        PUNCT \n",
      "I        PRON  \n",
      "do       AUX   \n",
      "n't      PART  \n",
      "want     VERB  \n",
      "to       PART  \n",
      "share    VERB  \n",
      "!        PUNCT \n",
      "I        PRON  \n",
      "want     VERB  \n",
      "one      NUM   \n",
      "all      DET   \n",
      "for      ADP   \n",
      "myself   PRON  \n",
      "next     ADJ   \n",
      "time     NOUN  \n",
      "!        PUNCT \n",
      ")        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "kimchi   PROPN \n",
      "fried    VERB  \n",
      "rice     NOUN  \n",
      "..       PUNCT \n",
      "hmmmmm   ADV   \n",
      "not      PART  \n",
      "good     ADJ   \n",
      "!        PUNCT \n",
      "A        DET   \n",
      "little   ADJ   \n",
      "on       ADP   \n",
      "the      DET   \n",
      "oily     ADJ   \n",
      "side     NOUN  \n",
      ".        PUNCT \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "much     ADJ   \n",
      "taste    NOUN  \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "spicy    ADJ   \n",
      "enough   ADV   \n",
      "?        PUNCT \n",
      "We       PRON  \n",
      "could    AUX   \n",
      "not      PART  \n",
      "finish   VERB  \n",
      "it       PRON  \n",
      "and      CCONJ \n",
      "end      VERB  \n",
      "up       ADP   \n",
      "have     VERB  \n",
      "to       PART  \n",
      "pack     VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "to       PART  \n",
      "go       VERB  \n",
      "...      PUNCT \n",
      "(not     INTJ  \n",
      "even     ADV   \n",
      "sure     ADJ   \n",
      "I        PRON  \n",
      "will     AUX   \n",
      "eat      VERB  \n",
      "it       PRON  \n",
      ",        PUNCT \n",
      "but      CCONJ \n",
      "can      AUX   \n",
      "not      PART  \n",
      "just     ADV   \n",
      "let      VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "go       VERB  \n",
      "to       AUX   \n",
      "waste    VERB  \n",
      ")        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "dumplings NOUN  \n",
      "arrived  VERB  \n",
      "last     ADJ   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "it       PRON  \n",
      "really   ADV   \n",
      "worth    ADJ   \n",
      "the      DET   \n",
      "wait     NOUN  \n",
      "!        PUNCT \n",
      "14       NUM   \n",
      "big      ADJ   \n",
      "fat      ADJ   \n",
      "hot      ADJ   \n",
      "dumplings NOUN  \n",
      "filled   VERB  \n",
      "up       ADP   \n",
      "the      DET   \n",
      "whole    ADJ   \n",
      "steamer  NOUN  \n",
      "!        PUNCT \n",
      "Yes      INTJ  \n",
      "I        PRON  \n",
      "counted  VERB  \n",
      "them     PRON  \n",
      "14       NUM   \n",
      "of       ADP   \n",
      "them     PRON  \n",
      "for      ADP   \n",
      "$        SYM   \n",
      "4.99     NUM   \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "are      AUX   \n",
      "all      DET   \n",
      "freshly  ADV   \n",
      "made     VERB  \n",
      ",        PUNCT \n",
      "freshly  ADV   \n",
      "steamed  ADJ   \n",
      "!        PUNCT \n",
      "The      DET   \n",
      "skin     NOUN  \n",
      "is       AUX   \n",
      "nice     ADJ   \n",
      "and      CCONJ \n",
      "thin     ADJ   \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "chewy    NOUN  \n",
      "and      CCONJ \n",
      "thick    ADJ   \n",
      "!        PUNCT \n",
      "Oh       INTJ  \n",
      "so       ADV   \n",
      "delicious ADJ   \n",
      "!        PUNCT \n",
      "One      NUM   \n",
      "bite     NOUN  \n",
      "and      CCONJ \n",
      "all      DET   \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "are      AUX   \n",
      "ozzing   VERB  \n",
      "out      ADP   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "if       SCONJ \n",
      "they     PRON  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "burning  VERB  \n",
      "hot      ADJ   \n",
      ",        PUNCT \n",
      "you      PRON  \n",
      "can      AUX   \n",
      "just     ADV   \n",
      "stuff    VERB  \n",
      "the      DET   \n",
      "whole    ADJ   \n",
      "thing    NOUN  \n",
      "into     ADP   \n",
      "your     PRON  \n",
      "mouth    NOUN  \n",
      "and      CCONJ \n",
      "let      VERB  \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "and      CCONJ \n",
      "favour   NOUN  \n",
      "just     ADV   \n",
      "explosed VERB  \n",
      "in       ADP   \n",
      "your     PRON  \n",
      "mouth    NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Just     ADV   \n",
      "a        DET   \n",
      "warning  NOUN  \n",
      ".        PUNCT \n",
      "If       SCONJ \n",
      "you      PRON  \n",
      "going    VERB  \n",
      "out      ADP   \n",
      "on       ADP   \n",
      "a        DET   \n",
      "date     NOUN  \n",
      ",        PUNCT \n",
      "please   INTJ  \n",
      "do       AUX   \n",
      "n't      PART  \n",
      "come     VERB  \n",
      "here     ADV   \n",
      ".        PUNCT \n",
      "Even     ADV   \n",
      "it       PRON  \n",
      "is       VERB  \n",
      "a        DET   \n",
      "good     ADJ   \n",
      "place    NOUN  \n",
      "for      ADP   \n",
      "a        DET   \n",
      "cheap    ADJ   \n",
      "date     NOUN  \n",
      ",        PUNCT \n",
      "our      PRON  \n",
      "bill     NOUN  \n",
      "was      AUX   \n",
      "$        SYM   \n",
      "18       NUM   \n",
      "for      ADP   \n",
      "2        NUM   \n",
      "and      CCONJ \n",
      "we       PRON  \n",
      "were     VERB  \n",
      "full     ADJ   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "the      DET   \n",
      "place    NOUN  \n",
      "can      AUX   \n",
      "be       VERB  \n",
      "noisy    ADJ   \n",
      ",        PUNCT \n",
      "the      DET   \n",
      "food     NOUN  \n",
      "too      ADV   \n",
      "good     ADJ   \n",
      "and      CCONJ \n",
      "your     PRON  \n",
      "date     NOUN  \n",
      "will     AUX   \n",
      "be       VERB  \n",
      "too      ADV   \n",
      "busy     ADJ   \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "just     ADV   \n",
      "ignore   VERB  \n",
      "you      PRON  \n",
      "!        PUNCT \n",
      "Or       CCONJ \n",
      "you      PRON  \n",
      "will     AUX   \n",
      "keep     VERB  \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "you      PRON  \n",
      "freak    VERB  \n",
      "all      DET   \n",
      "your     PRON  \n",
      "friends  NOUN  \n",
      "out      ADP   \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "services NOUN  \n",
      "here     ADV   \n",
      "is       AUX   \n",
      "cold     ADJ   \n",
      "but      CCONJ \n",
      "efficient ADJ   \n",
      ".        PUNCT \n",
      "We       PRON  \n",
      "were     AUX   \n",
      "given    VERB  \n",
      "extra    ADJ   \n",
      "napkins  NOUN  \n",
      "without  ADP   \n",
      "requesting VERB  \n",
      "...      PUNCT \n",
      "Maybe    ADV   \n",
      "we       PRON  \n",
      "looked   VERB  \n",
      "really   ADV   \n",
      "messy    ADJ   \n",
      "?        PUNCT \n",
      "?        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Cash     VERB  \n",
      "only     ADV   \n",
      "!        PUNCT \n",
      "best     ADJ   \n",
      "pizza    NOUN  \n",
      "in       ADP   \n",
      "south    ADJ   \n",
      "side     NOUN  \n",
      ".        PUNCT \n",
      "huge     ADJ   \n",
      "drink    NOUN  \n",
      "selection NOUN  \n",
      ".        PUNCT \n",
      "awesome  ADJ   \n",
      "atmosphere NOUN  \n",
      ".        PUNCT \n",
      "$        SYM   \n",
      "4        NUM   \n",
      "for      ADP   \n",
      "a        DET   \n",
      "slice    NOUN  \n",
      "that     DET   \n",
      "could    AUX   \n",
      "easily   ADV   \n",
      "feed     VERB  \n",
      "2        NUM   \n",
      "people   NOUN  \n",
      "!        PUNCT \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tagged = []\n",
    "for sentence in random_sentences:\n",
    "    spacy_tagged.append(nlp(sentence))\n",
    "for tagged in spacy_tagged:\n",
    "    for token in tagged:\n",
    "        print(f'{token.text:{8}} {token.pos_:{6}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627839e",
   "metadata": {},
   "source": [
    "# WORK COMPLETED UP TILL HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f44ab",
   "metadata": {},
   "source": [
    "### Writing Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "57662d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSOFurl():\n",
    "    while(True):\n",
    "        ip0 = 'stackoverflow'\n",
    "        ip1 = 'com'\n",
    "        ip2 = 'questions'\n",
    "        ip3 = str(random.randint(0, 100000000))\n",
    "        url = 'https://' + ip0 + '.' + ip1 + '/'+ ip2 + '/'+ ip3\n",
    "        try:\n",
    "            urlContent = urlopen(url).read()\n",
    "            if urlContent:\n",
    "                break\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    print(\"Found URL: \" + url)\n",
    "    page1 = requests.get(url)\n",
    "    soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "    text = list(soup1.find_all(\"p\"))\n",
    "    sof_text = [txt.get_text() for txt in text]\n",
    "   \n",
    "    return sof_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1ba56e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHWZurl():\n",
    "    url = 'https://www.hardwarezone.com.sg/home'\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "    urls = []\n",
    "    for link in soup.find_all('a'):\n",
    "        urls.append(link.get('href'))\n",
    "    random.shuffle(urls)\n",
    "    while(True):\n",
    "        for url in urls:\n",
    "            print(url)\n",
    "            try:\n",
    "                url = 'https://www.hardwarezone.com.sg' + url\n",
    "                urlContent = urlopen(url).read()\n",
    "                page1 = requests.get(url)\n",
    "                soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "                text = list(soup1.find_all(\"p\"))\n",
    "                hwz_text = [txt.get_text() for txt in text]\n",
    "                if urlContent and len(hwz_text)>10:\n",
    "                    break\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "        break\n",
    "    print(\"Found URL: \" + url)\n",
    "    return hwz_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "380f0b1c-e1d2-4e2e-a744-d92781fa7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "def getCNAurl():\n",
    "    url = 'https://www.channelnewsasia.com/'\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "    linked_urls = set()\n",
    "\n",
    "    def get_all_links(url):\n",
    "        urls = set()\n",
    "        domain_name = urlparse(url).netloc\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "        for a_tag in soup.findAll(\"a\"):\n",
    "            href = a_tag.attrs.get(\"href\")\n",
    "            if href == \"\" or href is None:\n",
    "                # href empty tag\n",
    "                continue\n",
    "            href = urljoin(url, href)\n",
    "            parsed_href = urlparse(href)\n",
    "            # remove URL GET parameters, URL fragments, etc.\n",
    "            href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "            if href in urls:\n",
    "                # already in the set\n",
    "                continue\n",
    "            if domain_name not in href:\n",
    "                # external link\n",
    "                continue\n",
    "            if len(href) > 60:\n",
    "                urls.add(href)\n",
    "        return urls\n",
    "    \n",
    "    urls_cna = get_all_links('https://www.channelnewsasia.com/')\n",
    "    url = random.sample(urls_cna, 1)\n",
    "    print(\"Found URL: \", url[0])\n",
    "    \n",
    "    page1 = requests.get(cna_text1)\n",
    "    soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "    text = list(soup1.find_all(\"p\"))\n",
    "    cna_text = [txt.get_text() for txt in text]\n",
    "   \n",
    "    return cna_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb97780",
   "metadata": {},
   "source": [
    "#### Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bf0b4c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n",
      "Found URL: https://stackoverflow.com/questions/68501807\n",
      "Found URL: https://stackoverflow.com/questions/62830548\n"
     ]
    }
   ],
   "source": [
    "sof_text1 = getSOFurl()\n",
    "sof_text2 = getSOFurl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d2104397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/features/list/30161-gallery\n",
      "/features/list/37763-deals\n",
      "https://forums.hardwarezone.com.sg\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "/tech-news/list\n",
      "/product-guide/23923-wearables/home\n",
      "/priceLists\n",
      "/tech-news-tpg-reveals-plans-trial-5g-nsa-year\n",
      "Found URL: https://www.hardwarezone.com.sg/tech-news-tpg-reveals-plans-trial-5g-nsa-year\n",
      "/product-guide/6580-projectors/home\n",
      "/product-guide/261-casings/home\n",
      "/priceLists\n",
      "/product-guide/195-motherboard/home\n",
      "/tech-news-thermaltake-pacific-pr32-d5-plus-reservoir-pump-combo-rgb-lighting\n",
      "Found URL: https://www.hardwarezone.com.sg/tech-news-thermaltake-pacific-pr32-d5-plus-reservoir-pump-combo-rgb-lighting\n"
     ]
    }
   ],
   "source": [
    "hwz_text1 = getHWZurl()\n",
    "hwz_text2 = getHWZurl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URL:  https://www.channelnewsasia.com/commentary/commentary-singapores-response-against-potential-air-threats-well-oiled-machinery-task-remains-challenging-2176066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colot\\AppData\\Local\\Temp/ipykernel_17396/3865290005.py:33: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  url = random.sample(urls_cna, 1)\n"
     ]
    },
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for '[\\'\\\\n\\\\n      World\\\\n  \\\\n\\', \\'\\\\n\\\\n      World\\\\n  \\\\n\\', \\'Seventh grade student Ryza Delos Santos, 10, works on her modules at home as her cousin observes, after a session at the makeshift rickshaw distance learning centre for the Aeta community in Porac, Pampanga, Philippines, Oct 12, 2020. (File photo: REUTERS/Eloisa Lopez)\\', \"MANILA: The United Nations children\\'s agency UNICEF has urged education authorities to reopen schools as soon as possible in countries where millions of students are still not allowed to return to classrooms 18 months into the COVID-19 pandemic.\", \\'Schools in around 17 countries remain fully closed, while those in 39 countries remain partially closed, according to a report released by UNICEF on Thursday (Sep 16).\\', \\'Among those \"almost completely closed\" are schools usually attended by nearly 77 million students in the Philippines, Bangladesh, Venezuela, Saudi Arabia, Panama and Kuwait.\\', \"Nearly a third of this figure is accounted for by the Philippines, which is fighting one of Asia\\'s worst COVID-19 outbreaks and where a new school year started this week.\", \\'Pupils from the six countries represent more than half of the 131 million students worldwide that have missed more than three-quarters of their in-person learning, UNICEF said.\\', \\'\"The education crisis is still here, and with each passing day that classrooms remain dark, the devastation worsens,\" said UNICEF Executive Director Henrietta Fore.\\', \\'The report said teachers should be prioritised for COVID-19 vaccines, after health workers and those most at risk, to protect them from community transmission.\\', \\'Students may be safer at home, but the availability of computers, mobile phones and Internet, and the uneven quality of education, are among challenges they continue to face.\\', \\'In the Philippines, some children have been forced to climb onto roofs just to get an Internet signal.\\', \\'In June, President Rodrigo Duterte rejected a proposal to allow face-to-face classes to resume in some areas, saying: \"I cannot gamble on the health of the children.\"\\', \"In a report released in April, the Asian Development Bank estimated school closures lasting more than a year could slash future earnings among the region\\'s students by as much as US$1.25 trillion, or equivalent to 5.4 per cent of GDP in 2020.\", \\'UNICEF and its partners will shut down their digital channels for 18 hours on Thursday to draw attention to the crisis and the \"18 months of lost learning\".\\', \\'\"This is a crisis we will not allow the world to ignore,\" UNICEF\\\\\\'s Fore said. \"Our channels are silent, but our message is loud: Every community, everywhere must reopen schools as soon as possible.\"\\', \\'\\\\xa0\\', \"\\\\n      This service is not intended for persons residing in the E.U. By clicking subscribe, I agree to receive news updates and promotional material from Mediacorp and Mediacorp\\'s partners. \\\\n  \", \\' Copyright© Mediacorp 2021. Mediacorp Pte Ltd. All rights reserved. \\', \"We know it\\'s a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be.\", \\'To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.\\', \\'Upgraded but still having issues? Contact us\\']'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17396/2649559054.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcna_text1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetCNAurl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcna_text2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetCNAurl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17396/3865290005.py\u001b[0m in \u001b[0;36mgetCNAurl\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Found URL: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mpage1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0msoup1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Get the appropriate adapter to use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[0madapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;31m# Start time (approximately) of the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget_adapter\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[1;31m# Nothing matches :-/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mInvalidSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No connection adapters were found for {!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidSchema\u001b[0m: No connection adapters were found for '[\\'\\\\n\\\\n      World\\\\n  \\\\n\\', \\'\\\\n\\\\n      World\\\\n  \\\\n\\', \\'Seventh grade student Ryza Delos Santos, 10, works on her modules at home as her cousin observes, after a session at the makeshift rickshaw distance learning centre for the Aeta community in Porac, Pampanga, Philippines, Oct 12, 2020. (File photo: REUTERS/Eloisa Lopez)\\', \"MANILA: The United Nations children\\'s agency UNICEF has urged education authorities to reopen schools as soon as possible in countries where millions of students are still not allowed to return to classrooms 18 months into the COVID-19 pandemic.\", \\'Schools in around 17 countries remain fully closed, while those in 39 countries remain partially closed, according to a report released by UNICEF on Thursday (Sep 16).\\', \\'Among those \"almost completely closed\" are schools usually attended by nearly 77 million students in the Philippines, Bangladesh, Venezuela, Saudi Arabia, Panama and Kuwait.\\', \"Nearly a third of this figure is accounted for by the Philippines, which is fighting one of Asia\\'s worst COVID-19 outbreaks and where a new school year started this week.\", \\'Pupils from the six countries represent more than half of the 131 million students worldwide that have missed more than three-quarters of their in-person learning, UNICEF said.\\', \\'\"The education crisis is still here, and with each passing day that classrooms remain dark, the devastation worsens,\" said UNICEF Executive Director Henrietta Fore.\\', \\'The report said teachers should be prioritised for COVID-19 vaccines, after health workers and those most at risk, to protect them from community transmission.\\', \\'Students may be safer at home, but the availability of computers, mobile phones and Internet, and the uneven quality of education, are among challenges they continue to face.\\', \\'In the Philippines, some children have been forced to climb onto roofs just to get an Internet signal.\\', \\'In June, President Rodrigo Duterte rejected a proposal to allow face-to-face classes to resume in some areas, saying: \"I cannot gamble on the health of the children.\"\\', \"In a report released in April, the Asian Development Bank estimated school closures lasting more than a year could slash future earnings among the region\\'s students by as much as US$1.25 trillion, or equivalent to 5.4 per cent of GDP in 2020.\", \\'UNICEF and its partners will shut down their digital channels for 18 hours on Thursday to draw attention to the crisis and the \"18 months of lost learning\".\\', \\'\"This is a crisis we will not allow the world to ignore,\" UNICEF\\\\\\'s Fore said. \"Our channels are silent, but our message is loud: Every community, everywhere must reopen schools as soon as possible.\"\\', \\'\\\\xa0\\', \"\\\\n      This service is not intended for persons residing in the E.U. By clicking subscribe, I agree to receive news updates and promotional material from Mediacorp and Mediacorp\\'s partners. \\\\n  \", \\' Copyright© Mediacorp 2021. Mediacorp Pte Ltd. All rights reserved. \\', \"We know it\\'s a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be.\", \\'To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.\\', \\'Upgraded but still having issues? Contact us\\']'"
     ]
    }
   ],
   "source": [
    "cna_text1 = getCNAurl()\n",
    "cna_text2 = getCNAurl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee8a1e",
   "metadata": {},
   "source": [
    "##### Cleaning SOF text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "cabf88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_text1 = sof_text1[4:]\n",
    "sof_text1 = sof_text1[:-10]\n",
    "temp_list = []\n",
    "for line in sof_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "sof_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5be9617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have a table where records have a (begin, end) time window of existence (for things like employement duration, birth and death, rent duration, ...)',\n",
       " 'begin IS NULL or end IS NULL if there is no bound.',\n",
       " 'I want column value to be unique at any point in time.',\n",
       " 'Is OK',\n",
       " 'Whereas',\n",
       " 'Is not OK, because both records have value=1 and overlapping time windows.',\n",
       " 'Is there a way to enforce such a constraint in SQL ?',\n",
       " \"You can't do this on the table, no, as there's nothing to make UNIQUE on.\",\n",
       " 'What you could do, however, is use a VIEW to enforce it.',\n",
       " \"Firstly, let's create your table.\",\n",
       " 'I assume the columns datetime, should actually be begin and end; I recommend against these names as they are reserved keywords.',\n",
       " 'As such I am calling them DateBegin and DateEnd.',\n",
       " 'I am also assuming that they are date only (no time portion) values and so define them as a date not a datetime:',\n",
       " 'And we\\'ll INSERT your first 2 rows, as they are \"ok\":',\n",
       " 'Now we need to make a VIEW, but we need one row per date.',\n",
       " \"As such you'll want to create a Calendar Table.\",\n",
       " \"I'm not going to cover how to create one here, but there are literally 100's of articles, such as there on SQL Server Central: Bones of SQL - The Calendar Table, Calendar Tables in T-SQL.\",\n",
       " 'Once you have your Calendar table, you can create the VIEW below, which JOINs the data in your table to the calendar table.',\n",
       " \"We're going to make it so that the VIEW just returns the columns value and the date.\",\n",
       " \"WE're also going to schemabind it; this means we'll be able to add an UNIQUE INDEX to it:\",\n",
       " 'Now we have a VIEW that has a row for each date, and for each value.',\n",
       " 'This means we can now create our UNIQUE INDEX:',\n",
       " \"Now if we try to INSERT a row that is on the same date and value, we'll get an error:\",\n",
       " \"Cannot insert duplicate key row in object 'dbo.MyView' with unique index 'MyIndex'.\",\n",
       " 'The duplicate key value is (1, 2021-07-23).',\n",
       " 'Trigger will help you, for example like this:',\n",
       " 'fiddle']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sof_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dc0f4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_text2 = sof_text2[4:]\n",
    "sof_text2 = sof_text2[:-10]\n",
    "temp_list = []\n",
    "for line in sof_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "sof_text2 = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fb7f9",
   "metadata": {},
   "source": [
    "##### Cleaning CNA text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n      World\\n  \\n',\n",
       " '\\n\\n      World\\n  \\n',\n",
       " 'Seventh grade student Ryza Delos Santos, 10, works on her modules at home as her cousin observes, after a session at the makeshift rickshaw distance learning centre for the Aeta community in Porac, Pampanga, Philippines, Oct 12, 2020. (File photo: REUTERS/Eloisa Lopez)',\n",
       " \"MANILA: The United Nations children's agency UNICEF has urged education authorities to reopen schools as soon as possible in countries where millions of students are still not allowed to return to classrooms 18 months into the COVID-19 pandemic.\",\n",
       " 'Schools in around 17 countries remain fully closed, while those in 39 countries remain partially closed, according to a report released by UNICEF on Thursday (Sep 16).',\n",
       " 'Among those \"almost completely closed\" are schools usually attended by nearly 77 million students in the Philippines, Bangladesh, Venezuela, Saudi Arabia, Panama and Kuwait.',\n",
       " \"Nearly a third of this figure is accounted for by the Philippines, which is fighting one of Asia's worst COVID-19 outbreaks and where a new school year started this week.\",\n",
       " 'Pupils from the six countries represent more than half of the 131 million students worldwide that have missed more than three-quarters of their in-person learning, UNICEF said.',\n",
       " '\"The education crisis is still here, and with each passing day that classrooms remain dark, the devastation worsens,\" said UNICEF Executive Director Henrietta Fore.',\n",
       " 'The report said teachers should be prioritised for COVID-19 vaccines, after health workers and those most at risk, to protect them from community transmission.',\n",
       " 'Students may be safer at home, but the availability of computers, mobile phones and Internet, and the uneven quality of education, are among challenges they continue to face.',\n",
       " 'In the Philippines, some children have been forced to climb onto roofs just to get an Internet signal.',\n",
       " 'In June, President Rodrigo Duterte rejected a proposal to allow face-to-face classes to resume in some areas, saying: \"I cannot gamble on the health of the children.\"',\n",
       " \"In a report released in April, the Asian Development Bank estimated school closures lasting more than a year could slash future earnings among the region's students by as much as US$1.25 trillion, or equivalent to 5.4 per cent of GDP in 2020.\",\n",
       " 'UNICEF and its partners will shut down their digital channels for 18 hours on Thursday to draw attention to the crisis and the \"18 months of lost learning\".',\n",
       " '\"This is a crisis we will not allow the world to ignore,\" UNICEF\\'s Fore said. \"Our channels are silent, but our message is loud: Every community, everywhere must reopen schools as soon as possible.\"',\n",
       " '\\xa0',\n",
       " \"\\n      This service is not intended for persons residing in the E.U. By clicking subscribe, I agree to receive news updates and promotional material from Mediacorp and Mediacorp's partners. \\n  \",\n",
       " ' Copyright© Mediacorp 2021. Mediacorp Pte Ltd. All rights reserved. ',\n",
       " \"We know it's a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be.\",\n",
       " 'To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.',\n",
       " 'Upgraded but still having issues? Contact us']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cna_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d0987",
   "metadata": {},
   "source": [
    "##### Cleaning HWZ text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5c822fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_text1 = hwz_text1[:-5]\n",
    "temp_list = []\n",
    "for line in hwz_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "hwz_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d96f688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_text2 = hwz_text2[:-5]\n",
    "temp_list = []\n",
    "for line in hwz_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "hwz_text2 = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11179763",
   "metadata": {},
   "source": [
    "##### First word in sentence capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "93702e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for sof_text1:  0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in sof_text1:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for sof_text1: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b7ebe5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for sof_text2:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in sof_text2:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for sof_text2: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "913e6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for hwz_text1:  1.0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in hwz_text1:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for hwz_text1: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d80943eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for hwz_text2:  0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in hwz_text2:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for hwz_text2: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for cna_text1:  0.6818181818181818\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in cna_text1:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for cna_text1: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9e31e9b",
   "metadata": {},
   "source": [
    "##### Length of articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1137d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentences in sof_text1:  27\n",
      "No of sentences in sof_text2:  15\n",
      "No of sentences in hwz_text1:  15\n",
      "No of sentences in hwz_text2:  21\n"
     ]
    }
   ],
   "source": [
    "print(\"No of sentences in sof_text1: \", len(sof_text1))\n",
    "print(\"No of sentences in sof_text2: \", len(sof_text2))\n",
    "print(\"No of sentences in hwz_text1: \", len(hwz_text1))\n",
    "print(\"No of sentences in hwz_text2: \", len(hwz_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90754d4",
   "metadata": {},
   "source": [
    "##### Proper nouns capitalised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in sof_text1:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in sof_text1: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in sof_text2:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in sof_text2: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a68814",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in hwz_text1:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in hwz_text1: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in hwz_text2:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in hwz_text2: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What kind of proper nouns used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stack Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_tagged = []\n",
    "sof_noun_dict = {}\n",
    "for sentence in sof_text1:\n",
    "    sof_tagged.append(nlp(sentence))\n",
    "for sentence in sof_text2:\n",
    "    sof_tagged.append(nlp(sentence))\n",
    "for tagged in sof_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in sof_noun_dict.keys():\n",
    "                sof_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                sof_noun_dict[token.text] = 1\n",
    "\n",
    "sof_noun_dict_sorted = sorted(sof_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sof_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_tagged = []\n",
    "hwz_noun_dict = {}\n",
    "for sentence in hwz_text1:\n",
    "    hwz_tagged.append(nlp(sentence))\n",
    "for sentence in hwz_text2:\n",
    "    hwz_tagged.append(nlp(sentence))\n",
    "for tagged in hwz_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in hwz_noun_dict.keys():\n",
    "                hwz_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                hwz_noun_dict[token.text] = 1\n",
    "\n",
    "hwz_noun_dict_sorted = sorted(hwz_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in hwz_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac262786",
   "metadata": {},
   "source": [
    "##### Good grammar?\n",
    "1. Tense matching\n",
    "2. Subject-verb agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subject-verb agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/pes10k/cs412-scorer/blob/master/agreement_utils.py\n",
    "singular_noun_tags = ('NN', 'NNP')\n",
    "plural_noun_tags = ('NNS', 'NNPS')\n",
    "\n",
    "noun_tags = singular_noun_tags + plural_noun_tags + ('PRP',)\n",
    "\n",
    "plural_verb_tags = ('VBZ',)\n",
    "singular_verb_tags = ('VBP',)\n",
    "general_verb_tags = ('VBD', 'VB')\n",
    "\n",
    "verb_tags = singular_verb_tags + plural_verb_tags + general_verb_tags\n",
    "\n",
    "plural_prop_nouns = ('they', 'we', 'them', 'themselves', 'us', 'those')\n",
    "singular_prop_nouns = ('he', 'she', 'i', 'him', 'me', 'myself', 'it')\n",
    "\n",
    "singulars = singular_noun_tags + singular_verb_tags + singular_prop_nouns\n",
    "plurals = plural_noun_tags + plural_verb_tags + plural_prop_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreed = 0\n",
    "disagreed = 0\n",
    "agree_pairs = []\n",
    "disagree_pairs = []\n",
    "for sentence in sof_text1:\n",
    "    sentence = sentence.lower().split()\n",
    "    tagged = nltk.pos_tag(sentence)\n",
    "    for index, word_tag_pair in enumerate(tagged):\n",
    "        if word_tag_pair[1] in singulars:\n",
    "            for j in range(index, len(tagged)):\n",
    "                if tagged[j][1] in singulars:\n",
    "                    agree_pairs.append((word_tag_pair[0], tagged[j][0]))\n",
    "                    agreed += 1\n",
    "                elif tagged[j][1] in plurals:\n",
    "                    disagree_pairs.append((word_tag_pair[0], tagged[j][0]))\n",
    "                    disagreed += 1\n",
    "        elif word_tag_pair[1] in plurals:\n",
    "            for j in range(index, len(tagged)):\n",
    "                if tagged[j][1] in singulars:\n",
    "                    disagree_pairs.append((word_tag_pair[0], tagged[j][0]))\n",
    "                    disagreed += 1\n",
    "                elif tagged[j][1] in plurals:\n",
    "                    agree_pairs.append((word_tag_pair[0], tagged[j][0]))\n",
    "                    agreed += 1\n",
    "print(agree_pairs)\n",
    "print(disagree_pairs)\n",
    "print(\"SVA ratio in sof_text1: \", agreed/(agreed+disagreed))\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So i learnt that this way is not good. i will attempt to try with spacy dependency tracker and then combine with nltk pos tagger to check if there is SVA. thx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is what i will do\n",
    "# 1. choose 6 good urls (2 from each source) so that there can be comparisons\n",
    "# 2. do the CNA preprocessing part\n",
    "# 3. continue with the two parts for checking for good grammar (any more ideas?)\n",
    "# 4. think n implement more things to analyse for writing style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
