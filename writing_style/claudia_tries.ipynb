{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "97e6fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from collections import Counter\n",
    "# import httplib2\n",
    "import itertools\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd \n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import urllib.request\n",
    "from urllib.request import urlopen, Request\n",
    "import random\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bc45d509-90f8-4db6-a063-13e17fad1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70f63a",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c2790af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_json('../data/reviewSelected100.json', encoding='ISO-8859-1', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e12df",
   "metadata": {},
   "source": [
    "## 3.2 Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14711fd",
   "metadata": {},
   "source": [
    "### Tokenisation and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "78f7f863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3101</th>\n",
       "      <td>kSylHWOYcVoo8UcLcYZFpg</td>\n",
       "      <td>gm3J6L35xYUMgwfrQOas9A</td>\n",
       "      <td>kAmNIcJpAZIZIp5uSMAYLA</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>BEWARE!!! DO NOT LIVE HERE!!! REPEAT DO NOT LI...</td>\n",
       "      <td>2015-08-06 20:48:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3108</th>\n",
       "      <td>nrVucC_UZQgPDaAdW6f-_w</td>\n",
       "      <td>OoyQYSeYNyRVOmdO3tsxYA</td>\n",
       "      <td>kAmNIcJpAZIZIp5uSMAYLA</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>PROS: Student living is the worst anywhere, bu...</td>\n",
       "      <td>2013-12-07 06:45:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>-o1FmGFSfe6mbDTYk-PHag</td>\n",
       "      <td>0Qku9zA5ytwzH6dYt2cg8Q</td>\n",
       "      <td>kAmNIcJpAZIZIp5uSMAYLA</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Rebel Place Apartments WAS a nice place to liv...</td>\n",
       "      <td>2015-07-03 23:24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>j3FmbADiyjW8AYqcnJG9Aw</td>\n",
       "      <td>Lb12DRCZgRuiKtlwC1-WAA</td>\n",
       "      <td>kAmNIcJpAZIZIp5uSMAYLA</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If I could give no stars, i would. the manager...</td>\n",
       "      <td>2018-09-03 05:28:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>4rTk9J67QTbtfKOVQxMBNQ</td>\n",
       "      <td>piQb_aEq6aimVfZAXyqmuA</td>\n",
       "      <td>kAmNIcJpAZIZIp5uSMAYLA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I lived here a few years ago and it was okay. ...</td>\n",
       "      <td>2017-05-26 15:12:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   review_id                 user_id             business_id  \\\n",
       "3101  kSylHWOYcVoo8UcLcYZFpg  gm3J6L35xYUMgwfrQOas9A  kAmNIcJpAZIZIp5uSMAYLA   \n",
       "3108  nrVucC_UZQgPDaAdW6f-_w  OoyQYSeYNyRVOmdO3tsxYA  kAmNIcJpAZIZIp5uSMAYLA   \n",
       "3127  -o1FmGFSfe6mbDTYk-PHag  0Qku9zA5ytwzH6dYt2cg8Q  kAmNIcJpAZIZIp5uSMAYLA   \n",
       "3142  j3FmbADiyjW8AYqcnJG9Aw  Lb12DRCZgRuiKtlwC1-WAA  kAmNIcJpAZIZIp5uSMAYLA   \n",
       "3150  4rTk9J67QTbtfKOVQxMBNQ  piQb_aEq6aimVfZAXyqmuA  kAmNIcJpAZIZIp5uSMAYLA   \n",
       "\n",
       "      stars  useful  funny  cool  \\\n",
       "3101      1      18      3     3   \n",
       "3108      2      11      7     3   \n",
       "3127      1      10      3     1   \n",
       "3142      1       2      0     0   \n",
       "3150      3       0      0     0   \n",
       "\n",
       "                                                   text                date  \n",
       "3101  BEWARE!!! DO NOT LIVE HERE!!! REPEAT DO NOT LI... 2015-08-06 20:48:47  \n",
       "3108  PROS: Student living is the worst anywhere, bu... 2013-12-07 06:45:50  \n",
       "3127  Rebel Place Apartments WAS a nice place to liv... 2015-07-03 23:24:05  \n",
       "3142  If I could give no stars, i would. the manager... 2018-09-03 05:28:38  \n",
       "3150  I lived here a few years ago and it was okay. ... 2017-05-26 15:12:32  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get reviews for a random business \n",
    "random_business = reviews.sample()\n",
    "random_business_id = random_business.iloc[0]['business_id']\n",
    "small_business_dataset = reviews.loc[reviews['business_id'] == random_business_id]\n",
    "small_business_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c91d93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_business_dataset_reviews = list(small_business_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0669b8f6-714b-4708-840b-2c65c03f97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the reviews into a concatenated string \n",
    "b1_review = ''.join(small_business_dataset_reviews)\n",
    "clean_review = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", b1_review)\n",
    "b1_review = nlp(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "deaa0847-5dec-4317-b9f5-ece17562bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 866), ('and', 678), ('to', 668), ('I', 530), ('a', 496), ('in', 382), ('is', 308), ('of', 307), ('you', 287), ('that', 284)]\n"
     ]
    }
   ],
   "source": [
    "# removed punctuation and get the top 10 most common words (including stopwords)\n",
    "b1_review_words = [token.text for token in b1_review if token.is_alpha == True] \n",
    "b1_word_freq = Counter(b1_review_words)\n",
    "common_words = b1_word_freq.most_common(10)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ede74a1b-4e6c-4436-8c3b-df2f479aad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nt', 175), ('place', 151), ('apartment', 82), ('management', 79), ('room', 60), ('people', 58), ('Rebel', 56), ('moved', 54), ('office', 53), ('living', 51)]\n"
     ]
    }
   ],
   "source": [
    "# removed punctuation and get the top 10 most common words (excluding stopwords)\n",
    "b1_review_words = [token.text for token in b1_review if token.is_stop != True and token.is_alpha == True] \n",
    "b1_word_freq = Counter(b1_review_words)\n",
    "common_words = b1_word_freq.most_common(10)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "82e4306f-2536-450b-ba7d-cc0f073dfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot log graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "eeb35a0e-7b64-4e9a-9d53-e89a299e51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do some stemming after removing the stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "porter_st = PorterStemmer()\n",
    "lancaster_st = LancasterStemmer()\n",
    "snow_st = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bfe90886-30ac-4282-978f-f66739952d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('place', 208), ('nt', 175), ('live', 128), ('manag', 115), ('apart', 113), ('move', 94), ('rebel', 83), ('room', 76), ('time', 72), ('peopl', 67)]\n"
     ]
    }
   ],
   "source": [
    "# Using Porter Stemmer\n",
    "porter_stemmed_words = [porter_st.stem(word) for word in b1_review_words]\n",
    "porter_freq = Counter(porter_stemmed_words)\n",
    "porter_common = porter_freq.most_common(10)\n",
    "print(porter_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "993edaec-02a4-4bef-9a21-c61302974a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('plac', 208), ('nt', 175), ('room', 132), ('liv', 129), ('man', 119), ('apart', 113), ('mov', 94), ('rebel', 83), ('tim', 72), ('peopl', 67)]\n"
     ]
    }
   ],
   "source": [
    "# Using Lancaster Stemmer\n",
    "lancaster_stemmed_words = [lancaster_st.stem(word) for word in b1_review_words]\n",
    "lancaster_freq = Counter(lancaster_stemmed_words)\n",
    "lancaster_common = lancaster_freq.most_common(10)\n",
    "print(lancaster_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "32ce69f4-074a-4ab0-ae2d-d6e466205781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('place', 208), ('nt', 175), ('live', 128), ('manag', 115), ('apart', 113), ('move', 94), ('rebel', 83), ('room', 76), ('time', 72), ('peopl', 67)]\n"
     ]
    }
   ],
   "source": [
    "# Using Snowball Stemmer\n",
    "snow_stemmed_words = [snow_st.stem(word) for word in b1_review_words]\n",
    "snow_freq = Counter(snow_stemmed_words)\n",
    "snow_common = snow_freq.most_common(10)\n",
    "print(snow_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d6c54",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d4c24448",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sentences = reviews.sample(5, random_state=42)\n",
    "random_sentences = list(random_sentences['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c81cc594-a486-47fa-91bf-0f4043a6cd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Que ce soit pour leurs délicieux bubbles tea/smooties, leurs ''Bánh mì'' , leurs petits snacks (viennoiseries, tapioca, ...), on adore Vua et aussi leurs prix très abordables. On y retourne lorsqu'on est dans le Quartier Latin !\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c8155e95-94cb-490a-bbff-2db99e5b7cf0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Que', 'NNP'),\n",
       "  ('ce', 'NN'),\n",
       "  ('soit', 'VBD'),\n",
       "  ('pour', 'JJ'),\n",
       "  ('leurs', 'NNS'),\n",
       "  ('délicieux', 'VBP'),\n",
       "  ('bubbles', 'NNS'),\n",
       "  ('tea/smooties', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('leurs', 'VBZ'),\n",
       "  ('``', '``'),\n",
       "  ('Bánh', 'NNP'),\n",
       "  ('mì', 'NN'),\n",
       "  (\"''\", \"''\"),\n",
       "  (',', ','),\n",
       "  ('leurs', 'VBZ'),\n",
       "  ('petits', 'NNS'),\n",
       "  ('snacks', 'NNS'),\n",
       "  ('(', '('),\n",
       "  ('viennoiseries', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('tapioca', 'NN'),\n",
       "  (',', ','),\n",
       "  ('...', ':'),\n",
       "  (')', ')'),\n",
       "  (',', ','),\n",
       "  ('on', 'IN'),\n",
       "  ('adore', 'IN'),\n",
       "  ('Vua', 'NNP'),\n",
       "  ('et', 'CC'),\n",
       "  ('aussi', 'JJ'),\n",
       "  ('leurs', 'NNS'),\n",
       "  ('prix', 'VBP'),\n",
       "  ('très', 'JJ'),\n",
       "  ('abordables', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('On', 'IN'),\n",
       "  ('y', 'JJ'),\n",
       "  ('retourne', 'JJ'),\n",
       "  (\"lorsqu'on\", 'NN'),\n",
       "  ('est', 'JJS'),\n",
       "  ('dans', 'NNS'),\n",
       "  ('le', 'VBP'),\n",
       "  ('Quartier', 'NNP'),\n",
       "  ('Latin', 'NNP'),\n",
       "  ('!', '.')],\n",
       " [('As', 'IN'),\n",
       "  ('I', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('said', 'VBD'),\n",
       "  ('previously', 'RB'),\n",
       "  ('...', ':'),\n",
       "  ('we', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('been', 'VBN'),\n",
       "  ('coming', 'VBG'),\n",
       "  ('to', 'TO'),\n",
       "  ('LMAH', 'NNP'),\n",
       "  ('for', 'IN'),\n",
       "  ('over', 'IN'),\n",
       "  ('10', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('At', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('15', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('And', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('ALWAYS', 'NNP'),\n",
       "  ('seen', 'VBN'),\n",
       "  ('Dr.', 'NNP'),\n",
       "  ('White', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('bring', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('to', 'TO'),\n",
       "  ('have', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('urine', 'JJ'),\n",
       "  ('sample', 'NN'),\n",
       "  ('done', 'VBN'),\n",
       "  ('.', '.'),\n",
       "  ('While', 'IN'),\n",
       "  ('being', 'VBG'),\n",
       "  ('done', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('pee', 'NN'),\n",
       "  (\"'d\", 'MD'),\n",
       "  ('on', 'IN'),\n",
       "  ('himself', 'PRP'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('vet', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('staff', 'NN'),\n",
       "  ('only', 'RB'),\n",
       "  ('tried', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('clean', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('off', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('him', 'PRP'),\n",
       "  ('with', 'IN'),\n",
       "  ('alcohol', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('try', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('actually', 'RB'),\n",
       "  ('get', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('off', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('him', 'PRP'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('TELL', 'VB'),\n",
       "  ('ME', 'NNP'),\n",
       "  ('that', 'IN'),\n",
       "  ('he', 'PRP'),\n",
       "  ('pee', 'VBZ'),\n",
       "  (\"'d\", 'MD'),\n",
       "  ('all', 'DT'),\n",
       "  ('over', 'IN'),\n",
       "  ('his', 'PRP$'),\n",
       "  ('stomach', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('legs', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('So', 'RB'),\n",
       "  ('now', 'RB'),\n",
       "  ('I', 'PRP'),\n",
       "  ('e', 'VBP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('give', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('a', 'DT'),\n",
       "  ('bath', 'NN'),\n",
       "  ('because', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('ca', 'MD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('take', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('urine', 'JJ'),\n",
       "  ('sample', 'NN'),\n",
       "  ('properly', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('LIVID', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('Pretty', 'NNP'),\n",
       "  ('decent', 'NN'),\n",
       "  ('but', 'CC'),\n",
       "  ('so', 'RB'),\n",
       "  ('spacious', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Very', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('for', 'IN'),\n",
       "  ('kids', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('order', 'NN'),\n",
       "  ('dim', 'VB'),\n",
       "  ('sum', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  (\"'re\", 'VBP'),\n",
       "  ('open', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('Free', 'JJ'),\n",
       "  ('wifi', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('awesome', 'JJ'),\n",
       "  (':', ':'),\n",
       "  (')', ')')],\n",
       " [('Dumpling', 'VBG'),\n",
       "  ('Village', 'NNP'),\n",
       "  ('really', 'RB'),\n",
       "  ('suffering', 'VBG'),\n",
       "  ('from', 'IN'),\n",
       "  ('identity', 'NN'),\n",
       "  ('crisis', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('Is', 'VBZ'),\n",
       "  ('it', 'PRP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('Restaurant', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('mostly', 'RB'),\n",
       "  ('Northern', 'NNP'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('items', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('menu', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('Is', 'VBZ'),\n",
       "  ('it', 'PRP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Northern', 'NNP'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('restaurant', 'NN'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('dishes', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('menu', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('pork', 'NN'),\n",
       "  ('bone', 'NN'),\n",
       "  ('soup', 'NN'),\n",
       "  (',', ','),\n",
       "  ('cold', 'JJ'),\n",
       "  ('wheat', 'NN'),\n",
       "  ('bucknoodle', 'NN'),\n",
       "  ('soup', 'NN'),\n",
       "  (',', ','),\n",
       "  ('kimchi', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bean', 'NN'),\n",
       "  ('sprout', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('appetitizers', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('only', 'JJ'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('characters', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('3', 'CD'),\n",
       "  ('on', 'IN'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('business', 'NN'),\n",
       "  ('sign', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('front', 'NN'),\n",
       "  ('..', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('rest', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('in', 'IN'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('English', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('All', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('wait', 'NN'),\n",
       "  ('staffs', 'NNS'),\n",
       "  ('here', 'RB'),\n",
       "  ('speak', 'VBP'),\n",
       "  ('Mandarian', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('even', 'RB'),\n",
       "  ('most', 'JJS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('patrons', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('Chinese', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('It', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('located', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('really', 'RB'),\n",
       "  ('tiny', 'JJ'),\n",
       "  ('strip', 'NN'),\n",
       "  ('mall', 'NN'),\n",
       "  ('..', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('limited', 'JJ'),\n",
       "  ('parkings', 'NNS'),\n",
       "  ('!', '.'),\n",
       "  ('To', 'TO'),\n",
       "  ('tell', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('the', 'DT'),\n",
       "  ('truth', 'NN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('area', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('over', 'IN'),\n",
       "  ('10', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('I', 'PRP'),\n",
       "  ('never', 'RB'),\n",
       "  ('notice', 'VBP'),\n",
       "  ('it', 'PRP'),\n",
       "  ('until', 'IN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('read', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('reviews', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('yelp', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('around', 'RB'),\n",
       "  ('6', 'CD'),\n",
       "  ('pm', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('Saturday', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('restaurant', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('almost', 'RB'),\n",
       "  ('full', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('tables', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('packed', 'VBN'),\n",
       "  ('together', 'RB'),\n",
       "  (',', ','),\n",
       "  ('so', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('still', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('certain', 'JJ'),\n",
       "  ('degree', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('privacy', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('tell', 'VB'),\n",
       "  ('what', 'WP'),\n",
       "  ('the', 'DT'),\n",
       "  ('table', 'NN'),\n",
       "  ('next', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('me', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('eating', 'VBG'),\n",
       "  ('.', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('decided', 'VBD'),\n",
       "  ('on', 'IN'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('favourite', 'JJ'),\n",
       "  ('chives', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('pork', 'NN'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('kimchi', 'NNS'),\n",
       "  ('fried', 'VBD'),\n",
       "  ('rice', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('Korean', 'NNP'),\n",
       "  ('cold', 'VBD'),\n",
       "  ('bucknoodle', 'JJ'),\n",
       "  ('soup', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('$', '$'),\n",
       "  ('4.99', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('noodle', 'JJ'),\n",
       "  ('soup', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('so', 'RB'),\n",
       "  ('so', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Nice', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('refreshing', 'VBG'),\n",
       "  ('!', '.'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('based', 'VBN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('sour', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('really', 'RB'),\n",
       "  ('stimulate', 'VB'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('tastebuds', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('just', 'RB'),\n",
       "  ('make', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('want', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('eat', 'VB'),\n",
       "  ('more', 'JJR'),\n",
       "  ('!', '.'),\n",
       "  ('(', '('),\n",
       "  ('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('finish', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('myself', 'PRP'),\n",
       "  ('!', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('want', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('share', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('want', 'VBP'),\n",
       "  ('one', 'CD'),\n",
       "  ('all', 'DT'),\n",
       "  ('for', 'IN'),\n",
       "  ('myself', 'PRP'),\n",
       "  ('next', 'JJ'),\n",
       "  ('time', 'NN'),\n",
       "  ('!', '.'),\n",
       "  (')', ')'),\n",
       "  ('The', 'DT'),\n",
       "  ('kimchi', 'NN'),\n",
       "  ('fried', 'VBD'),\n",
       "  ('rice', 'NN'),\n",
       "  ('..', 'NNP'),\n",
       "  ('hmmmmm', 'NN'),\n",
       "  ('not', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('A', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('oily', 'JJ'),\n",
       "  ('side', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('much', 'JJ'),\n",
       "  ('taste', 'NN'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('spicy', 'VB'),\n",
       "  ('enough', 'RB'),\n",
       "  ('?', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('could', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('finish', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('and', 'CC'),\n",
       "  ('end', 'VB'),\n",
       "  ('up', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('pack', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('to', 'TO'),\n",
       "  ('go', 'VB'),\n",
       "  ('...', ':'),\n",
       "  ('(', '('),\n",
       "  ('not', 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('sure', 'JJ'),\n",
       "  ('I', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('eat', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  (',', ','),\n",
       "  ('but', 'CC'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('just', 'RB'),\n",
       "  ('let', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('go', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('waste', 'NN'),\n",
       "  (')', ')'),\n",
       "  ('The', 'DT'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('last', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('really', 'RB'),\n",
       "  ('worth', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('wait', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('14', 'CD'),\n",
       "  ('big', 'JJ'),\n",
       "  ('fat', 'NN'),\n",
       "  ('hot', 'JJ'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('filled', 'VBN'),\n",
       "  ('up', 'RP'),\n",
       "  ('the', 'DT'),\n",
       "  ('whole', 'JJ'),\n",
       "  ('steamer', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('Yes', 'UH'),\n",
       "  ('I', 'PRP'),\n",
       "  ('counted', 'VBD'),\n",
       "  ('them', 'PRP'),\n",
       "  ('14', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('them', 'PRP'),\n",
       "  ('for', 'IN'),\n",
       "  ('$', '$'),\n",
       "  ('4.99', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('freshly', 'RB'),\n",
       "  ('made', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('freshly', 'RB'),\n",
       "  ('steamed', 'VBN'),\n",
       "  ('!', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('skin', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('nice', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('thin', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('chewy', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('thick', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Oh', 'NNP'),\n",
       "  ('so', 'RB'),\n",
       "  ('delicious', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('One', 'CD'),\n",
       "  ('bite', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('ozzing', 'VBG'),\n",
       "  ('out', 'RP'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('if', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('burning', 'VBG'),\n",
       "  ('hot', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('just', 'RB'),\n",
       "  ('stuff', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('whole', 'JJ'),\n",
       "  ('thing', 'NN'),\n",
       "  ('into', 'IN'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('mouth', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('let', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('favour', 'NN'),\n",
       "  ('just', 'RB'),\n",
       "  ('explosed', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('mouth', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('Just', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('warning', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('If', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('going', 'VBG'),\n",
       "  ('out', 'RP'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('date', 'NN'),\n",
       "  (',', ','),\n",
       "  ('please', 'VB'),\n",
       "  ('do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('come', 'VB'),\n",
       "  ('here', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('Even', 'RB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('good', 'JJ'),\n",
       "  ('place', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('cheap', 'JJ'),\n",
       "  ('date', 'NN'),\n",
       "  (',', ','),\n",
       "  ('our', 'PRP$'),\n",
       "  ('bill', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('$', '$'),\n",
       "  ('18', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('2', 'CD'),\n",
       "  ('and', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('full', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('place', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('noisy', 'RB'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('food', 'NN'),\n",
       "  ('too', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('date', 'NN'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('too', 'RB'),\n",
       "  ('busy', 'JJ'),\n",
       "  ('eating', 'VBG'),\n",
       "  ('and', 'CC'),\n",
       "  ('just', 'RB'),\n",
       "  ('ignore', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('!', '.'),\n",
       "  ('Or', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('keep', 'VB'),\n",
       "  ('eating', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('eating', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('freak', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('friends', 'NNS'),\n",
       "  ('out', 'RP'),\n",
       "  ('!', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('services', 'NNS'),\n",
       "  ('here', 'RB'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('but', 'CC'),\n",
       "  ('efficient', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('given', 'VBN'),\n",
       "  ('extra', 'JJ'),\n",
       "  ('napkins', 'NNS'),\n",
       "  ('without', 'IN'),\n",
       "  ('requesting', 'VBG'),\n",
       "  ('...', ':'),\n",
       "  ('Maybe', 'RB'),\n",
       "  ('we', 'PRP'),\n",
       "  ('looked', 'VBD'),\n",
       "  ('really', 'RB'),\n",
       "  ('messy', 'VBN'),\n",
       "  ('?', '.'),\n",
       "  ('?', '.'),\n",
       "  ('Cash', 'NNP'),\n",
       "  ('only', 'RB'),\n",
       "  ('!', '.')],\n",
       " [('best', 'JJS'),\n",
       "  ('pizza', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('south', 'JJ'),\n",
       "  ('side', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('huge', 'JJ'),\n",
       "  ('drink', 'NN'),\n",
       "  ('selection', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('awesome', 'VB'),\n",
       "  ('atmosphere', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('$', '$'),\n",
       "  ('4', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('slice', 'NN'),\n",
       "  ('that', 'WDT'),\n",
       "  ('could', 'MD'),\n",
       "  ('easily', 'RB'),\n",
       "  ('feed', 'VB'),\n",
       "  ('2', 'CD'),\n",
       "  ('people', 'NNS'),\n",
       "  ('!', '.')]]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tagged = []\n",
    "for sentence in random_sentences:\n",
    "    nltk_tagged.append((nltk.pos_tag(word_tokenize(sentence))))\n",
    "nltk_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5d874c6c-1f0f-4232-b819-693f49a971a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que      PROPN \n",
      "ce       PROPN \n",
      "soit     ADJ   \n",
      "pour     NOUN  \n",
      "leurs    VERB  \n",
      "délicieux NOUN  \n",
      "bubbles  NOUN  \n",
      "tea      NOUN  \n",
      "/        SYM   \n",
      "smooties NOUN  \n",
      ",        PUNCT \n",
      "leurs    VERB  \n",
      "''       PUNCT \n",
      "Bánh     PROPN \n",
      "mì       INTJ  \n",
      "''       PUNCT \n",
      ",        PUNCT \n",
      "leurs    NOUN  \n",
      "petits   VERB  \n",
      "snacks   NOUN  \n",
      "(        PUNCT \n",
      "viennoiseries NOUN  \n",
      ",        PUNCT \n",
      "tapioca  INTJ  \n",
      ",        PUNCT \n",
      "...      PUNCT \n",
      ")        PUNCT \n",
      ",        PUNCT \n",
      "on       ADP   \n",
      "adore    PROPN \n",
      "Vua      PROPN \n",
      "et       PROPN \n",
      "aussi    PROPN \n",
      "leurs    VERB  \n",
      "prix     NOUN  \n",
      "très     ADJ   \n",
      "abordables NOUN  \n",
      ".        PUNCT \n",
      "On       ADP   \n",
      "y        PROPN \n",
      "retourne VERB  \n",
      "lorsqu'on PROPN \n",
      "est      PROPN \n",
      "dans     PROPN \n",
      "le       X     \n",
      "Quartier PROPN \n",
      "Latin    PROPN \n",
      "!        PUNCT \n",
      "As       ADP   \n",
      "I        PRON  \n",
      "'ve      AUX   \n",
      "said     VERB  \n",
      "previously ADV   \n",
      "...      PUNCT \n",
      "we've    PROPN \n",
      "been     AUX   \n",
      "coming   VERB  \n",
      "to       ADP   \n",
      "LMAH     PROPN \n",
      "for      ADP   \n",
      "over     ADP   \n",
      "10       NUM   \n",
      "years    NOUN  \n",
      ".        PUNCT \n",
      "At       ADV   \n",
      "least    ADJ   \n",
      "15       NUM   \n",
      ".        PUNCT \n",
      "And      CCONJ \n",
      "we       PRON  \n",
      "'ve      AUX   \n",
      "ALWAYS   ADV   \n",
      "seen     VERB  \n",
      "Dr.      PROPN \n",
      "White    PROPN \n",
      ".        PUNCT \n",
      "\n",
      "        SPACE \n",
      "I        PRON  \n",
      "had      VERB  \n",
      "to       PART  \n",
      "bring    VERB  \n",
      "my       PRON  \n",
      "cat      NOUN  \n",
      "in       ADP   \n",
      "to       PART  \n",
      "have     VERB  \n",
      "a        DET   \n",
      "urine    NOUN  \n",
      "sample   NOUN  \n",
      "done     VERB  \n",
      ".        PUNCT \n",
      "While    SCONJ \n",
      "being    AUX   \n",
      "done     VERB  \n",
      ",        PUNCT \n",
      "my       PRON  \n",
      "cat      NOUN  \n",
      "pee'd    ADV   \n",
      "on       ADP   \n",
      "himself  PRON  \n",
      ".        PUNCT \n",
      "The      DET   \n",
      "vet      NOUN  \n",
      "or       CCONJ \n",
      "staff    NOUN  \n",
      "only     ADV   \n",
      "tried    VERB  \n",
      "to       PART  \n",
      "clean    VERB  \n",
      "it       PRON  \n",
      "off      ADP   \n",
      "of       ADP   \n",
      "him      PRON  \n",
      "with     ADP   \n",
      "alcohol  NOUN  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "did      AUX   \n",
      "n't      PART  \n",
      "try      VERB  \n",
      "to       PART  \n",
      "actually ADV   \n",
      "get      VERB  \n",
      "it       PRON  \n",
      "off      ADP   \n",
      "of       ADP   \n",
      "him      PRON  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "did      AUX   \n",
      "n't      PART  \n",
      "even     ADV   \n",
      "TELL     VERB  \n",
      "ME       PRON  \n",
      "that     SCONJ \n",
      "he       PRON  \n",
      "pee'd    VERB  \n",
      "all      ADV   \n",
      "over     ADP   \n",
      "his      PRON  \n",
      "stomach  NOUN  \n",
      "and      CCONJ \n",
      "legs     NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "So       ADV   \n",
      "now      ADV   \n",
      "I        PRON  \n",
      "e        VERB  \n",
      "had      VERB  \n",
      "to       PART  \n",
      "give     VERB  \n",
      "my       PRON  \n",
      "cat      NOUN  \n",
      "a        DET   \n",
      "bath     NOUN  \n",
      "because  SCONJ \n",
      "they     PRON  \n",
      "ca       AUX   \n",
      "n't      PART  \n",
      "take     VERB  \n",
      "a        DET   \n",
      "urine    NOUN  \n",
      "sample   NOUN  \n",
      "properly ADV   \n",
      ".        PUNCT \n",
      "I        PRON  \n",
      "am       AUX   \n",
      "LIVID    PROPN \n",
      ".        PUNCT \n",
      "Pretty   ADV   \n",
      "decent   ADJ   \n",
      "but      CCONJ \n",
      "so       ADV   \n",
      "spacious ADJ   \n",
      "!        PUNCT \n",
      "         SPACE \n",
      "Very     ADV   \n",
      "good     ADJ   \n",
      "for      ADP   \n",
      "kids     NOUN  \n",
      "and      CCONJ \n",
      "you      PRON  \n",
      "can      AUX   \n",
      "order    VERB  \n",
      "dim      NOUN  \n",
      "sum      NOUN  \n",
      "!        PUNCT \n",
      "They     PRON  \n",
      "'re      VERB  \n",
      "open     ADJ   \n",
      "!        PUNCT \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Free     ADJ   \n",
      "wifi     NOUN  \n",
      "and      CCONJ \n",
      "awesome  ADJ   \n",
      ":)       PUNCT \n",
      "Dumpling PROPN \n",
      "Village  PROPN \n",
      "really   ADV   \n",
      "suffering VERB  \n",
      "from     ADP   \n",
      "identity NOUN  \n",
      "crisis   NOUN  \n",
      "!        PUNCT \n",
      "!        PUNCT \n",
      "Is       AUX   \n",
      "it       PRON  \n",
      "a        DET   \n",
      "Korean   PROPN \n",
      "Restaurant PROPN \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "really   ADV   \n",
      ".        PUNCT \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "mostly   ADV   \n",
      "Northern ADJ   \n",
      "Chinese  ADJ   \n",
      "items    NOUN  \n",
      "on       ADP   \n",
      "the      DET   \n",
      "menu     NOUN  \n",
      "like     ADP   \n",
      "dumplings NOUN  \n",
      ".        PUNCT \n",
      "Is       AUX   \n",
      "it       PRON  \n",
      "a        DET   \n",
      "Northern ADJ   \n",
      "Chinese  ADJ   \n",
      "restaurant NOUN  \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "really   ADV   \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "a        DET   \n",
      "lot      NOUN  \n",
      "of       ADP   \n",
      "Korean   ADJ   \n",
      "dishes   NOUN  \n",
      "         SPACE \n",
      "in       ADP   \n",
      "the      DET   \n",
      "menu     NOUN  \n",
      "like     ADP   \n",
      "pork     NOUN  \n",
      "bone     NOUN  \n",
      "soup     NOUN  \n",
      ",        PUNCT \n",
      "cold     ADJ   \n",
      "wheat    NOUN  \n",
      "bucknoodle NOUN  \n",
      "soup     NOUN  \n",
      ",        PUNCT \n",
      "kimchi   PROPN \n",
      "and      CCONJ \n",
      "bean     NOUN  \n",
      "sprout   NOUN  \n",
      "as       ADP   \n",
      "appetitizers NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "only     ADJ   \n",
      "Korean   ADJ   \n",
      "characters NOUN  \n",
      "are      AUX   \n",
      "the      DET   \n",
      "3        NUM   \n",
      "on       ADP   \n",
      "their    PRON  \n",
      "business NOUN  \n",
      "sign     NOUN  \n",
      "in       ADP   \n",
      "the      DET   \n",
      "front    NOUN  \n",
      "..       PUNCT \n",
      "and      CCONJ \n",
      "all      DET   \n",
      "the      DET   \n",
      "rest     NOUN  \n",
      "are      VERB  \n",
      "in       ADP   \n",
      "Chinese  PROPN \n",
      "and      CCONJ \n",
      "English  PROPN \n",
      ".        PUNCT \n",
      "All      DET   \n",
      "the      DET   \n",
      "wait     ADJ   \n",
      "staffs   NOUN  \n",
      "here     ADV   \n",
      "speak    VERB  \n",
      "Mandarian PROPN \n",
      "and      CCONJ \n",
      "even     ADV   \n",
      "most     ADJ   \n",
      "of       ADP   \n",
      "the      DET   \n",
      "patrons  NOUN  \n",
      "are      VERB  \n",
      "Chinese  ADJ   \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "It       PRON  \n",
      "is       AUX   \n",
      "located  VERB  \n",
      "in       ADP   \n",
      "a        DET   \n",
      "really   ADV   \n",
      "tiny     ADJ   \n",
      "strip    NOUN  \n",
      "mall     NOUN  \n",
      "..       PUNCT \n",
      "with     ADP   \n",
      "limited  ADJ   \n",
      "parkings NOUN  \n",
      "!        PUNCT \n",
      "To       PART  \n",
      "tell     VERB  \n",
      "you      PRON  \n",
      "the      DET   \n",
      "truth    NOUN  \n",
      "I        PRON  \n",
      "am       VERB  \n",
      "in       ADP   \n",
      "the      DET   \n",
      "area     NOUN  \n",
      "for      ADP   \n",
      "over     ADP   \n",
      "10       NUM   \n",
      "years    NOUN  \n",
      "I        PRON  \n",
      "never    ADV   \n",
      "notice   VERB  \n",
      "it       PRON  \n",
      "until    ADP   \n",
      "I        PRON  \n",
      "read     VERB  \n",
      "the      DET   \n",
      "reviews  NOUN  \n",
      "on       ADP   \n",
      "yelp     NOUN  \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "We       PRON  \n",
      "arrived  VERB  \n",
      "around   ADV   \n",
      "6        NUM   \n",
      "pm       NOUN  \n",
      "on       ADP   \n",
      "a        DET   \n",
      "Saturday PROPN \n",
      ".        PUNCT \n",
      "The      DET   \n",
      "restaurant NOUN  \n",
      "was      AUX   \n",
      "almost   ADV   \n",
      "full     ADJ   \n",
      ".        PUNCT \n",
      "         SPACE \n",
      "The      DET   \n",
      "tables   NOUN  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "really   ADV   \n",
      "packed   VERB  \n",
      "together ADV   \n",
      ",        PUNCT \n",
      "so       ADV   \n",
      "you      PRON  \n",
      "still    ADV   \n",
      "have     VERB  \n",
      "a        DET   \n",
      "certain  ADJ   \n",
      "degree   NOUN  \n",
      "of       ADP   \n",
      "privacy  NOUN  \n",
      "at       ADP   \n",
      "least    ADJ   \n",
      "I        PRON  \n",
      "can      AUX   \n",
      "not      PART  \n",
      "tell     VERB  \n",
      "what     PRON  \n",
      "the      DET   \n",
      "table    NOUN  \n",
      "next     ADV   \n",
      "to       ADP   \n",
      "me       PRON  \n",
      "were     AUX   \n",
      "eating   VERB  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "We       PRON  \n",
      "decided  VERB  \n",
      "on       ADP   \n",
      "my       PRON  \n",
      "favourite ADJ   \n",
      "chives   NOUN  \n",
      "and      CCONJ \n",
      "pork     NOUN  \n",
      "dumplings NOUN  \n",
      ",        PUNCT \n",
      "kimchi   PROPN \n",
      "fried    VERB  \n",
      "rice     NOUN  \n",
      "and      CCONJ \n",
      "Korean   ADJ   \n",
      "cold     ADJ   \n",
      "bucknoodle NOUN  \n",
      "soup     NOUN  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "are      AUX   \n",
      "all      ADV   \n",
      "$        SYM   \n",
      "4.99     NUM   \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "cold     ADJ   \n",
      "noodle   NOUN  \n",
      "soup     NOUN  \n",
      "is       AUX   \n",
      "so       ADV   \n",
      "so       ADV   \n",
      "good     ADJ   \n",
      "!        PUNCT \n",
      "Nice     ADJ   \n",
      "and      CCONJ \n",
      "refreshing ADJ   \n",
      "!        PUNCT \n",
      "and      CCONJ \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "based    VERB  \n",
      "is       AUX   \n",
      "a        DET   \n",
      "little   ADJ   \n",
      "sour     ADJ   \n",
      "and      CCONJ \n",
      "it       PRON  \n",
      "really   ADV   \n",
      "stimulate VERB  \n",
      "your     PRON  \n",
      "tastebuds NOUN  \n",
      "and      CCONJ \n",
      "just     ADV   \n",
      "make     VERB  \n",
      "you      PRON  \n",
      "want     VERB  \n",
      "to       PART  \n",
      "eat      VERB  \n",
      "more     ADJ   \n",
      "!        PUNCT \n",
      "(        PUNCT \n",
      "I        PRON  \n",
      "can      AUX   \n",
      "finish   VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "myself   PRON  \n",
      "!        PUNCT \n",
      "I        PRON  \n",
      "do       AUX   \n",
      "n't      PART  \n",
      "want     VERB  \n",
      "to       PART  \n",
      "share    VERB  \n",
      "!        PUNCT \n",
      "I        PRON  \n",
      "want     VERB  \n",
      "one      NUM   \n",
      "all      DET   \n",
      "for      ADP   \n",
      "myself   PRON  \n",
      "next     ADJ   \n",
      "time     NOUN  \n",
      "!        PUNCT \n",
      ")        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "kimchi   PROPN \n",
      "fried    VERB  \n",
      "rice     NOUN  \n",
      "..       PUNCT \n",
      "hmmmmm   ADV   \n",
      "not      PART  \n",
      "good     ADJ   \n",
      "!        PUNCT \n",
      "A        DET   \n",
      "little   ADJ   \n",
      "on       ADP   \n",
      "the      DET   \n",
      "oily     ADJ   \n",
      "side     NOUN  \n",
      ".        PUNCT \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "much     ADJ   \n",
      "taste    NOUN  \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "spicy    ADJ   \n",
      "enough   ADV   \n",
      "?        PUNCT \n",
      "We       PRON  \n",
      "could    AUX   \n",
      "not      PART  \n",
      "finish   VERB  \n",
      "it       PRON  \n",
      "and      CCONJ \n",
      "end      VERB  \n",
      "up       ADP   \n",
      "have     VERB  \n",
      "to       PART  \n",
      "pack     VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "to       PART  \n",
      "go       VERB  \n",
      "...      PUNCT \n",
      "(not     INTJ  \n",
      "even     ADV   \n",
      "sure     ADJ   \n",
      "I        PRON  \n",
      "will     AUX   \n",
      "eat      VERB  \n",
      "it       PRON  \n",
      ",        PUNCT \n",
      "but      CCONJ \n",
      "can      AUX   \n",
      "not      PART  \n",
      "just     ADV   \n",
      "let      VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "go       VERB  \n",
      "to       AUX   \n",
      "waste    VERB  \n",
      ")        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "dumplings NOUN  \n",
      "arrived  VERB  \n",
      "last     ADJ   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "it       PRON  \n",
      "really   ADV   \n",
      "worth    ADJ   \n",
      "the      DET   \n",
      "wait     NOUN  \n",
      "!        PUNCT \n",
      "14       NUM   \n",
      "big      ADJ   \n",
      "fat      ADJ   \n",
      "hot      ADJ   \n",
      "dumplings NOUN  \n",
      "filled   VERB  \n",
      "up       ADP   \n",
      "the      DET   \n",
      "whole    ADJ   \n",
      "steamer  NOUN  \n",
      "!        PUNCT \n",
      "Yes      INTJ  \n",
      "I        PRON  \n",
      "counted  VERB  \n",
      "them     PRON  \n",
      "14       NUM   \n",
      "of       ADP   \n",
      "them     PRON  \n",
      "for      ADP   \n",
      "$        SYM   \n",
      "4.99     NUM   \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "are      AUX   \n",
      "all      DET   \n",
      "freshly  ADV   \n",
      "made     VERB  \n",
      ",        PUNCT \n",
      "freshly  ADV   \n",
      "steamed  ADJ   \n",
      "!        PUNCT \n",
      "The      DET   \n",
      "skin     NOUN  \n",
      "is       AUX   \n",
      "nice     ADJ   \n",
      "and      CCONJ \n",
      "thin     ADJ   \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "chewy    NOUN  \n",
      "and      CCONJ \n",
      "thick    ADJ   \n",
      "!        PUNCT \n",
      "Oh       INTJ  \n",
      "so       ADV   \n",
      "delicious ADJ   \n",
      "!        PUNCT \n",
      "One      NUM   \n",
      "bite     NOUN  \n",
      "and      CCONJ \n",
      "all      DET   \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "are      AUX   \n",
      "ozzing   VERB  \n",
      "out      ADP   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "if       SCONJ \n",
      "they     PRON  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "burning  VERB  \n",
      "hot      ADJ   \n",
      ",        PUNCT \n",
      "you      PRON  \n",
      "can      AUX   \n",
      "just     ADV   \n",
      "stuff    VERB  \n",
      "the      DET   \n",
      "whole    ADJ   \n",
      "thing    NOUN  \n",
      "into     ADP   \n",
      "your     PRON  \n",
      "mouth    NOUN  \n",
      "and      CCONJ \n",
      "let      VERB  \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "and      CCONJ \n",
      "favour   NOUN  \n",
      "just     ADV   \n",
      "explosed VERB  \n",
      "in       ADP   \n",
      "your     PRON  \n",
      "mouth    NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Just     ADV   \n",
      "a        DET   \n",
      "warning  NOUN  \n",
      ".        PUNCT \n",
      "If       SCONJ \n",
      "you      PRON  \n",
      "going    VERB  \n",
      "out      ADP   \n",
      "on       ADP   \n",
      "a        DET   \n",
      "date     NOUN  \n",
      ",        PUNCT \n",
      "please   INTJ  \n",
      "do       AUX   \n",
      "n't      PART  \n",
      "come     VERB  \n",
      "here     ADV   \n",
      ".        PUNCT \n",
      "Even     ADV   \n",
      "it       PRON  \n",
      "is       VERB  \n",
      "a        DET   \n",
      "good     ADJ   \n",
      "place    NOUN  \n",
      "for      ADP   \n",
      "a        DET   \n",
      "cheap    ADJ   \n",
      "date     NOUN  \n",
      ",        PUNCT \n",
      "our      PRON  \n",
      "bill     NOUN  \n",
      "was      AUX   \n",
      "$        SYM   \n",
      "18       NUM   \n",
      "for      ADP   \n",
      "2        NUM   \n",
      "and      CCONJ \n",
      "we       PRON  \n",
      "were     VERB  \n",
      "full     ADJ   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "the      DET   \n",
      "place    NOUN  \n",
      "can      AUX   \n",
      "be       VERB  \n",
      "noisy    ADJ   \n",
      ",        PUNCT \n",
      "the      DET   \n",
      "food     NOUN  \n",
      "too      ADV   \n",
      "good     ADJ   \n",
      "and      CCONJ \n",
      "your     PRON  \n",
      "date     NOUN  \n",
      "will     AUX   \n",
      "be       VERB  \n",
      "too      ADV   \n",
      "busy     ADJ   \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "just     ADV   \n",
      "ignore   VERB  \n",
      "you      PRON  \n",
      "!        PUNCT \n",
      "Or       CCONJ \n",
      "you      PRON  \n",
      "will     AUX   \n",
      "keep     VERB  \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "you      PRON  \n",
      "freak    VERB  \n",
      "all      DET   \n",
      "your     PRON  \n",
      "friends  NOUN  \n",
      "out      ADP   \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "services NOUN  \n",
      "here     ADV   \n",
      "is       AUX   \n",
      "cold     ADJ   \n",
      "but      CCONJ \n",
      "efficient ADJ   \n",
      ".        PUNCT \n",
      "We       PRON  \n",
      "were     AUX   \n",
      "given    VERB  \n",
      "extra    ADJ   \n",
      "napkins  NOUN  \n",
      "without  ADP   \n",
      "requesting VERB  \n",
      "...      PUNCT \n",
      "Maybe    ADV   \n",
      "we       PRON  \n",
      "looked   VERB  \n",
      "really   ADV   \n",
      "messy    ADJ   \n",
      "?        PUNCT \n",
      "?        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Cash     VERB  \n",
      "only     ADV   \n",
      "!        PUNCT \n",
      "best     ADJ   \n",
      "pizza    NOUN  \n",
      "in       ADP   \n",
      "south    ADJ   \n",
      "side     NOUN  \n",
      ".        PUNCT \n",
      "huge     ADJ   \n",
      "drink    NOUN  \n",
      "selection NOUN  \n",
      ".        PUNCT \n",
      "awesome  ADJ   \n",
      "atmosphere NOUN  \n",
      ".        PUNCT \n",
      "$        SYM   \n",
      "4        NUM   \n",
      "for      ADP   \n",
      "a        DET   \n",
      "slice    NOUN  \n",
      "that     DET   \n",
      "could    AUX   \n",
      "easily   ADV   \n",
      "feed     VERB  \n",
      "2        NUM   \n",
      "people   NOUN  \n",
      "!        PUNCT \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tagged = []\n",
    "for sentence in random_sentences:\n",
    "    spacy_tagged.append(nlp(sentence))\n",
    "for tagged in spacy_tagged:\n",
    "    for token in tagged:\n",
    "        print(f'{token.text:{8}} {token.pos_:{6}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627839e",
   "metadata": {},
   "source": [
    "# WORK COMPLETED UP TILL HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f44ab",
   "metadata": {},
   "source": [
    "### Writing Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "57662d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSOFurl():\n",
    "    while(True):\n",
    "        ip0 = 'stackoverflow'\n",
    "        ip1 = 'com'\n",
    "        ip2 = 'questions'\n",
    "        ip3 = str(random.randint(0, 100000000))\n",
    "        url = 'https://' + ip0 + '.' + ip1 + '/'+ ip2 + '/'+ ip3\n",
    "        try:\n",
    "            urlContent = urlopen(url).read()\n",
    "            if urlContent:\n",
    "                break\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    print(\"Found URL: \" + url)\n",
    "    page1 = requests.get(url)\n",
    "    soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "    text = list(soup1.find_all(\"p\"))\n",
    "    sof_text = [txt.get_text() for txt in text]\n",
    "   \n",
    "    return sof_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1ba56e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHWZurl():\n",
    "    url = 'https://www.hardwarezone.com.sg/home'\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "    urls = []\n",
    "    for link in soup.find_all('a'):\n",
    "        urls.append(link.get('href'))\n",
    "    random.shuffle(urls)\n",
    "    while(True):\n",
    "        for url in urls:\n",
    "            print(url)\n",
    "            try:\n",
    "                url = 'https://www.hardwarezone.com.sg' + url\n",
    "                urlContent = urlopen(url).read()\n",
    "                page1 = requests.get(url)\n",
    "                soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "                text = list(soup1.find_all(\"p\"))\n",
    "                hwz_text = [txt.get_text() for txt in text]\n",
    "                if urlContent and len(hwz_text)>10:\n",
    "                    break\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "        break\n",
    "    print(\"Found URL: \" + url)\n",
    "    return hwz_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "380f0b1c-e1d2-4e2e-a744-d92781fa7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "def getCNAurl():\n",
    "    url = 'https://www.channelnewsasia.com/'\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "    linked_urls = set()\n",
    "\n",
    "    def get_all_links(url):\n",
    "        urls = set()\n",
    "        domain_name = urlparse(url).netloc\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "        for a_tag in soup.findAll(\"a\"):\n",
    "            href = a_tag.attrs.get(\"href\")\n",
    "            if href == \"\" or href is None:\n",
    "                # href empty tag\n",
    "                continue\n",
    "            href = urljoin(url, href)\n",
    "            parsed_href = urlparse(href)\n",
    "            # remove URL GET parameters, URL fragments, etc.\n",
    "            href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "            if href in urls:\n",
    "                # already in the set\n",
    "                continue\n",
    "            if domain_name not in href:\n",
    "                # external link\n",
    "                continue\n",
    "            if len(href) > 60:\n",
    "                urls.add(href)\n",
    "        return urls\n",
    "    \n",
    "    urls_cna = get_all_links('https://www.channelnewsasia.com/')\n",
    "    url = random.sample(urls_cna, 1)\n",
    "    print(\"Found URL: \", url[0])\n",
    "    \n",
    "    page1 = requests.get(cna_text1)\n",
    "    soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "    text = list(soup1.find_all(\"p\"))\n",
    "    cna_text = [txt.get_text() for txt in text]\n",
    "   \n",
    "    return cna_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb97780",
   "metadata": {},
   "source": [
    "#### Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bf0b4c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "Found URL: https://stackoverflow.com/questions/50282765\n",
      "HTTP Error 404: Not Found\n",
      "Found URL: https://stackoverflow.com/questions/15773218\n"
     ]
    }
   ],
   "source": [
    "sof_text1 = getSOFurl()\n",
    "sof_text2 = getSOFurl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d2104397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://forums.hardwarezone.com.sg/login/\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "/tech-news/list\n",
      "/feature-review-netflixs-squid-game-throws-likeable-characters-battle-royale\n",
      "Found URL: https://www.hardwarezone.com.sg/feature-review-netflixs-squid-game-throws-likeable-characters-battle-royale\n",
      "/product-guide/203-photography/home\n",
      "https://forums.hardwarezone.com.sg/forumdisplay.php?f=23\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n",
      "/feature-review-netflixs-squid-game-throws-likeable-characters-battle-royale\n",
      "Found URL: https://www.hardwarezone.com.sg/feature-review-netflixs-squid-game-throws-likeable-characters-battle-royale\n"
     ]
    }
   ],
   "source": [
    "hwz_text1 = getHWZurl()\n",
    "hwz_text2 = getHWZurl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URL:  https://www.channelnewsasia.com/commentary/malaysia-covid-19-kita-jaga-kita-white-flag-help-politics-2179596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colot\\AppData\\Local\\Temp/ipykernel_17396/3865290005.py:33: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  url = random.sample(urls_cna, 1)\n"
     ]
    },
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for '[\\'\\\\n\\\\n      World\\\\n  \\\\n\\', \\'\\\\n\\\\n      World\\\\n  \\\\n\\', \\'Seventh grade student Ryza Delos Santos, 10, works on her modules at home as her cousin observes, after a session at the makeshift rickshaw distance learning centre for the Aeta community in Porac, Pampanga, Philippines, Oct 12, 2020. (File photo: REUTERS/Eloisa Lopez)\\', \"MANILA: The United Nations children\\'s agency UNICEF has urged education authorities to reopen schools as soon as possible in countries where millions of students are still not allowed to return to classrooms 18 months into the COVID-19 pandemic.\", \\'Schools in around 17 countries remain fully closed, while those in 39 countries remain partially closed, according to a report released by UNICEF on Thursday (Sep 16).\\', \\'Among those \"almost completely closed\" are schools usually attended by nearly 77 million students in the Philippines, Bangladesh, Venezuela, Saudi Arabia, Panama and Kuwait.\\', \"Nearly a third of this figure is accounted for by the Philippines, which is fighting one of Asia\\'s worst COVID-19 outbreaks and where a new school year started this week.\", \\'Pupils from the six countries represent more than half of the 131 million students worldwide that have missed more than three-quarters of their in-person learning, UNICEF said.\\', \\'\"The education crisis is still here, and with each passing day that classrooms remain dark, the devastation worsens,\" said UNICEF Executive Director Henrietta Fore.\\', \\'The report said teachers should be prioritised for COVID-19 vaccines, after health workers and those most at risk, to protect them from community transmission.\\', \\'Students may be safer at home, but the availability of computers, mobile phones and Internet, and the uneven quality of education, are among challenges they continue to face.\\', \\'In the Philippines, some children have been forced to climb onto roofs just to get an Internet signal.\\', \\'In June, President Rodrigo Duterte rejected a proposal to allow face-to-face classes to resume in some areas, saying: \"I cannot gamble on the health of the children.\"\\', \"In a report released in April, the Asian Development Bank estimated school closures lasting more than a year could slash future earnings among the region\\'s students by as much as US$1.25 trillion, or equivalent to 5.4 per cent of GDP in 2020.\", \\'UNICEF and its partners will shut down their digital channels for 18 hours on Thursday to draw attention to the crisis and the \"18 months of lost learning\".\\', \\'\"This is a crisis we will not allow the world to ignore,\" UNICEF\\\\\\'s Fore said. \"Our channels are silent, but our message is loud: Every community, everywhere must reopen schools as soon as possible.\"\\', \\'\\\\xa0\\', \"\\\\n      This service is not intended for persons residing in the E.U. By clicking subscribe, I agree to receive news updates and promotional material from Mediacorp and Mediacorp\\'s partners. \\\\n  \", \\' Copyright© Mediacorp 2021. Mediacorp Pte Ltd. All rights reserved. \\', \"We know it\\'s a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be.\", \\'To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.\\', \\'Upgraded but still having issues? Contact us\\']'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17396/2649559054.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcna_text1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetCNAurl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcna_text2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetCNAurl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17396/3865290005.py\u001b[0m in \u001b[0;36mgetCNAurl\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Found URL: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mpage1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0msoup1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Get the appropriate adapter to use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[0madapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;31m# Start time (approximately) of the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cz4045\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget_adapter\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[1;31m# Nothing matches :-/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mInvalidSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No connection adapters were found for {!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidSchema\u001b[0m: No connection adapters were found for '[\\'\\\\n\\\\n      World\\\\n  \\\\n\\', \\'\\\\n\\\\n      World\\\\n  \\\\n\\', \\'Seventh grade student Ryza Delos Santos, 10, works on her modules at home as her cousin observes, after a session at the makeshift rickshaw distance learning centre for the Aeta community in Porac, Pampanga, Philippines, Oct 12, 2020. (File photo: REUTERS/Eloisa Lopez)\\', \"MANILA: The United Nations children\\'s agency UNICEF has urged education authorities to reopen schools as soon as possible in countries where millions of students are still not allowed to return to classrooms 18 months into the COVID-19 pandemic.\", \\'Schools in around 17 countries remain fully closed, while those in 39 countries remain partially closed, according to a report released by UNICEF on Thursday (Sep 16).\\', \\'Among those \"almost completely closed\" are schools usually attended by nearly 77 million students in the Philippines, Bangladesh, Venezuela, Saudi Arabia, Panama and Kuwait.\\', \"Nearly a third of this figure is accounted for by the Philippines, which is fighting one of Asia\\'s worst COVID-19 outbreaks and where a new school year started this week.\", \\'Pupils from the six countries represent more than half of the 131 million students worldwide that have missed more than three-quarters of their in-person learning, UNICEF said.\\', \\'\"The education crisis is still here, and with each passing day that classrooms remain dark, the devastation worsens,\" said UNICEF Executive Director Henrietta Fore.\\', \\'The report said teachers should be prioritised for COVID-19 vaccines, after health workers and those most at risk, to protect them from community transmission.\\', \\'Students may be safer at home, but the availability of computers, mobile phones and Internet, and the uneven quality of education, are among challenges they continue to face.\\', \\'In the Philippines, some children have been forced to climb onto roofs just to get an Internet signal.\\', \\'In June, President Rodrigo Duterte rejected a proposal to allow face-to-face classes to resume in some areas, saying: \"I cannot gamble on the health of the children.\"\\', \"In a report released in April, the Asian Development Bank estimated school closures lasting more than a year could slash future earnings among the region\\'s students by as much as US$1.25 trillion, or equivalent to 5.4 per cent of GDP in 2020.\", \\'UNICEF and its partners will shut down their digital channels for 18 hours on Thursday to draw attention to the crisis and the \"18 months of lost learning\".\\', \\'\"This is a crisis we will not allow the world to ignore,\" UNICEF\\\\\\'s Fore said. \"Our channels are silent, but our message is loud: Every community, everywhere must reopen schools as soon as possible.\"\\', \\'\\\\xa0\\', \"\\\\n      This service is not intended for persons residing in the E.U. By clicking subscribe, I agree to receive news updates and promotional material from Mediacorp and Mediacorp\\'s partners. \\\\n  \", \\' Copyright© Mediacorp 2021. Mediacorp Pte Ltd. All rights reserved. \\', \"We know it\\'s a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be.\", \\'To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.\\', \\'Upgraded but still having issues? Contact us\\']'"
     ]
    }
   ],
   "source": [
    "cna_text1 = getCNAurl()\n",
    "cna_text2 = getCNAurl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee8a1e",
   "metadata": {},
   "source": [
    "##### Cleaning SOF text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cabf88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_text1 = sof_text1[4:]\n",
    "sof_text1 = sof_text1[:-10]\n",
    "temp_list = []\n",
    "for line in sof_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "sof_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5be9617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm trying to make subplots in pandas of certain graphs and i keep getting this error: float() argument must be a string or a number, not 'AxesSubplot' any ideas on what fix could be used?\",\n",
       " 'Code preview here',\n",
       " \"Well, according to exception traceback, you'r trying to convert graph_sex_and_wellbeing tofloat.\",\n",
       " 'And float in Python can accept either string or number argument.',\n",
       " 'graph_sex_and_wellbeing has typeAxesSubplot.',\n",
       " 'You need to get a string or number from this variable and pass it to a float.']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sof_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dc0f4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_text2 = sof_text2[4:]\n",
    "sof_text2 = sof_text2[:-10]\n",
    "temp_list = []\n",
    "for line in sof_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "sof_text2 = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fb7f9",
   "metadata": {},
   "source": [
    "##### Cleaning CNA text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626d0987",
   "metadata": {},
   "source": [
    "##### Cleaning HWZ text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5c822fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_text1 = hwz_text1[:-5]\n",
    "temp_list = []\n",
    "for line in hwz_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "hwz_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d96f688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_text2 = hwz_text2[:-5]\n",
    "temp_list = []\n",
    "for line in hwz_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "hwz_text2 = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11179763",
   "metadata": {},
   "source": [
    "##### First word in sentence capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "93702e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for sof_text1:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in sof_text1:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for sof_text1: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b7ebe5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for sof_text2:  1.0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in sof_text2:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for sof_text2: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "913e6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for hwz_text1:  0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in hwz_text1:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for hwz_text1: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d80943eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for hwz_text2:  0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "uppercount=0\n",
    "for sent in hwz_text2:\n",
    "    if sent[0].isupper():\n",
    "        uppercount+=1\n",
    "    count+=1\n",
    "print(\"Fraction of first letter being capitalised for hwz_text2: \", uppercount/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e31e9b",
   "metadata": {},
   "source": [
    "##### Length of articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1137d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentences in sof_text1:  6\n",
      "No of sentences in sof_text2:  11\n",
      "No of sentences in hwz_text1:  42\n",
      "No of sentences in hwz_text2:  42\n"
     ]
    }
   ],
   "source": [
    "print(\"No of sentences in sof_text1: \", len(sof_text1))\n",
    "print(\"No of sentences in sof_text2: \", len(sof_text2))\n",
    "print(\"No of sentences in hwz_text1: \", len(hwz_text1))\n",
    "print(\"No of sentences in hwz_text2: \", len(hwz_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90754d4",
   "metadata": {},
   "source": [
    "##### Proper nouns capitalised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7a5c0f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in sof_text1:  1.0\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in sof_text1:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in sof_text1: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aa0d3f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in sof_text2:  0.0\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in sof_text2:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in sof_text2: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "66a68814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in hwz_text1:  0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in hwz_text1:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in hwz_text1: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "adda956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in hwz_text2:  0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "uppercount = 0\n",
    "count = 0\n",
    "for sentence in hwz_text2:\n",
    "    tagged.append(nlp(sentence))\n",
    "for tag in tagged:\n",
    "    for token in tag:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            if token.text[0].isupper():\n",
    "                uppercount += 1\n",
    "            count += 1\n",
    "print('Fraction of proper nouns capitalised in hwz_text2: ', uppercount/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What kind of proper nouns used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stack Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "form 5\n",
      "string 3\n",
      "number 3\n",
      "line 3\n",
      "float 2\n",
      "argument 2\n",
      "way 2\n",
      "subplots 1\n",
      "pandas 1\n",
      "graphs 1\n",
      "error 1\n",
      "AxesSubplot 1\n",
      "ideas 1\n",
      "fix 1\n",
      "Code 1\n",
      "preview 1\n",
      "exception 1\n",
      "traceback 1\n",
      "you'r 1\n",
      "tofloat 1\n",
      "Python 1\n",
      "graph_sex_and_wellbeing 1\n",
      "typeAxesSubplot 1\n",
      "variable 1\n",
      "icons 1\n",
      "search 1\n",
      "php 1\n",
      "example 1\n",
      "styles 1\n",
      "style.css 1\n",
      "searchform.php 1\n",
      "element 1\n",
      "display 1\n",
      "inline 1\n",
      "making 1\n"
     ]
    }
   ],
   "source": [
    "sof_tagged = []\n",
    "sof_noun_dict = {}\n",
    "for sentence in sof_text1:\n",
    "    sof_tagged.append(nlp(sentence))\n",
    "for sentence in sof_text2:\n",
    "    sof_tagged.append(nlp(sentence))\n",
    "for tagged in sof_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in sof_noun_dict.keys():\n",
    "                sof_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                sof_noun_dict[token.text] = 1\n",
    "\n",
    "sof_noun_dict_sorted = sorted(sof_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sof_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squid 20\n",
      "Game 20\n",
      "people 16\n",
      "game 12\n",
      "games 12\n",
      "show 10\n",
      "episodes 10\n",
      "money 8\n",
      "story 8\n",
      "Netflix 6\n",
      "premise 6\n",
      "prize 6\n",
      "time 6\n",
      "drama 4\n",
      "week 4\n",
      "themes 4\n",
      "characters 4\n",
      "lives 4\n",
      "location 4\n",
      "episode 4\n",
      "’s 4\n",
      "Light 4\n",
      "center 4\n",
      "Gi 4\n",
      "hun 4\n",
      "cast 4\n",
      "one 4\n",
      "winner 4\n",
      "person 4\n",
      "Fall 2\n",
      "Guys 2\n",
      "Hwang 2\n",
      "Dong 2\n",
      "hyuk 2\n",
      "Fortress 2\n",
      "Miss 2\n",
      "Granny 2\n",
      "parade 2\n",
      "Originals 2\n",
      "production 2\n",
      "design 2\n",
      "setpieces 2\n",
      "battle 2\n",
      "royale 2\n",
      "gauntlet 2\n",
      "minigames 2\n",
      "sum 2\n",
      "subtlety 2\n",
      "sledgehammer 2\n",
      "thanks 2\n",
      "choices 2\n",
      "expectations 2\n",
      "landing 2\n",
      "centers 2\n",
      "group 2\n",
      "survival 2\n",
      "cash 2\n",
      "brink 2\n",
      "need 2\n",
      "unfold 2\n",
      "pilot 2\n",
      "tour 2\n",
      "motivation 2\n",
      "sneak 2\n",
      "peek 2\n",
      "brutality 2\n",
      "participants 2\n",
      "fun 2\n",
      "air 2\n",
      "sea 2\n",
      "terms 2\n",
      "participation 2\n",
      "trailer 2\n",
      "sense 2\n",
      "unease 2\n",
      "point 2\n",
      "environments 2\n",
      "numbers 2\n",
      "humans 2\n",
      "faceless 2\n",
      "grunts 2\n",
      "suits 2\n",
      "guns 2\n",
      "arena 2\n",
      "field 2\n",
      "doll 2\n",
      "kid 2\n",
      "Red 2\n",
      "Green 2\n",
      "bullets 2\n",
      "winning 2\n",
      "cost 2\n",
      "chase 2\n",
      "stakes 2\n",
      "morbid 2\n",
      "entertainment 2\n",
      "thing 2\n",
      "protagonist 2\n",
      "gambling 2\n",
      "habit 2\n",
      "debt 2\n",
      "loan 2\n",
      "sharks 2\n",
      "family 2\n",
      "mother 2\n",
      "daughter 2\n",
      "heart 2\n",
      "tale 2\n",
      "humour 2\n",
      "levity 2\n",
      "situations 2\n",
      "face 2\n",
      "reasons 2\n",
      "world 2\n",
      "Others 2\n",
      "strangers 2\n",
      "bunch 2\n",
      "slain 2\n",
      "level 2\n",
      "investment 2\n",
      "friends 2\n",
      "series 2\n",
      "territory 2\n",
      "novelty 2\n",
      "likeability 2\n",
      "punches 2\n",
      "helpings 2\n",
      "tragedy 2\n",
      "concern 2\n",
      "ending 2\n",
      "end 2\n",
      "road 2\n",
      "organisers 2\n",
      "scot 2\n",
      "store 2\n",
      "endgame 2\n",
      "identity 2\n",
      "mystery 2\n",
      "horrors 2\n",
      "capitalism 2\n",
      "September 2\n",
      "feedback 2\n",
      "article 2\n",
      "team 2\n"
     ]
    }
   ],
   "source": [
    "hwz_tagged = []\n",
    "hwz_noun_dict = {}\n",
    "for sentence in hwz_text1:\n",
    "    hwz_tagged.append(nlp(sentence))\n",
    "for sentence in hwz_text2:\n",
    "    hwz_tagged.append(nlp(sentence))\n",
    "for tagged in hwz_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in hwz_noun_dict.keys():\n",
    "                hwz_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                hwz_noun_dict[token.text] = 1\n",
    "\n",
    "hwz_noun_dict_sorted = sorted(hwz_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in hwz_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac262786",
   "metadata": {},
   "source": [
    "##### Good grammar?\n",
    "1. Tense matching\n",
    "2. Subject-verb agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subject-verb agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5c3b53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/pes10k/cs412-scorer/blob/master/agreement_utils.py\n",
    "singular_noun_tags = ('NN', 'NNP')\n",
    "plural_noun_tags = ('NNS', 'NNPS')\n",
    "\n",
    "noun_tags = singular_noun_tags + plural_noun_tags + ('PRP',)\n",
    "\n",
    "plural_verb_tags = ('VBZ',)\n",
    "singular_verb_tags = ('VBP',)\n",
    "general_verb_tags = ('VBD', 'VB')\n",
    "\n",
    "verb_tags = singular_verb_tags + plural_verb_tags + general_verb_tags\n",
    "\n",
    "plural_prop_nouns = ('they', 'we', 'them', 'themselves', 'us', 'those')\n",
    "singular_prop_nouns = ('he', 'she', 'i', 'him', 'me', 'myself', 'it')\n",
    "\n",
    "singulars = singular_noun_tags + singular_verb_tags + singular_prop_nouns\n",
    "plurals = plural_noun_tags + plural_verb_tags + plural_prop_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"i'm\", \"i'm\"), (\"i'm\", 'pandas'), (\"i'm\", 'graphs'), (\"i'm\", 'keep'), (\"i'm\", 'error:'), (\"i'm\", 'float()'), (\"i'm\", 'argument'), (\"i'm\", 'string'), (\"i'm\", 'fix'), ('subplots', 'subplots'), ('subplots', 'ideas'), ('pandas', 'pandas'), ('pandas', 'graphs'), ('pandas', 'keep'), ('pandas', 'error:'), ('pandas', 'float()'), ('pandas', 'argument'), ('pandas', 'string'), ('pandas', 'fix'), ('graphs', 'graphs'), ('graphs', 'keep'), ('graphs', 'error:'), ('graphs', 'float()'), ('graphs', 'argument'), ('graphs', 'string'), ('graphs', 'fix'), ('keep', 'keep'), ('keep', 'error:'), ('keep', 'float()'), ('keep', 'argument'), ('keep', 'string'), ('keep', 'fix'), ('error:', 'error:'), ('error:', 'float()'), ('error:', 'argument'), ('error:', 'string'), ('error:', 'fix'), ('float()', 'float()'), ('float()', 'argument'), ('float()', 'string'), ('float()', 'fix'), ('argument', 'argument'), ('argument', 'string'), ('argument', 'fix'), ('string', 'string'), ('string', 'fix'), ('ideas', 'ideas'), ('fix', 'fix'), ('code', 'code'), ('code', 'preview'), ('preview', 'preview'), ('well,', 'well,'), ('well,', 'exception'), ('well,', 'traceback,'), ('well,', \"you'r\"), ('well,', 'tofloat.'), ('exception', 'exception'), ('exception', 'traceback,'), ('exception', \"you'r\"), ('exception', 'tofloat.'), ('traceback,', 'traceback,'), ('traceback,', \"you'r\"), ('traceback,', 'tofloat.'), (\"you'r\", \"you'r\"), (\"you'r\", 'tofloat.'), ('tofloat.', 'tofloat.'), ('float', 'float'), ('float', 'python'), ('float', 'number'), ('float', 'argument.'), ('python', 'python'), ('python', 'number'), ('python', 'argument.'), ('number', 'number'), ('number', 'argument.'), ('argument.', 'argument.'), ('graph_sex_and_wellbeing', 'graph_sex_and_wellbeing'), ('has', 'has'), ('need', 'need'), ('need', 'string'), ('need', 'number'), ('need', 'pass'), ('need', 'float.'), ('string', 'string'), ('string', 'number'), ('string', 'pass'), ('string', 'float.'), ('number', 'number'), ('number', 'pass'), ('number', 'float.'), ('pass', 'pass'), ('pass', 'float.'), ('float.', 'float.')]\n",
      "[(\"i'm\", 'subplots'), (\"i'm\", 'ideas'), ('subplots', 'pandas'), ('subplots', 'graphs'), ('subplots', 'keep'), ('subplots', 'error:'), ('subplots', 'float()'), ('subplots', 'argument'), ('subplots', 'string'), ('subplots', 'fix'), ('pandas', 'ideas'), ('graphs', 'ideas'), ('keep', 'ideas'), ('error:', 'ideas'), ('float()', 'ideas'), ('argument', 'ideas'), ('string', 'ideas'), ('ideas', 'fix'), ('graph_sex_and_wellbeing', 'has')]\n",
      "SVA ratio in sof_text1:  0.8303571428571429\n"
     ]
    }
   ],
   "source": [
    "agreed = 0\n",
    "disagreed = 0\n",
    "agree_pairs = []\n",
    "disagree_pairs = []\n",
    "for sentence in sof_text1:\n",
    "    sentence = sentence.lower().split()\n",
    "    tagged = nltk.pos_tag(sentence)\n",
    "    for index, word_tag_pair in enumerate(tagged):\n",
    "        if word_tag_pair[1] in singulars:\n",
    "            for j in range(index, len(tagged)):\n",
    "                if tagged[j][1] in singulars:\n",
    "                    agree_pairs.append((word_tag_pair[0], tagged[j][0]))\n",
    "                    agreed += 1\n",
    "                elif tagged[j][1] in plurals:\n",
    "                    disagree_pairs.append((word_tag_pair[0], tagged[j][0]))\n",
    "                    disagreed += 1\n",
    "        elif word_tag_pair[1] in plurals:\n",
    "            for j in range(index, len(tagged)):\n",
    "                if tagged[j][1] in singulars:\n",
    "                    disagree_pairs.append((word_tag_pair[0], tagged[j][0]))\n",
    "                    disagreed += 1\n",
    "                elif tagged[j][1] in plurals:\n",
    "                    agree_pairs.append((word_tag_pair[0], tagged[j][0]))\n",
    "                    agreed += 1\n",
    "print(agree_pairs)\n",
    "print(disagree_pairs)\n",
    "print(\"SVA ratio in sof_text1: \", agreed/(agreed+disagreed))\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
