{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e50125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from collections import Counter\n",
    "# import httplib2\n",
    "import itertools\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd \n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import urllib.request\n",
    "from urllib.request import urlopen, Request\n",
    "import random\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9d9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e81b680",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ede952",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_json('../data/reviewSelected100.json', encoding='ISO-8859-1', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127870f",
   "metadata": {},
   "source": [
    "## 3.2 Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d7a9f",
   "metadata": {},
   "source": [
    "### Tokenisation and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6091b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9308</th>\n",
       "      <td>ogULNfkkK9m0tsAf3uTpYg</td>\n",
       "      <td>EBLSMfHBAxg5wu-PHY9bdA</td>\n",
       "      <td>QxRKQ_Lzu4lMOejwVgcdAg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Their lounge fries are fantastic. Always get t...</td>\n",
       "      <td>2017-10-02 02:10:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9316</th>\n",
       "      <td>dvGmTwBkKMHSfxUteJ1HMA</td>\n",
       "      <td>KeQpOId8uJhlbU_Jzv9Tew</td>\n",
       "      <td>QxRKQ_Lzu4lMOejwVgcdAg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>My co-workers and I come here for lunch severa...</td>\n",
       "      <td>2017-01-14 21:26:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9327</th>\n",
       "      <td>2Cii2hAnBi3_crTcRbLGNw</td>\n",
       "      <td>7nIzgNF7YzI-UAyYr3y9hw</td>\n",
       "      <td>QxRKQ_Lzu4lMOejwVgcdAg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NPL is a great restaurant and I go there weekl...</td>\n",
       "      <td>2013-09-04 04:02:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9343</th>\n",
       "      <td>gciRG7KNEiAcNR5D9xYAGA</td>\n",
       "      <td>HdRdLgvD34CeCd7UAS5iEw</td>\n",
       "      <td>QxRKQ_Lzu4lMOejwVgcdAg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We have eaten here for years and really enjoy ...</td>\n",
       "      <td>2018-02-11 04:03:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9352</th>\n",
       "      <td>TJlJkT5dJOofomrcvGDhVw</td>\n",
       "      <td>1wpILrjIBzJ5wUAa0_5TBQ</td>\n",
       "      <td>QxRKQ_Lzu4lMOejwVgcdAg</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NPL is very close to home and I have come here...</td>\n",
       "      <td>2012-11-06 20:29:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   review_id                 user_id             business_id  \\\n",
       "9308  ogULNfkkK9m0tsAf3uTpYg  EBLSMfHBAxg5wu-PHY9bdA  QxRKQ_Lzu4lMOejwVgcdAg   \n",
       "9316  dvGmTwBkKMHSfxUteJ1HMA  KeQpOId8uJhlbU_Jzv9Tew  QxRKQ_Lzu4lMOejwVgcdAg   \n",
       "9327  2Cii2hAnBi3_crTcRbLGNw  7nIzgNF7YzI-UAyYr3y9hw  QxRKQ_Lzu4lMOejwVgcdAg   \n",
       "9343  gciRG7KNEiAcNR5D9xYAGA  HdRdLgvD34CeCd7UAS5iEw  QxRKQ_Lzu4lMOejwVgcdAg   \n",
       "9352  TJlJkT5dJOofomrcvGDhVw  1wpILrjIBzJ5wUAa0_5TBQ  QxRKQ_Lzu4lMOejwVgcdAg   \n",
       "\n",
       "      stars  useful  funny  cool  \\\n",
       "9308      4       0      0     0   \n",
       "9316      5       0      0     0   \n",
       "9327      4       0      0     0   \n",
       "9343      5       0      0     0   \n",
       "9352      2       0      0     0   \n",
       "\n",
       "                                                   text                date  \n",
       "9308  Their lounge fries are fantastic. Always get t... 2017-10-02 02:10:28  \n",
       "9316  My co-workers and I come here for lunch severa... 2017-01-14 21:26:31  \n",
       "9327  NPL is a great restaurant and I go there weekl... 2013-09-04 04:02:47  \n",
       "9343  We have eaten here for years and really enjoy ... 2018-02-11 04:03:44  \n",
       "9352  NPL is very close to home and I have come here... 2012-11-06 20:29:53  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get reviews for a random business \n",
    "random_business = reviews.sample()\n",
    "random_business_id = random_business.iloc[0]['business_id']\n",
    "small_business_dataset = reviews.loc[reviews['business_id'] == random_business_id]\n",
    "small_business_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3b9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_business_dataset_reviews = list(small_business_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda54073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the reviews into a concatenated string \n",
    "b1_review = ''.join(small_business_dataset_reviews)\n",
    "clean_review = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", b1_review)\n",
    "b1_review = nlp(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0e2d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 370), ('and', 356), ('a', 303), ('I', 206), ('to', 205), ('was', 184), ('of', 135), ('is', 133), ('for', 114), ('The', 99)]\n"
     ]
    }
   ],
   "source": [
    "# removed punctuation and get the top 10 most common words (including stopwords)\n",
    "b1_review_words = [token.text for token in b1_review if token.is_alpha == True] \n",
    "b1_word_freq = Counter(b1_review_words)\n",
    "common_words = b1_word_freq.most_common(10)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dccef76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 69), ('good', 68), ('nt', 68), ('place', 57), ('fries', 45), ('bar', 37), ('like', 33), ('great', 32), ('fish', 32), ('sandwich', 29)]\n"
     ]
    }
   ],
   "source": [
    "# removed punctuation and get the top 10 most common words (excluding stopwords)\n",
    "b1_review_words = [token.text for token in b1_review if token.is_stop != True and token.is_alpha == True] \n",
    "b1_word_freq = Counter(b1_review_words)\n",
    "common_words = b1_word_freq.most_common(10)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb716768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot log graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7be04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do some stemming after removing the stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "porter_st = PorterStemmer()\n",
    "lancaster_st = LancasterStemmer()\n",
    "snow_st = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eeb6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 73), ('good', 72), ('nt', 68), ('place', 68), ('fri', 60), ('order', 49), ('sandwich', 43), ('great', 42), ('like', 41), ('bar', 39)]\n"
     ]
    }
   ],
   "source": [
    "# Using Porter Stemmer\n",
    "porter_stemmed_words = [porter_st.stem(word) for word in b1_review_words]\n",
    "porter_freq = Counter(porter_stemmed_words)\n",
    "porter_common = porter_freq.most_common(10)\n",
    "print(porter_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da263e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 73), ('good', 72), ('nt', 68), ('plac', 68), ('fri', 60), ('serv', 53), ('ord', 49), ('sandwich', 43), ('gre', 42), ('lik', 41)]\n"
     ]
    }
   ],
   "source": [
    "# Using Lancaster Stemmer\n",
    "lancaster_stemmed_words = [lancaster_st.stem(word) for word in b1_review_words]\n",
    "lancaster_freq = Counter(lancaster_stemmed_words)\n",
    "lancaster_common = lancaster_freq.most_common(10)\n",
    "print(lancaster_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5360dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 73), ('good', 72), ('nt', 68), ('place', 68), ('fri', 60), ('order', 49), ('sandwich', 43), ('great', 42), ('like', 41), ('bar', 39)]\n"
     ]
    }
   ],
   "source": [
    "# Using Snowball Stemmer\n",
    "snow_stemmed_words = [snow_st.stem(word) for word in b1_review_words]\n",
    "snow_freq = Counter(snow_stemmed_words)\n",
    "snow_common = snow_freq.most_common(10)\n",
    "print(snow_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc079e",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7913cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sentences = reviews.sample(5, random_state=42)\n",
    "random_sentences = list(random_sentences['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd9f5250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Que ce soit pour leurs délicieux bubbles tea/smooties, leurs ''Bánh mì'' , leurs petits snacks (viennoiseries, tapioca, ...), on adore Vua et aussi leurs prix très abordables. On y retourne lorsqu'on est dans le Quartier Latin !\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a623df2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Que', 'NNP'),\n",
       "  ('ce', 'NN'),\n",
       "  ('soit', 'VBD'),\n",
       "  ('pour', 'JJ'),\n",
       "  ('leurs', 'NNS'),\n",
       "  ('délicieux', 'VBP'),\n",
       "  ('bubbles', 'NNS'),\n",
       "  ('tea/smooties', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('leurs', 'VBZ'),\n",
       "  ('``', '``'),\n",
       "  ('Bánh', 'NNP'),\n",
       "  ('mì', 'NN'),\n",
       "  (\"''\", \"''\"),\n",
       "  (',', ','),\n",
       "  ('leurs', 'VBZ'),\n",
       "  ('petits', 'NNS'),\n",
       "  ('snacks', 'NNS'),\n",
       "  ('(', '('),\n",
       "  ('viennoiseries', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('tapioca', 'NN'),\n",
       "  (',', ','),\n",
       "  ('...', ':'),\n",
       "  (')', ')'),\n",
       "  (',', ','),\n",
       "  ('on', 'IN'),\n",
       "  ('adore', 'IN'),\n",
       "  ('Vua', 'NNP'),\n",
       "  ('et', 'CC'),\n",
       "  ('aussi', 'JJ'),\n",
       "  ('leurs', 'NNS'),\n",
       "  ('prix', 'VBP'),\n",
       "  ('très', 'JJ'),\n",
       "  ('abordables', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('On', 'IN'),\n",
       "  ('y', 'JJ'),\n",
       "  ('retourne', 'JJ'),\n",
       "  (\"lorsqu'on\", 'NN'),\n",
       "  ('est', 'JJS'),\n",
       "  ('dans', 'NNS'),\n",
       "  ('le', 'VBP'),\n",
       "  ('Quartier', 'NNP'),\n",
       "  ('Latin', 'NNP'),\n",
       "  ('!', '.')],\n",
       " [('As', 'IN'),\n",
       "  ('I', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('said', 'VBD'),\n",
       "  ('previously', 'RB'),\n",
       "  ('...', ':'),\n",
       "  ('we', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('been', 'VBN'),\n",
       "  ('coming', 'VBG'),\n",
       "  ('to', 'TO'),\n",
       "  ('LMAH', 'NNP'),\n",
       "  ('for', 'IN'),\n",
       "  ('over', 'IN'),\n",
       "  ('10', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('At', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('15', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('And', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  (\"'ve\", 'VBP'),\n",
       "  ('ALWAYS', 'NNP'),\n",
       "  ('seen', 'VBN'),\n",
       "  ('Dr.', 'NNP'),\n",
       "  ('White', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('bring', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('to', 'TO'),\n",
       "  ('have', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('urine', 'JJ'),\n",
       "  ('sample', 'NN'),\n",
       "  ('done', 'VBN'),\n",
       "  ('.', '.'),\n",
       "  ('While', 'IN'),\n",
       "  ('being', 'VBG'),\n",
       "  ('done', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('pee', 'NN'),\n",
       "  (\"'d\", 'MD'),\n",
       "  ('on', 'IN'),\n",
       "  ('himself', 'PRP'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('vet', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('staff', 'NN'),\n",
       "  ('only', 'RB'),\n",
       "  ('tried', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('clean', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('off', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('him', 'PRP'),\n",
       "  ('with', 'IN'),\n",
       "  ('alcohol', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('try', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('actually', 'RB'),\n",
       "  ('get', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('off', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('him', 'PRP'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('TELL', 'VB'),\n",
       "  ('ME', 'NNP'),\n",
       "  ('that', 'IN'),\n",
       "  ('he', 'PRP'),\n",
       "  ('pee', 'VBZ'),\n",
       "  (\"'d\", 'MD'),\n",
       "  ('all', 'DT'),\n",
       "  ('over', 'IN'),\n",
       "  ('his', 'PRP$'),\n",
       "  ('stomach', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('legs', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('So', 'RB'),\n",
       "  ('now', 'RB'),\n",
       "  ('I', 'PRP'),\n",
       "  ('e', 'VBP'),\n",
       "  ('had', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('give', 'VB'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('a', 'DT'),\n",
       "  ('bath', 'NN'),\n",
       "  ('because', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('ca', 'MD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('take', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('urine', 'JJ'),\n",
       "  ('sample', 'NN'),\n",
       "  ('properly', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('LIVID', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('Pretty', 'NNP'),\n",
       "  ('decent', 'NN'),\n",
       "  ('but', 'CC'),\n",
       "  ('so', 'RB'),\n",
       "  ('spacious', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Very', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('for', 'IN'),\n",
       "  ('kids', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('order', 'NN'),\n",
       "  ('dim', 'VB'),\n",
       "  ('sum', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  (\"'re\", 'VBP'),\n",
       "  ('open', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('Free', 'JJ'),\n",
       "  ('wifi', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('awesome', 'JJ'),\n",
       "  (':', ':'),\n",
       "  (')', ')')],\n",
       " [('Dumpling', 'VBG'),\n",
       "  ('Village', 'NNP'),\n",
       "  ('really', 'RB'),\n",
       "  ('suffering', 'VBG'),\n",
       "  ('from', 'IN'),\n",
       "  ('identity', 'NN'),\n",
       "  ('crisis', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('Is', 'VBZ'),\n",
       "  ('it', 'PRP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('Restaurant', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('mostly', 'RB'),\n",
       "  ('Northern', 'NNP'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('items', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('menu', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('Is', 'VBZ'),\n",
       "  ('it', 'PRP'),\n",
       "  ('a', 'DT'),\n",
       "  ('Northern', 'NNP'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('restaurant', 'NN'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('lot', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('dishes', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('menu', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('pork', 'NN'),\n",
       "  ('bone', 'NN'),\n",
       "  ('soup', 'NN'),\n",
       "  (',', ','),\n",
       "  ('cold', 'JJ'),\n",
       "  ('wheat', 'NN'),\n",
       "  ('bucknoodle', 'NN'),\n",
       "  ('soup', 'NN'),\n",
       "  (',', ','),\n",
       "  ('kimchi', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('bean', 'NN'),\n",
       "  ('sprout', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('appetitizers', 'NNS'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('only', 'JJ'),\n",
       "  ('Korean', 'JJ'),\n",
       "  ('characters', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('3', 'CD'),\n",
       "  ('on', 'IN'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('business', 'NN'),\n",
       "  ('sign', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('front', 'NN'),\n",
       "  ('..', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('rest', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('in', 'IN'),\n",
       "  ('Chinese', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('English', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('All', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('wait', 'NN'),\n",
       "  ('staffs', 'NNS'),\n",
       "  ('here', 'RB'),\n",
       "  ('speak', 'VBP'),\n",
       "  ('Mandarian', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('even', 'RB'),\n",
       "  ('most', 'JJS'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('patrons', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('Chinese', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('It', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('located', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('really', 'RB'),\n",
       "  ('tiny', 'JJ'),\n",
       "  ('strip', 'NN'),\n",
       "  ('mall', 'NN'),\n",
       "  ('..', 'NN'),\n",
       "  ('with', 'IN'),\n",
       "  ('limited', 'JJ'),\n",
       "  ('parkings', 'NNS'),\n",
       "  ('!', '.'),\n",
       "  ('To', 'TO'),\n",
       "  ('tell', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('the', 'DT'),\n",
       "  ('truth', 'NN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('am', 'VBP'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('area', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('over', 'IN'),\n",
       "  ('10', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('I', 'PRP'),\n",
       "  ('never', 'RB'),\n",
       "  ('notice', 'VBP'),\n",
       "  ('it', 'PRP'),\n",
       "  ('until', 'IN'),\n",
       "  ('I', 'PRP'),\n",
       "  ('read', 'VBP'),\n",
       "  ('the', 'DT'),\n",
       "  ('reviews', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('yelp', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('around', 'RB'),\n",
       "  ('6', 'CD'),\n",
       "  ('pm', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('Saturday', 'NNP'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('restaurant', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('almost', 'RB'),\n",
       "  ('full', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('tables', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('really', 'RB'),\n",
       "  ('packed', 'VBN'),\n",
       "  ('together', 'RB'),\n",
       "  (',', ','),\n",
       "  ('so', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('still', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('a', 'DT'),\n",
       "  ('certain', 'JJ'),\n",
       "  ('degree', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('privacy', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('tell', 'VB'),\n",
       "  ('what', 'WP'),\n",
       "  ('the', 'DT'),\n",
       "  ('table', 'NN'),\n",
       "  ('next', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('me', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('eating', 'VBG'),\n",
       "  ('.', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('decided', 'VBD'),\n",
       "  ('on', 'IN'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('favourite', 'JJ'),\n",
       "  ('chives', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('pork', 'NN'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('kimchi', 'NNS'),\n",
       "  ('fried', 'VBD'),\n",
       "  ('rice', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('Korean', 'NNP'),\n",
       "  ('cold', 'VBD'),\n",
       "  ('bucknoodle', 'JJ'),\n",
       "  ('soup', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('$', '$'),\n",
       "  ('4.99', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('noodle', 'JJ'),\n",
       "  ('soup', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('so', 'RB'),\n",
       "  ('so', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Nice', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('refreshing', 'VBG'),\n",
       "  ('!', '.'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('based', 'VBN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('sour', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('really', 'RB'),\n",
       "  ('stimulate', 'VB'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('tastebuds', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('just', 'RB'),\n",
       "  ('make', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('want', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('eat', 'VB'),\n",
       "  ('more', 'JJR'),\n",
       "  ('!', '.'),\n",
       "  ('(', '('),\n",
       "  ('I', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('finish', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('myself', 'PRP'),\n",
       "  ('!', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('want', 'VB'),\n",
       "  ('to', 'TO'),\n",
       "  ('share', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('I', 'PRP'),\n",
       "  ('want', 'VBP'),\n",
       "  ('one', 'CD'),\n",
       "  ('all', 'DT'),\n",
       "  ('for', 'IN'),\n",
       "  ('myself', 'PRP'),\n",
       "  ('next', 'JJ'),\n",
       "  ('time', 'NN'),\n",
       "  ('!', '.'),\n",
       "  (')', ')'),\n",
       "  ('The', 'DT'),\n",
       "  ('kimchi', 'NN'),\n",
       "  ('fried', 'VBD'),\n",
       "  ('rice', 'NN'),\n",
       "  ('..', 'NNP'),\n",
       "  ('hmmmmm', 'NN'),\n",
       "  ('not', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('A', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('on', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('oily', 'JJ'),\n",
       "  ('side', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'EX'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('much', 'JJ'),\n",
       "  ('taste', 'NN'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('spicy', 'VB'),\n",
       "  ('enough', 'RB'),\n",
       "  ('?', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('could', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('finish', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('and', 'CC'),\n",
       "  ('end', 'VB'),\n",
       "  ('up', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('pack', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('to', 'TO'),\n",
       "  ('go', 'VB'),\n",
       "  ('...', ':'),\n",
       "  ('(', '('),\n",
       "  ('not', 'RB'),\n",
       "  ('even', 'RB'),\n",
       "  ('sure', 'JJ'),\n",
       "  ('I', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('eat', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  (',', ','),\n",
       "  ('but', 'CC'),\n",
       "  ('can', 'MD'),\n",
       "  ('not', 'RB'),\n",
       "  ('just', 'RB'),\n",
       "  ('let', 'VB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('all', 'DT'),\n",
       "  ('go', 'VBP'),\n",
       "  ('to', 'TO'),\n",
       "  ('waste', 'NN'),\n",
       "  (')', ')'),\n",
       "  ('The', 'DT'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('last', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('it', 'PRP'),\n",
       "  ('really', 'RB'),\n",
       "  ('worth', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('wait', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('14', 'CD'),\n",
       "  ('big', 'JJ'),\n",
       "  ('fat', 'NN'),\n",
       "  ('hot', 'JJ'),\n",
       "  ('dumplings', 'NNS'),\n",
       "  ('filled', 'VBN'),\n",
       "  ('up', 'RP'),\n",
       "  ('the', 'DT'),\n",
       "  ('whole', 'JJ'),\n",
       "  ('steamer', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('Yes', 'UH'),\n",
       "  ('I', 'PRP'),\n",
       "  ('counted', 'VBD'),\n",
       "  ('them', 'PRP'),\n",
       "  ('14', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('them', 'PRP'),\n",
       "  ('for', 'IN'),\n",
       "  ('$', '$'),\n",
       "  ('4.99', 'CD'),\n",
       "  ('.', '.'),\n",
       "  ('They', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('freshly', 'RB'),\n",
       "  ('made', 'VBN'),\n",
       "  (',', ','),\n",
       "  ('freshly', 'RB'),\n",
       "  ('steamed', 'VBN'),\n",
       "  ('!', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('skin', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('nice', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('thin', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('not', 'RB'),\n",
       "  ('chewy', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('thick', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('Oh', 'NNP'),\n",
       "  ('so', 'RB'),\n",
       "  ('delicious', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('One', 'CD'),\n",
       "  ('bite', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('all', 'PDT'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('are', 'VBP'),\n",
       "  ('ozzing', 'VBG'),\n",
       "  ('out', 'RP'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('if', 'IN'),\n",
       "  ('they', 'PRP'),\n",
       "  ('are', 'VBP'),\n",
       "  ('not', 'RB'),\n",
       "  ('burning', 'VBG'),\n",
       "  ('hot', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('you', 'PRP'),\n",
       "  ('can', 'MD'),\n",
       "  ('just', 'RB'),\n",
       "  ('stuff', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('whole', 'JJ'),\n",
       "  ('thing', 'NN'),\n",
       "  ('into', 'IN'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('mouth', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('let', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('soup', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('favour', 'NN'),\n",
       "  ('just', 'RB'),\n",
       "  ('explosed', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('mouth', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('Just', 'VB'),\n",
       "  ('a', 'DT'),\n",
       "  ('warning', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('If', 'IN'),\n",
       "  ('you', 'PRP'),\n",
       "  ('going', 'VBG'),\n",
       "  ('out', 'RP'),\n",
       "  ('on', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('date', 'NN'),\n",
       "  (',', ','),\n",
       "  ('please', 'VB'),\n",
       "  ('do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('come', 'VB'),\n",
       "  ('here', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('Even', 'RB'),\n",
       "  ('it', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('good', 'JJ'),\n",
       "  ('place', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('cheap', 'JJ'),\n",
       "  ('date', 'NN'),\n",
       "  (',', ','),\n",
       "  ('our', 'PRP$'),\n",
       "  ('bill', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('$', '$'),\n",
       "  ('18', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('2', 'CD'),\n",
       "  ('and', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('full', 'JJ'),\n",
       "  ('!', '.'),\n",
       "  ('But', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('place', 'NN'),\n",
       "  ('can', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('noisy', 'RB'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('food', 'NN'),\n",
       "  ('too', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('date', 'NN'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('too', 'RB'),\n",
       "  ('busy', 'JJ'),\n",
       "  ('eating', 'VBG'),\n",
       "  ('and', 'CC'),\n",
       "  ('just', 'RB'),\n",
       "  ('ignore', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  ('!', '.'),\n",
       "  ('Or', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('keep', 'VB'),\n",
       "  ('eating', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('eating', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('you', 'PRP'),\n",
       "  ('freak', 'VBP'),\n",
       "  ('all', 'DT'),\n",
       "  ('your', 'PRP$'),\n",
       "  ('friends', 'NNS'),\n",
       "  ('out', 'RP'),\n",
       "  ('!', '.'),\n",
       "  ('The', 'DT'),\n",
       "  ('services', 'NNS'),\n",
       "  ('here', 'RB'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('cold', 'JJ'),\n",
       "  ('but', 'CC'),\n",
       "  ('efficient', 'JJ'),\n",
       "  ('.', '.'),\n",
       "  ('We', 'PRP'),\n",
       "  ('were', 'VBD'),\n",
       "  ('given', 'VBN'),\n",
       "  ('extra', 'JJ'),\n",
       "  ('napkins', 'NNS'),\n",
       "  ('without', 'IN'),\n",
       "  ('requesting', 'VBG'),\n",
       "  ('...', ':'),\n",
       "  ('Maybe', 'RB'),\n",
       "  ('we', 'PRP'),\n",
       "  ('looked', 'VBD'),\n",
       "  ('really', 'RB'),\n",
       "  ('messy', 'VBN'),\n",
       "  ('?', '.'),\n",
       "  ('?', '.'),\n",
       "  ('Cash', 'NNP'),\n",
       "  ('only', 'RB'),\n",
       "  ('!', '.')],\n",
       " [('best', 'JJS'),\n",
       "  ('pizza', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('south', 'JJ'),\n",
       "  ('side', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('huge', 'JJ'),\n",
       "  ('drink', 'NN'),\n",
       "  ('selection', 'NN'),\n",
       "  ('.', '.'),\n",
       "  ('awesome', 'VB'),\n",
       "  ('atmosphere', 'RB'),\n",
       "  ('.', '.'),\n",
       "  ('$', '$'),\n",
       "  ('4', 'CD'),\n",
       "  ('for', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('slice', 'NN'),\n",
       "  ('that', 'WDT'),\n",
       "  ('could', 'MD'),\n",
       "  ('easily', 'RB'),\n",
       "  ('feed', 'VB'),\n",
       "  ('2', 'CD'),\n",
       "  ('people', 'NNS'),\n",
       "  ('!', '.')]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tagged = []\n",
    "for sentence in random_sentences:\n",
    "    nltk_tagged.append((nltk.pos_tag(word_tokenize(sentence))))\n",
    "nltk_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ec5a536",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que      PROPN \n",
      "ce       PROPN \n",
      "soit     PROPN \n",
      "pour     PROPN \n",
      "leurs    VERB  \n",
      "délicieux NOUN  \n",
      "bubbles  NOUN  \n",
      "tea      NOUN  \n",
      "/        SYM   \n",
      "smooties NOUN  \n",
      ",        PUNCT \n",
      "leurs    NOUN  \n",
      "'        PUNCT \n",
      "'        PUNCT \n",
      "Bánh     PROPN \n",
      "mì       PROPN \n",
      "'        PUNCT \n",
      "'        PUNCT \n",
      ",        PUNCT \n",
      "leurs    VERB  \n",
      "petits   NOUN  \n",
      "snacks   NOUN  \n",
      "(        PUNCT \n",
      "viennoiseries NOUN  \n",
      ",        PUNCT \n",
      "tapioca  NOUN  \n",
      ",        PUNCT \n",
      "...      PUNCT \n",
      ")        PUNCT \n",
      ",        PUNCT \n",
      "on       ADP   \n",
      "adore    PROPN \n",
      "Vua      PROPN \n",
      "et       PROPN \n",
      "aussi    PROPN \n",
      "leurs    PROPN \n",
      "prix     PROPN \n",
      "très     PROPN \n",
      "abordables NOUN  \n",
      ".        PUNCT \n",
      "On       ADP   \n",
      "y        PROPN \n",
      "retourne PROPN \n",
      "lorsqu'on PROPN \n",
      "est      PROPN \n",
      "dans     NOUN  \n",
      "le       VERB  \n",
      "Quartier PROPN \n",
      "Latin    PROPN \n",
      "!        PUNCT \n",
      "As       SCONJ \n",
      "I        PRON  \n",
      "'ve      AUX   \n",
      "said     VERB  \n",
      "previously ADV   \n",
      "...      PUNCT \n",
      "we've    PROPN \n",
      "been     AUX   \n",
      "coming   VERB  \n",
      "to       ADP   \n",
      "LMAH     PROPN \n",
      "for      ADP   \n",
      "over     ADP   \n",
      "10       NUM   \n",
      "years    NOUN  \n",
      ".        PUNCT \n",
      "At       ADP   \n",
      "least    ADJ   \n",
      "15       NUM   \n",
      ".        PUNCT \n",
      "And      CCONJ \n",
      "we       PRON  \n",
      "'ve      AUX   \n",
      "ALWAYS   PROPN \n",
      "seen     VERB  \n",
      "Dr.      PROPN \n",
      "White    PROPN \n",
      ".        PUNCT \n",
      "\n",
      "        SPACE \n",
      "I        PRON  \n",
      "had      AUX   \n",
      "to       PART  \n",
      "bring    VERB  \n",
      "my       DET   \n",
      "cat      NOUN  \n",
      "in       ADP   \n",
      "to       PART  \n",
      "have     AUX   \n",
      "a        DET   \n",
      "urine    NOUN  \n",
      "sample   NOUN  \n",
      "done     VERB  \n",
      ".        PUNCT \n",
      "While    SCONJ \n",
      "being    AUX   \n",
      "done     VERB  \n",
      ",        PUNCT \n",
      "my       DET   \n",
      "cat      NOUN  \n",
      "pee'd    VERB  \n",
      "on       ADP   \n",
      "himself  PRON  \n",
      ".        PUNCT \n",
      "The      DET   \n",
      "vet      NOUN  \n",
      "or       CCONJ \n",
      "staff    NOUN  \n",
      "only     ADV   \n",
      "tried    VERB  \n",
      "to       PART  \n",
      "clean    VERB  \n",
      "it       PRON  \n",
      "off      ADP   \n",
      "of       ADP   \n",
      "him      PRON  \n",
      "with     ADP   \n",
      "alcohol  NOUN  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "did      AUX   \n",
      "n't      PART  \n",
      "try      VERB  \n",
      "to       PART  \n",
      "actually ADV   \n",
      "get      AUX   \n",
      "it       PRON  \n",
      "off      ADP   \n",
      "of       ADP   \n",
      "him      PRON  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "did      AUX   \n",
      "n't      PART  \n",
      "even     ADV   \n",
      "TELL     VERB  \n",
      "ME       PROPN \n",
      "that     SCONJ \n",
      "he       PRON  \n",
      "pee'd    VERB  \n",
      "all      ADV   \n",
      "over     ADP   \n",
      "his      DET   \n",
      "stomach  NOUN  \n",
      "and      CCONJ \n",
      "legs     NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "So       ADV   \n",
      "now      ADV   \n",
      "I        PRON  \n",
      "e        VERB  \n",
      "had      AUX   \n",
      "to       PART  \n",
      "give     VERB  \n",
      "my       DET   \n",
      "cat      NOUN  \n",
      "a        DET   \n",
      "bath     NOUN  \n",
      "because  SCONJ \n",
      "they     PRON  \n",
      "ca       VERB  \n",
      "n't      PART  \n",
      "take     VERB  \n",
      "a        DET   \n",
      "urine    NOUN  \n",
      "sample   NOUN  \n",
      "properly ADV   \n",
      ".        PUNCT \n",
      "I        PRON  \n",
      "am       AUX   \n",
      "LIVID    PROPN \n",
      ".        PUNCT \n",
      "Pretty   ADV   \n",
      "decent   ADJ   \n",
      "but      CCONJ \n",
      "so       ADV   \n",
      "spacious ADJ   \n",
      "!        PUNCT \n",
      "         SPACE \n",
      "Very     ADV   \n",
      "good     ADJ   \n",
      "for      ADP   \n",
      "kids     NOUN  \n",
      "and      CCONJ \n",
      "you      PRON  \n",
      "can      VERB  \n",
      "order    VERB  \n",
      "dim      NOUN  \n",
      "sum      NOUN  \n",
      "!        PUNCT \n",
      "They     PRON  \n",
      "'re      AUX   \n",
      "open     ADJ   \n",
      "!        PUNCT \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Free     ADJ   \n",
      "wifi     NOUN  \n",
      "and      CCONJ \n",
      "awesome  ADJ   \n",
      ":)       PUNCT \n",
      "Dumpling VERB  \n",
      "Village  PROPN \n",
      "really   ADV   \n",
      "suffering VERB  \n",
      "from     ADP   \n",
      "identity NOUN  \n",
      "crisis   NOUN  \n",
      "!        PUNCT \n",
      "!        PUNCT \n",
      "Is       AUX   \n",
      "it       PRON  \n",
      "a        DET   \n",
      "Korean   PROPN \n",
      "Restaurant PROPN \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "really   ADV   \n",
      ".        PUNCT \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "mostly   ADV   \n",
      "Northern ADJ   \n",
      "Chinese  ADJ   \n",
      "items    NOUN  \n",
      "on       ADP   \n",
      "the      DET   \n",
      "menu     NOUN  \n",
      "like     SCONJ \n",
      "dumplings NOUN  \n",
      ".        PUNCT \n",
      "Is       AUX   \n",
      "it       PRON  \n",
      "a        DET   \n",
      "Northern ADJ   \n",
      "Chinese  ADJ   \n",
      "restaurant NOUN  \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "really   ADV   \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "a        DET   \n",
      "lot      NOUN  \n",
      "of       ADP   \n",
      "Korean   ADJ   \n",
      "dishes   NOUN  \n",
      "         SPACE \n",
      "in       ADP   \n",
      "the      DET   \n",
      "menu     NOUN  \n",
      "like     SCONJ \n",
      "pork     NOUN  \n",
      "bone     NOUN  \n",
      "soup     NOUN  \n",
      ",        PUNCT \n",
      "cold     ADJ   \n",
      "wheat    NOUN  \n",
      "bucknoodle PROPN \n",
      "soup     PROPN \n",
      ",        PUNCT \n",
      "kimchi   NOUN  \n",
      "and      CCONJ \n",
      "bean     NOUN  \n",
      "sprout   NOUN  \n",
      "as       SCONJ \n",
      "appetitizers NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "only     ADJ   \n",
      "Korean   ADJ   \n",
      "characters NOUN  \n",
      "are      AUX   \n",
      "the      DET   \n",
      "3        NUM   \n",
      "on       ADP   \n",
      "their    DET   \n",
      "business NOUN  \n",
      "sign     NOUN  \n",
      "in       ADP   \n",
      "the      DET   \n",
      "front    NOUN  \n",
      "..       PUNCT \n",
      "and      CCONJ \n",
      "all      DET   \n",
      "the      DET   \n",
      "rest     NOUN  \n",
      "are      AUX   \n",
      "in       ADP   \n",
      "Chinese  PROPN \n",
      "and      CCONJ \n",
      "English  PROPN \n",
      ".        PUNCT \n",
      "All      DET   \n",
      "the      DET   \n",
      "wait     NOUN  \n",
      "staffs   NOUN  \n",
      "here     ADV   \n",
      "speak    VERB  \n",
      "Mandarian ADJ   \n",
      "and      CCONJ \n",
      "even     ADV   \n",
      "most     ADJ   \n",
      "of       ADP   \n",
      "the      DET   \n",
      "patrons  NOUN  \n",
      "are      AUX   \n",
      "Chinese  ADJ   \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "It       PRON  \n",
      "is       AUX   \n",
      "located  VERB  \n",
      "in       ADP   \n",
      "a        DET   \n",
      "really   ADV   \n",
      "tiny     ADJ   \n",
      "strip    PROPN \n",
      "mall     PROPN \n",
      "..       PUNCT \n",
      "with     ADP   \n",
      "limited  ADJ   \n",
      "parkings NOUN  \n",
      "!        PUNCT \n",
      "To       PART  \n",
      "tell     VERB  \n",
      "you      PRON  \n",
      "the      DET   \n",
      "truth    NOUN  \n",
      "I        PRON  \n",
      "am       AUX   \n",
      "in       ADP   \n",
      "the      DET   \n",
      "area     NOUN  \n",
      "for      ADP   \n",
      "over     ADP   \n",
      "10       NUM   \n",
      "years    NOUN  \n",
      "I        PRON  \n",
      "never    ADV   \n",
      "notice   VERB  \n",
      "it       PRON  \n",
      "until    ADP   \n",
      "I        PRON  \n",
      "read     VERB  \n",
      "the      DET   \n",
      "reviews  NOUN  \n",
      "on       ADP   \n",
      "yelp     PROPN \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "We       PRON  \n",
      "arrived  VERB  \n",
      "around   ADV   \n",
      "6        NUM   \n",
      "pm       NOUN  \n",
      "on       ADP   \n",
      "a        DET   \n",
      "Saturday PROPN \n",
      ".        PUNCT \n",
      "The      DET   \n",
      "restaurant NOUN  \n",
      "was      AUX   \n",
      "almost   ADV   \n",
      "full     ADJ   \n",
      ".        PUNCT \n",
      "         SPACE \n",
      "The      DET   \n",
      "tables   NOUN  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "really   ADV   \n",
      "packed   ADJ   \n",
      "together ADV   \n",
      ",        PUNCT \n",
      "so       ADV   \n",
      "you      PRON  \n",
      "still    ADV   \n",
      "have     AUX   \n",
      "a        DET   \n",
      "certain  ADJ   \n",
      "degree   NOUN  \n",
      "of       ADP   \n",
      "privacy  NOUN  \n",
      "at       ADP   \n",
      "least    ADJ   \n",
      "I        PRON  \n",
      "can      VERB  \n",
      "not      PART  \n",
      "tell     VERB  \n",
      "what     PRON  \n",
      "the      DET   \n",
      "table    NOUN  \n",
      "next     ADV   \n",
      "to       ADP   \n",
      "me       PRON  \n",
      "were     AUX   \n",
      "eating   VERB  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "We       PRON  \n",
      "decided  VERB  \n",
      "on       ADP   \n",
      "my       DET   \n",
      "favourite ADJ   \n",
      "chives   NOUN  \n",
      "and      CCONJ \n",
      "pork     NOUN  \n",
      "dumplings NOUN  \n",
      ",        PUNCT \n",
      "kimchi   PROPN \n",
      "fried    VERB  \n",
      "rice     NOUN  \n",
      "and      CCONJ \n",
      "Korean   ADJ   \n",
      "cold     ADJ   \n",
      "bucknoodle NOUN  \n",
      "soup     NOUN  \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "are      AUX   \n",
      "all      DET   \n",
      "$        SYM   \n",
      "4.99     NUM   \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "cold     ADJ   \n",
      "noodle   NOUN  \n",
      "soup     NOUN  \n",
      "is       AUX   \n",
      "so       ADV   \n",
      "so       ADV   \n",
      "good     ADJ   \n",
      "!        PUNCT \n",
      "Nice     ADJ   \n",
      "and      CCONJ \n",
      "refreshing ADJ   \n",
      "!        PUNCT \n",
      "and      CCONJ \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "based    VERB  \n",
      "is       AUX   \n",
      "a        DET   \n",
      "little   ADJ   \n",
      "sour     ADJ   \n",
      "and      CCONJ \n",
      "it       PRON  \n",
      "really   ADV   \n",
      "stimulate VERB  \n",
      "your     DET   \n",
      "tastebuds NOUN  \n",
      "and      CCONJ \n",
      "just     ADV   \n",
      "make     VERB  \n",
      "you      PRON  \n",
      "want     VERB  \n",
      "to       PART  \n",
      "eat      VERB  \n",
      "more     ADJ   \n",
      "!        PUNCT \n",
      "(        PUNCT \n",
      "I        PRON  \n",
      "can      VERB  \n",
      "finish   VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "myself   PRON  \n",
      "!        PUNCT \n",
      "I        PRON  \n",
      "do       AUX   \n",
      "n't      PART  \n",
      "want     VERB  \n",
      "to       PART  \n",
      "share    VERB  \n",
      "!        PUNCT \n",
      "I        PRON  \n",
      "want     VERB  \n",
      "one      NUM   \n",
      "all      DET   \n",
      "for      ADP   \n",
      "myself   PRON  \n",
      "next     ADJ   \n",
      "time     NOUN  \n",
      "!        PUNCT \n",
      ")        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "kimchi   ADJ   \n",
      "fried    VERB  \n",
      "rice     NOUN  \n",
      "..       PUNCT \n",
      "hmmmmm   VERB  \n",
      "not      PART  \n",
      "good     ADJ   \n",
      "!        PUNCT \n",
      "A        DET   \n",
      "little   ADJ   \n",
      "on       ADP   \n",
      "the      DET   \n",
      "oily     ADJ   \n",
      "side     NOUN  \n",
      ".        PUNCT \n",
      "There    PRON  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "much     ADJ   \n",
      "taste    NOUN  \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "spicy    ADJ   \n",
      "enough   ADV   \n",
      "?        PUNCT \n",
      "We       PRON  \n",
      "could    VERB  \n",
      "not      PART  \n",
      "finish   VERB  \n",
      "it       PRON  \n",
      "and      CCONJ \n",
      "end      VERB  \n",
      "up       ADP   \n",
      "have     AUX   \n",
      "to       PART  \n",
      "pack     VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "to       PART  \n",
      "go       VERB  \n",
      "...      PUNCT \n",
      "(not     PUNCT \n",
      "even     ADV   \n",
      "sure     INTJ  \n",
      "I        PRON  \n",
      "will     VERB  \n",
      "eat      VERB  \n",
      "it       PRON  \n",
      ",        PUNCT \n",
      "but      CCONJ \n",
      "can      VERB  \n",
      "not      PART  \n",
      "just     ADV   \n",
      "let      VERB  \n",
      "it       PRON  \n",
      "all      DET   \n",
      "go       VERB  \n",
      "to       ADP   \n",
      "waste    NOUN  \n",
      ")        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "dumplings NOUN  \n",
      "arrived  VERB  \n",
      "last     ADV   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "it       PRON  \n",
      "really   ADV   \n",
      "worth    VERB  \n",
      "the      DET   \n",
      "wait     NOUN  \n",
      "!        PUNCT \n",
      "14       NUM   \n",
      "big      ADJ   \n",
      "fat      ADJ   \n",
      "hot      ADJ   \n",
      "dumplings NOUN  \n",
      "filled   VERB  \n",
      "up       ADP   \n",
      "the      DET   \n",
      "whole    ADJ   \n",
      "steamer  NOUN  \n",
      "!        PUNCT \n",
      "Yes      INTJ  \n",
      "I        PRON  \n",
      "counted  VERB  \n",
      "them     PRON  \n",
      "14       NUM   \n",
      "of       ADP   \n",
      "them     PRON  \n",
      "for      ADP   \n",
      "$        SYM   \n",
      "4.99     NUM   \n",
      ".        PUNCT \n",
      "They     PRON  \n",
      "are      AUX   \n",
      "all      DET   \n",
      "freshly  ADV   \n",
      "made     VERB  \n",
      ",        PUNCT \n",
      "freshly  ADV   \n",
      "steamed  ADJ   \n",
      "!        PUNCT \n",
      "The      DET   \n",
      "skin     NOUN  \n",
      "is       AUX   \n",
      "nice     ADJ   \n",
      "and      CCONJ \n",
      "thin     ADJ   \n",
      ",        PUNCT \n",
      "not      PART  \n",
      "chewy    NOUN  \n",
      "and      CCONJ \n",
      "thick    ADJ   \n",
      "!        PUNCT \n",
      "Oh       INTJ  \n",
      "so       ADV   \n",
      "delicious ADJ   \n",
      "!        PUNCT \n",
      "One      NUM   \n",
      "bite     NOUN  \n",
      "and      CCONJ \n",
      "all      DET   \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "are      AUX   \n",
      "ozzing   VERB  \n",
      "out      ADP   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "if       SCONJ \n",
      "they     PRON  \n",
      "are      AUX   \n",
      "not      PART  \n",
      "burning  VERB  \n",
      "hot      ADJ   \n",
      ",        PUNCT \n",
      "you      PRON  \n",
      "can      VERB  \n",
      "just     ADV   \n",
      "stuff    VERB  \n",
      "the      DET   \n",
      "whole    ADJ   \n",
      "thing    NOUN  \n",
      "into     ADP   \n",
      "your     DET   \n",
      "mouth    NOUN  \n",
      "and      CCONJ \n",
      "let      VERB  \n",
      "the      DET   \n",
      "soup     NOUN  \n",
      "and      CCONJ \n",
      "favour   NOUN  \n",
      "just     ADV   \n",
      "explosed VERB  \n",
      "in       ADP   \n",
      "your     DET   \n",
      "mouth    NOUN  \n",
      ".        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Just     ADV   \n",
      "a        DET   \n",
      "warning  NOUN  \n",
      ".        PUNCT \n",
      "If       SCONJ \n",
      "you      PRON  \n",
      "going    VERB  \n",
      "out      ADP   \n",
      "on       ADP   \n",
      "a        DET   \n",
      "date     NOUN  \n",
      ",        PUNCT \n",
      "please   INTJ  \n",
      "do       AUX   \n",
      "n't      PART  \n",
      "come     VERB  \n",
      "here     ADV   \n",
      ".        PUNCT \n",
      "Even     ADV   \n",
      "it       PRON  \n",
      "is       AUX   \n",
      "a        DET   \n",
      "good     ADJ   \n",
      "place    NOUN  \n",
      "for      ADP   \n",
      "a        DET   \n",
      "cheap    ADJ   \n",
      "date     NOUN  \n",
      ",        PUNCT \n",
      "our      DET   \n",
      "bill     NOUN  \n",
      "was      AUX   \n",
      "$        SYM   \n",
      "18       NUM   \n",
      "for      ADP   \n",
      "2        NUM   \n",
      "and      CCONJ \n",
      "we       PRON  \n",
      "were     AUX   \n",
      "full     ADJ   \n",
      "!        PUNCT \n",
      "But      CCONJ \n",
      "the      DET   \n",
      "place    NOUN  \n",
      "can      VERB  \n",
      "be       AUX   \n",
      "noisy    ADJ   \n",
      ",        PUNCT \n",
      "the      DET   \n",
      "food     NOUN  \n",
      "too      ADV   \n",
      "good     ADJ   \n",
      "and      CCONJ \n",
      "your     DET   \n",
      "date     NOUN  \n",
      "will     VERB  \n",
      "be       AUX   \n",
      "too      ADV   \n",
      "busy     ADJ   \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "just     ADV   \n",
      "ignore   VERB  \n",
      "you      PRON  \n",
      "!        PUNCT \n",
      "Or       CCONJ \n",
      "you      PRON  \n",
      "will     VERB  \n",
      "keep     VERB  \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "eating   VERB  \n",
      "and      CCONJ \n",
      "you      PRON  \n",
      "freak    VERB  \n",
      "all      DET   \n",
      "your     DET   \n",
      "friends  NOUN  \n",
      "out      ADP   \n",
      "!        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "The      DET   \n",
      "services NOUN  \n",
      "here     ADV   \n",
      "is       AUX   \n",
      "cold     ADJ   \n",
      "but      CCONJ \n",
      "efficient ADJ   \n",
      ".        PUNCT \n",
      "We       PRON  \n",
      "were     AUX   \n",
      "given    VERB  \n",
      "extra    ADJ   \n",
      "napkins  NOUN  \n",
      "without  ADP   \n",
      "requesting VERB  \n",
      "...      PUNCT \n",
      "Maybe    ADV   \n",
      "we       PRON  \n",
      "looked   VERB  \n",
      "really   ADV   \n",
      "messy    ADJ   \n",
      "?        PUNCT \n",
      "?        PUNCT \n",
      "\n",
      "\n",
      "       SPACE \n",
      "Cash     VERB  \n",
      "only     ADV   \n",
      "!        PUNCT \n",
      "best     ADJ   \n",
      "pizza    PROPN \n",
      "in       ADP   \n",
      "south    PROPN \n",
      "side     NOUN  \n",
      ".        PUNCT \n",
      "huge     ADJ   \n",
      "drink    NOUN  \n",
      "selection NOUN  \n",
      ".        PUNCT \n",
      "awesome  ADJ   \n",
      "atmosphere NOUN  \n",
      ".        PUNCT \n",
      "$        SYM   \n",
      "4        NUM   \n",
      "for      ADP   \n",
      "a        DET   \n",
      "slice    NOUN  \n",
      "that     DET   \n",
      "could    VERB  \n",
      "easily   ADV   \n",
      "feed     VERB  \n",
      "2        NUM   \n",
      "people   NOUN  \n",
      "!        PUNCT \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tagged = []\n",
    "for sentence in random_sentences:\n",
    "    spacy_tagged.append(nlp(sentence))\n",
    "for tagged in spacy_tagged:\n",
    "    for token in tagged:\n",
    "        print(f'{token.text:{8}} {token.pos_:{6}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49fa66",
   "metadata": {},
   "source": [
    "# WORK COMPLETED UP TILL HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82a470",
   "metadata": {},
   "source": [
    "### Writing Style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c6762",
   "metadata": {},
   "source": [
    "#### Getting SOF article data (originally random, source code in get_urls.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee1c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = requests.get('https://stackoverflow.com/questions/5652693')\n",
    "soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "text = list(soup1.find_all(\"p\"))\n",
    "sof_text1 = [txt.get_text() for txt in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8872e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = requests.get('https://stackoverflow.com/questions/52832519')\n",
    "soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "text = list(soup1.find_all(\"p\"))\n",
    "sof_text2 = [txt.get_text() for txt in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46743997",
   "metadata": {},
   "source": [
    "#### Getting HWZ article data (originally random, source code in get_urls.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c24aab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = requests.get('https://www.hardwarezone.com.sg/feature-apple-watchos-8-takes-first-serious-step-outdoor-cycling-fitness-workout')\n",
    "soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "text = list(soup1.find_all(\"p\"))\n",
    "hwz_text1 = [txt.get_text() for txt in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8abb417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = requests.get('https://www.hardwarezone.com.sg/feature-apple-iphone-13-iphone-13-pro-2021-review-singapore-price-specs')\n",
    "soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "text = list(soup1.find_all(\"p\"))\n",
    "hwz_text2 = [txt.get_text() for txt in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfddcf",
   "metadata": {},
   "source": [
    "#### Getting CNA article data (originally random, source code in get_urls.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aac93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page1 = requests.get('https://www.channelnewsasia.com/singapore/de-beers-wong-tian-jun-sugar-daddy-appeal-sentence-2199481')\n",
    "soup1 = BeautifulSoup(page1.content, 'html.parser')\n",
    "text = list(soup1.find_all('p'))\n",
    "print(text)\n",
    "cna_text1 = [txt.get_text() for txt in text]\n",
    "cna_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "043144be",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = requests.get('https://www.channelnewsasia.com/singapore/de-beers-wong-tian-jun-sugar-daddy-appeal-sentence-2199481')\n",
    "soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "text = list(soup1.find_all(\"p\"))\n",
    "cna_text2 = [txt.get_text() for txt in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bcdab",
   "metadata": {},
   "source": [
    "##### Cleaning SOF text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a8ff76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_text1 = sof_text1[4:]\n",
    "sof_text1 = sof_text1[:-10]\n",
    "temp_list = []\n",
    "for line in sof_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "sof_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "678f4259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's say I have the XQuery code below:\",\n",
       " 'Can I use a counter, to count how many my code will enter inside the second for loop?',\n",
       " 'I tried this:',\n",
       " 'but I got compile errors.',\n",
       " 'I also I need to sum some constraints like this:',\n",
       " \"but of course this didn't work either :(.\",\n",
       " 'Any suggestions will be highly appreciated.',\n",
       " 'Also, can you suggest a good book/tutorial/site for doing such stuff?',\n",
       " \"You don't need it very often, but if you really do need a counter variable inside an XQuery for expression (analogous to position() in an xsl:for-each) you can use an at variable\",\n",
       " \"I don't think you have understood the basics of declarative paradigm.\",\n",
       " 'Total count and sum would be:',\n",
       " 'Partial count and sum would be:',\n",
       " 'To Display counter in loop the best and easier way is to add at $pos in your for loop.',\n",
       " '$pos will work as a counter.',\n",
       " 'Code Snippet :',\n",
       " 'Output:',\n",
       " \"Here is another solution, if you don't want to count or act on every item of a collection, but you want to count/act only, if a certain condition applies.\",\n",
       " 'In that case you might remember the functional programming paradigm of XQuery and implement this counting/acting by using recursive function calls:',\n",
       " 'The more easier way would be the usage of an appropriate XPath expression:',\n",
       " 'or even',\n",
       " 'The recursive solution might be quite helpful, if you want to implement something more complex with XQuery...']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sof_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43dd083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_text2 = sof_text2[4:]\n",
    "sof_text2 = sof_text2[:-10]\n",
    "temp_list = []\n",
    "for line in sof_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "sof_text2 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cba87cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In Oracle SQL Developer 3.1.07 The two subqueries below run correctly on their own.',\n",
       " \"All I'm trying to do is union them together but Oracle gives an ORA-12704: charcter set mismatch error.\",\n",
       " \"I have checked similar posts to check the UNION syntax and I don't understand where the error is coming from.\",\n",
       " 'The select statements are selecting fields from the same columns in both queries so there should be no data type problems.',\n",
       " 'Help would be greatly appreciated!',\n",
       " 'Ok so credit to my manager Mr K who actually solved this.',\n",
       " 'The problem was with the THIRDPARTY column which is the same data type in both subqueries.',\n",
       " 'In order to solve the problem we used CAST (field AS VARCHAR(3)) in both subqueries.',\n",
       " 'We identified the problem field by stripping all but the first field out of the queries and then adding them back in one by one until it broke.',\n",
       " \"What we still don't understand is why this error occured so any information on this would still be apprieciated.\",\n",
       " 'Here is the resolved code:']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sof_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205317e2",
   "metadata": {},
   "source": [
    "##### Cleaning CNA text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c295fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cna_text1 = cna_text1[2:]\n",
    "cna_text1 = cna_text1[:-7]\n",
    "temp_list = []\n",
    "for line in cna_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "cna_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbb36428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cna_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfa86aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cna_text2 = cna_text2[2:]\n",
    "cna_text2 = cna_text2[:-8]\n",
    "temp_list = []\n",
    "for line in cna_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "cna_text2 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adc08c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cna_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b790273",
   "metadata": {},
   "source": [
    "##### Cleaning HWZ text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7fef1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_text1 = hwz_text1[:-5]\n",
    "temp_list = []\n",
    "for line in hwz_text1:\n",
    "    temp_list += sent_tokenize(line)\n",
    "hwz_text1 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8d2d5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When Apple revealed at its California Streaming event that its watchOS 8 update (available now) would bring with it some major quality of life upgrades for its Outdoor Cycle workout, cyclists like me sat up.',\n",
       " 'Up until now, tracking features on the Apple watches for cycling have been pretty limited - functions like auto-pause and ride detection that most of us cyclists take for granted on our Garmin or Wahoo computers are absent.',\n",
       " 'But watchOS 8 will change all of these and then some.',\n",
       " 'That Apple showed and focused on a mountain biker during the presentation is a watershed moment for cyclists using an Apple Watch.',\n",
       " 'It’s possibly the only major fitness segment that the company hasn’t quite dominate like it did with other mainstream fitness areas like jogging and yoga.',\n",
       " 'With cycling facing its biggest “bike boom” during the pandemic (be prepared to wait for months if you’re looking to get a foldie or a roadbike now), it’s timely for Apple to jazz up the latest watchOS with sensible cycling functions that meet the demands of cyclists.',\n",
       " '“With watchOS 8, we just wanted to do more to improve the experience,” Julz Arney, Director of Apple’s Fitness for Health Technologies told me over a video call.',\n",
       " '“And this year just felt like the right time to pull together features that not just people who might consider themselves as pro cyclists, but also everyone who rides a bike could enjoy.”',\n",
       " 'Essentially, watchOS 8 turns your Apple Watch into a cycling computer of sort with the improved Outdoor Cycling workout.',\n",
       " \"Now it can automatically detect when you begin cycling outside and remind you to start a workout based on advanced algorithms that analyse data from the watch’s sensors – the GPS, heart rate, accelerometer and gyroscope – to understand when you're riding a bike.\",\n",
       " \"To prevent false starts (and Strava cheats), it also recognizes an elevated heart rate due to pedaling, which can help the watch distinguish whether you're truly riding a bike or traveling in a slow-moving car.\",\n",
       " 'Julz also revealed that the watch’s magnetometer, which is basically the compass, helps detect any change in orientation as well.',\n",
       " 'This detects subtle swaying, a natural cycling movement, and helps to confirm that you’re riding a bicycle before sending an outdoor cycling workout reminder.',\n",
       " 'Then there’s self-explanatory auto pause and resume feature – a blessing for OCD cyclists who must constantly pause their workout and watch out for traffic at the same time.',\n",
       " 'It’s not ground-breaking, to be certain but it’s a handy feature for anyone who bikes in dense cities like Singapore.',\n",
       " 'But perhaps the most critical update is the improved fall detection.',\n",
       " 'Apple introduced fall detection three years ago for their watches and it quickly became an impactful feature for many users, especially the elderlies.',\n",
       " 'With the new update, the Apple Watch will now be able to recognise the unique motion and impact of falls from a bike, in a bid to keep users safe when out on the roads.',\n",
       " \"“We realized that falls can be incredibly detrimental not just in everyday activity, but also and maybe especially when you're being active while exercising.\",\n",
       " \"When you're on your bike, whether you take a turn too tightly, hit something on the road, or perhaps forgetting to unclip at a traffic stop, falls from cycling are no small thing,” said Julz.\",\n",
       " \"“So, we've updated fall detection for cycling by optimising and tuning the algorithms to recognize that unique motion and impact of falls from a bike.”\",\n",
       " 'To do this, Apple conducted extensive studies to validate the motion and impact of falls that happened during an outdoor cycling workout.',\n",
       " 'So, just like how fall detection has always worked, a hard fall alert is sound off to a user’s Apple Watch following a significant impact, and you can either dismiss it, or initiate a call to emergency services right from the notification.',\n",
       " \"If Apple Watch detects that you're not moving, and you can't communicate with emergency services, it plays an audio message when the call connects, so you can be sure to get help.\",\n",
       " 'A real lifesaver should the unthinkable happen.',\n",
       " 'The best part of it all is that you don’t even need the announced but yet-to-be-launched Apple Watch Series 7.',\n",
       " 'While watchOS 8 will require an iPhone 6S or later, and running on iOS 14, it will be compatible with all Apple watches, including the Apple Watch 6\\xa0and Apple Watch SE, although fall detection is only supported from Apple Watch Series 4\\xa0onwards.',\n",
       " 'That said, watchOS 8 is still not ready to replace your Garmin or Wahoo cycling computer – yet.',\n",
       " 'Apple Watch cannot measure other key readings, such as cadence and power.',\n",
       " 'Julz explained to me that the company have heard from a lot of customers, mostly customers who consider themselves a\\xa0pro level or pro recreational level that they would love for their additional sensors\\xa0such as a cadence meter or a power meter, or even their head mount, connect directly to Apple Watch.',\n",
       " '“They have asked if, you know, we would open up the Bluetooth profiles in order to do that.',\n",
       " \"We don't have anything to announce around that right now.\",\n",
       " \"But it's exciting that so many cyclists are motivated by their Apple Watch and we hope that this will just be the beginning of what we could do for people who love to ride their bike,” Julz said, as we finished off our video call.\",\n",
       " 'Apple watchOS 8 is available as an update now.',\n",
       " 'Have feedback on the article for the editorial team?',\n",
       " 'You can reach out to them here.',\n",
       " 'No upfront, no interest - check out StarHub’s new EasyGo option!',\n",
       " 'Find out how Aftershock stays ahead of the custom PC curve!']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwz_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a692ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_text2 = hwz_text2[:-5]\n",
    "temp_list = []\n",
    "for line in hwz_text2:\n",
    "    temp_list += sent_tokenize(line)\n",
    "hwz_text2 = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5224d823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Note: This review was first published on 21 September 2021 and it's republished now because the new iPhones are available in retail.\",\n",
       " 'Despite reservations about the name, Apple has stuck with what’s sensible and went with the number 13 for their latest iPhones.',\n",
       " 'You might have already heard, Apple’s newest iPhones are the iPhone 13 and iPhone 13 Mini, and the iPhone 13 Pro and iPhone 13 Pro Max .',\n",
       " 'No surprises, there.',\n",
       " 'However, there are some profound changes compared to last year’s lineup and that could affect your purchasing decision.',\n",
       " 'Sit down, grab a cuppa, this is a long one.',\n",
       " 'Last year’s iPhones\\xa0got\\xa0a major redesign so we knew we weren’t going to be getting phones that look drastically different this year.',\n",
       " 'The basic recipe for this year’s iPhones is nearly identical to last year’s models.',\n",
       " 'In fact, unless you know what to look out for, you’d be hard-pressed to tell the difference between this year’s models and last year’s.',\n",
       " 'They all have flat sides and the iPhone 13 and 13 Mini have aluminium bodies while the iPhone 13 Pro and 13 Pro Max have stainless steel bodies.',\n",
       " 'The sides of the iPhone 13 and 13 Mini are matte, whereas the iPhone 13 Pro and 13 Pro Max have highly polished sides that are fingerprint and dirt magnets.',\n",
       " 'Round the back, the regular iPhones continue to have clear glossy glass backs while the Pro iPhones have matte glass backs.',\n",
       " 'Speaking of glass, the front of the phones are protected by Ceramic Shield, a glass that was developed with Corning to be the strongest smartphone glass ever – again, the same as last year’s models.',\n",
       " 'And since we are on the topic of protection, the IP ratings of the phones are unchanged, so all of this year’s iPhones are rated IP68 which means they can stay submerged in up to six metres of water for up to 30 minutes.',\n",
       " 'Buttons and ports are also all carried over, which means the Lightning port\\xa0is sticking around for another year.',\n",
       " 'Now, I have strong thoughts about the Lightning port.',\n",
       " 'At this point, the Lightning port should make way for USB-C.',\n",
       " 'Previously, a point could be made for compatibility but since all new Macs and nearly all new\\xa0iPads rely on USB-C, it’s time that iPhones do the same.',\n",
       " 'USB-C not only enables faster charging, it also transfers data quicker, and works with a greater variety of accessories.',\n",
       " 'It’s funny because Apple talks about all the benefits of USB-C in its Mac and iPad events and yet puts Lightning ports on their phones.',\n",
       " 'Alright, rant over.',\n",
       " 'Overall dimensions are slightly different.',\n",
       " 'Heights and widths are identical but the new iPhones all gain 0.25mm in thickness.',\n",
       " 'This is to accommodate the larger battery.',\n",
       " 'And because there’s a larger battery inside, the weight has gone up a bit.',\n",
       " 'Is it noticeable?',\n",
       " 'Sure, but only if you hold them next to each other.',\n",
       " 'Readers who have invested in expensive phone cases, however, should note that the camera bump is larger to put up the new sensors and lenses (more on this in the photography section).',\n",
       " 'So yes, if you are upgrading from last year’s phones to this year’s, these differences are enough that you will need new cases for these phones, even if you think they can stretch to accommodate the extra thickness.',\n",
       " 'Another minor difference is the notch on the display.',\n",
       " 'The TrueDepth camera system is unchanged and continues to take great 12-megapixel photos and offer Face ID authentication, but Apple has shrunk the system so the notch is 20% smaller.',\n",
       " 'Honestly, I didn’t really notice the difference until I placed it next to an older iPhone.',\n",
       " 'There are new colours to choose from.',\n",
       " 'The iPhone 13 and 13 Mini are available five colours: PRODUCT(RED), Starlight, Midnight, Blue, and Pink.',\n",
       " 'The units I have are the iPhone 13 in Pink and the iPhone 13 Mini in Midnight.',\n",
       " 'Pink is a very lovely soft shade of pink that I think can be more accurately described as pastel pink.',\n",
       " 'Midnight, on the other hand, can be easily passed off as black.',\n",
       " 'Unfortunately, I couldn’t get to see the entire lineup so I can’t comment on the other colours.',\n",
       " 'The iPhone 13 Pro and 13 Pro Max are available in four colours.',\n",
       " 'Graphite, Gold, and Silver make their return and joining them this year is the new Sierra Blue finish.',\n",
       " 'Like last year’s models, the highly polished finish is achieved using a physical vapour deposition process (PVD).',\n",
       " 'And according to Apple, the Sierra Blue model required an entirely new process that involves placing multiple layers of nanometer-scale metallic ceramics to achieve the colour.',\n",
       " 'As for the Pro iPhones, the units I have are the iPhone 13 Pro in Gold and the iPhone 13 Pro Max in Sierra Blue.',\n",
       " \"This year’s Gold finish is slightly different from last year's.\",\n",
       " \"The polished sides are indistinguishable from one another but the glass back of this year's model is a richer darker shade of gold.\",\n",
       " \"As for Sierra Blue, it appears to be a lighter shade of last year's Pacific Blue.\",\n",
       " 'Overall, fans of last year’s new design will love this year’s phones.',\n",
       " 'But even if you don’t fancy the squared-off edges and flat sides, the outstanding build quality of these phones is undeniable.',\n",
       " 'Yes, these phones are expensive but they most definitely look and feel every bit as much as they cost.',\n",
       " 'Have feedback on the article for the editorial team?',\n",
       " 'You can reach out to them here.',\n",
       " 'No upfront, no interest - check out StarHub’s new EasyGo option!',\n",
       " 'Find out how Aftershock stays ahead of the custom PC curve!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwz_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eea11d",
   "metadata": {},
   "source": [
    "##### First word in sentence capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "11d465b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_word_cap(text):\n",
    "    count=0\n",
    "    uppercount=0\n",
    "    for sent in text:\n",
    "        if sent[0].isupper():\n",
    "            uppercount+=1\n",
    "        count+=1\n",
    "    return uppercount/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "492154e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of first letter being capitalised for sof_text1:  0.8095238095238095\n",
      "Fraction of first letter being capitalised for sof_text1:  1.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16468/1995656186.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fraction of first letter being capitalised for sof_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_word_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msof_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fraction of first letter being capitalised for sof_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_word_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msof_text2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fraction of first letter being capitalised for sof_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_word_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwz_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fraction of first letter being capitalised for sof_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_word_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwz_text2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fraction of first letter being capitalised for sof_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_word_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16468/1403663621.py\u001b[0m in \u001b[0;36mfirst_word_cap\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0muppercount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0muppercount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of first letter being capitalised for sof_text1: \", first_word_cap(sof_text1))\n",
    "print(\"Fraction of first letter being capitalised for sof_text1: \", first_word_cap(sof_text2))\n",
    "print(\"Fraction of first letter being capitalised for hwz_text1: \", first_word_cap(hwz_text1))\n",
    "print(\"Fraction of first letter being capitalised for hwz_text2: \", first_word_cap(hwz_text2))\n",
    "print(\"Fraction of first letter being capitalised for cna_text1: \", first_word_cap(cna_text1))\n",
    "print(\"Fraction of first letter being capitalised for cna_text2: \", first_word_cap(cna_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6f9d3",
   "metadata": {},
   "source": [
    "##### Length of articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f2e3133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentences in sof_text1:  15\n",
      "No of sentences in sof_text2:  9\n",
      "No of sentences in hwz_text1:  37\n",
      "No of sentences in hwz_text2:  52\n",
      "No of sentences in cna_text1:  0\n",
      "No of sentences in cna_text2:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"No of sentences in sof_text1: \", len(sof_text1))\n",
    "print(\"No of sentences in sof_text2: \", len(sof_text2))\n",
    "print(\"No of sentences in hwz_text1: \", len(hwz_text1))\n",
    "print(\"No of sentences in hwz_text2: \", len(hwz_text2))\n",
    "print(\"No of sentences in cna_text1: \", len(cna_text1))\n",
    "print(\"No of sentences in cna_text2: \", len(cna_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e1f48",
   "metadata": {},
   "source": [
    "##### Proper nouns capitalised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3b79d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_nouns_cap(text):\n",
    "    tagged = []\n",
    "    uppercount = 0\n",
    "    count = 0\n",
    "    for sentence in text:\n",
    "        tagged.append(nlp(sentence))\n",
    "    for tag in tagged:\n",
    "        for token in tag:\n",
    "            if token.pos_ == 'PROPN':\n",
    "                if token.text[0].isupper():\n",
    "                    uppercount += 1\n",
    "                count += 1\n",
    "    return uppercount/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6c6d08b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of proper nouns capitalised in sof_text1:  0.8888888888888888\n",
      "Fraction of proper nouns capitalised in sof_text1:  0.8571428571428571\n",
      "Fraction of proper nouns capitalised in sof_text1:  0.8513513513513513\n",
      "Fraction of proper nouns capitalised in sof_text1:  0.7191011235955056\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16468/2290732189.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fraction of proper nouns capitalised in sof_text1: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop_nouns_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwz_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fraction of proper nouns capitalised in sof_text1: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop_nouns_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwz_text2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fraction of proper nouns capitalised in sof_text1: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop_nouns_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fraction of proper nouns capitalised in sof_text1: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop_nouns_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16468/1805096903.py\u001b[0m in \u001b[0;36mprop_nouns_cap\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0muppercount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0muppercount\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Fraction of proper nouns capitalised in sof_text1: ', prop_nouns_cap(sof_text1))\n",
    "print('Fraction of proper nouns capitalised in sof_text2: ', prop_nouns_cap(sof_text2))\n",
    "print('Fraction of proper nouns capitalised in hwz_text1: ', prop_nouns_cap(hwz_text1))\n",
    "print('Fraction of proper nouns capitalised in hwz_text2: ', prop_nouns_cap(hwz_text2))\n",
    "print('Fraction of proper nouns capitalised in cna_text1: ', prop_nouns_cap(cna_text1))\n",
    "print('Fraction of proper nouns capitalised in cna_text2: ', prop_nouns_cap(cna_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b6570",
   "metadata": {},
   "source": [
    "###### What kind of proper nouns used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116dd58f",
   "metadata": {},
   "source": [
    "1. Stack Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ff8d612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQuery 4\n",
      "counter 4\n",
      "code 3\n",
      "loop 3\n",
      "subqueries 3\n",
      "error 3\n",
      "problem 3\n",
      "field 3\n",
      "variable 2\n",
      "expression 2\n",
      "paradigm 2\n",
      "count 2\n",
      "sum 2\n",
      "way 2\n",
      "pos 2\n",
      "solution 2\n",
      "Oracle 2\n",
      "queries 2\n",
      "data 2\n",
      "type 2\n",
      "compile 1\n",
      "errors 1\n",
      "constraints 1\n",
      "course 1\n",
      "suggestions 1\n",
      "book 1\n",
      "tutorial 1\n",
      "site 1\n",
      "stuff 1\n",
      "position 1\n",
      "xsl 1\n",
      "basics 1\n",
      "Display 1\n",
      "Code 1\n",
      "Snippet 1\n",
      "Output 1\n",
      "item 1\n",
      "collection 1\n",
      "condition 1\n",
      "case 1\n",
      "programming 1\n",
      "counting 1\n",
      "function 1\n",
      "calls 1\n",
      "usage 1\n",
      "XPath 1\n",
      "SQL 1\n",
      "Developer 1\n",
      "3.1.07 1\n",
      "run 1\n",
      "charcter 1\n",
      "set 1\n",
      "mismatch 1\n",
      "posts 1\n",
      "UNION 1\n",
      "syntax 1\n",
      "statements 1\n",
      "fields 1\n",
      "columns 1\n",
      "problems 1\n",
      "Help 1\n",
      "credit 1\n",
      "manager 1\n",
      "Mr 1\n",
      "K 1\n",
      "THIRDPARTY 1\n",
      "column 1\n",
      "order 1\n",
      "CAST 1\n",
      "information 1\n"
     ]
    }
   ],
   "source": [
    "sof_tagged = []\n",
    "sof_noun_dict = {}\n",
    "for sentence in sof_text1:\n",
    "    sof_tagged.append(nlp(sentence))\n",
    "for sentence in sof_text2:\n",
    "    sof_tagged.append(nlp(sentence))\n",
    "for tagged in sof_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in sof_noun_dict.keys():\n",
    "                sof_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                sof_noun_dict[token.text] = 1\n",
    "\n",
    "sof_noun_dict_sorted = sorted(sof_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sof_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8c239877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 26\n",
      "year 21\n",
      "iPhone 16\n",
      "Watch 12\n",
      "Pro 12\n",
      "iPhones 11\n",
      "cycling 10\n",
      "phones 9\n",
      "watchOS 8\n",
      "bike 8\n",
      "cyclists 7\n",
      "workout 6\n",
      "detection 6\n",
      "’s 6\n",
      "fall 6\n",
      "glass 6\n",
      "Blue 6\n",
      "Julz 5\n",
      "Mini 5\n",
      "Max 5\n",
      "sides 5\n",
      "update 4\n",
      "call 4\n",
      "impact 4\n",
      "falls 4\n",
      "models 4\n",
      "Lightning 4\n",
      "USB 4\n",
      "colours 4\n",
      "Sierra 4\n",
      "time 3\n",
      "watch 3\n",
      "sensors 3\n",
      "feature 3\n",
      "motion 3\n",
      "difference 3\n",
      "port 3\n",
      "C 3\n",
      "Midnight 3\n",
      "Pink 3\n",
      "shade 3\n",
      "Gold 3\n",
      "finish 3\n",
      "quality 2\n",
      "Outdoor 2\n",
      "features 2\n",
      "functions 2\n",
      "auto 2\n",
      "pause 2\n",
      "Garmin 2\n",
      "Wahoo 2\n",
      "fitness 2\n",
      "company 2\n",
      "video 2\n",
      "people 2\n",
      "computer 2\n",
      "algorithms 2\n",
      "data 2\n",
      "heart 2\n",
      "rate 2\n",
      "traffic 2\n",
      "watches 2\n",
      "users 2\n",
      "emergency 2\n",
      "services 2\n",
      "Series 2\n",
      "cadence 2\n",
      "power 2\n",
      "customers 2\n",
      "level 2\n",
      "meter 2\n",
      "feedback 2\n",
      "article 2\n",
      "team 2\n",
      "Aftershock 2\n",
      "custom 2\n",
      "PC 2\n",
      "curve 2\n",
      "Topics 2\n",
      "Sections 2\n",
      "AWARDS 2\n",
      "ACCOLADES 2\n",
      "supporters 2\n",
      "media 2\n",
      "awards 2\n",
      "industry 2\n",
      "lineup 2\n",
      "bodies 2\n",
      "matte 2\n",
      "backs 2\n",
      "ports 2\n",
      "point 2\n",
      "thickness 2\n",
      "battery 2\n",
      "bit 2\n",
      "cases 2\n",
      "camera 2\n",
      "notch 2\n",
      "system 2\n",
      "units 2\n",
      "pink 2\n",
      "process 2\n",
      "model 2\n",
      "California 1\n",
      "Streaming 1\n",
      "event 1\n",
      "life 1\n",
      "upgrades 1\n",
      "Cycle 1\n",
      "computers 1\n",
      "mountain 1\n",
      "biker 1\n",
      "presentation 1\n",
      "moment 1\n",
      "segment 1\n",
      "areas 1\n",
      "jogging 1\n",
      "yoga 1\n",
      "boom 1\n",
      "pandemic 1\n",
      "months 1\n",
      "foldie 1\n",
      "roadbike 1\n",
      "demands 1\n",
      "experience 1\n",
      "Arney 1\n",
      "Director 1\n",
      "Fitness 1\n",
      "Health 1\n",
      "Technologies 1\n",
      "sort 1\n",
      "Cycling 1\n",
      "GPS 1\n",
      "accelerometer 1\n",
      "gyroscope 1\n",
      "starts 1\n",
      "Strava 1\n",
      "cheats 1\n",
      "pedaling 1\n",
      "car 1\n",
      "magnetometer 1\n",
      "compass 1\n",
      "change 1\n",
      "orientation 1\n",
      "swaying 1\n",
      "movement 1\n",
      "bicycle 1\n",
      "reminder 1\n",
      "self 1\n",
      "blessing 1\n",
      "ground 1\n",
      "breaking 1\n",
      "cities 1\n",
      "Singapore 1\n",
      "years 1\n",
      "elderlies 1\n",
      "bid 1\n",
      "roads 1\n",
      "activity 1\n",
      "turn 1\n",
      "road 1\n",
      "stop 1\n",
      "thing 1\n",
      "studies 1\n",
      "alert 1\n",
      "user 1\n",
      "notification 1\n",
      "message 1\n",
      "help 1\n",
      "lifesaver 1\n",
      "part 1\n",
      "6S 1\n",
      "iOS 1\n",
      "SE 1\n",
      "readings 1\n",
      "lot 1\n",
      "head 1\n",
      "mount 1\n",
      "Bluetooth 1\n",
      "profiles 1\n",
      "order 1\n",
      "beginning 1\n",
      "review 1\n",
      "September 1\n",
      "retail 1\n",
      "reservations 1\n",
      "name 1\n",
      "number 1\n",
      "surprises 1\n",
      "changes 1\n",
      "purchasing 1\n",
      "decision 1\n",
      "cuppa 1\n",
      "redesign 1\n",
      "recipe 1\n",
      "fact 1\n",
      "aluminium 1\n",
      "steel 1\n",
      "fingerprint 1\n",
      "dirt 1\n",
      "magnets 1\n",
      "back 1\n",
      "front 1\n",
      "Ceramic 1\n",
      "Shield 1\n",
      "Corning 1\n",
      "topic 1\n",
      "protection 1\n",
      "IP 1\n",
      "ratings 1\n",
      "IP68 1\n",
      "metres 1\n",
      "water 1\n",
      "minutes 1\n",
      "Buttons 1\n",
      "thoughts 1\n",
      "way 1\n",
      "C. 1\n",
      "Previously 1\n",
      "compatibility 1\n",
      "Macs 1\n",
      "iPads 1\n",
      "variety 1\n",
      "accessories 1\n",
      "talks 1\n",
      "benefits 1\n",
      "Mac 1\n",
      "iPad 1\n",
      "events 1\n",
      "dimensions 1\n",
      "Heights 1\n",
      "widths 1\n",
      "mm 1\n",
      "weight 1\n",
      "Readers 1\n",
      "phone 1\n",
      "bump 1\n",
      "lenses 1\n",
      "photography 1\n",
      "section 1\n",
      "differences 1\n",
      "display 1\n",
      "TrueDepth 1\n",
      "megapixel 1\n",
      "photos 1\n",
      "Face 1\n",
      "ID 1\n",
      "authentication 1\n",
      "% 1\n",
      "Starlight 1\n",
      "hand 1\n",
      "Graphite 1\n",
      "Silver 1\n",
      "return 1\n",
      "vapour 1\n",
      "deposition 1\n",
      "PVD 1\n",
      "layers 1\n",
      "nanometer 1\n",
      "scale 1\n",
      "ceramics 1\n",
      "colour 1\n",
      "gold 1\n",
      "Pacific 1\n",
      "fans 1\n",
      "design 1\n",
      "edges 1\n",
      "build 1\n"
     ]
    }
   ],
   "source": [
    "hwz_tagged = []\n",
    "hwz_noun_dict = {}\n",
    "for sentence in hwz_text1:\n",
    "    hwz_tagged.append(nlp(sentence))\n",
    "for sentence in hwz_text2:\n",
    "    hwz_tagged.append(nlp(sentence))\n",
    "for tagged in hwz_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in hwz_noun_dict.keys():\n",
    "                hwz_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                hwz_noun_dict[token.text] = 1\n",
    "\n",
    "hwz_noun_dict_sorted = sorted(hwz_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in hwz_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a0839846",
   "metadata": {},
   "outputs": [],
   "source": [
    "cna_tagged = []\n",
    "cna_noun_dict = {}\n",
    "for sentence in cna_text1:\n",
    "    cna_tagged.append(nlp(sentence))\n",
    "for sentence in cna_text2:\n",
    "    cna_tagged.append(nlp(sentence))\n",
    "for tagged in cna_tagged:\n",
    "    for token in tagged:\n",
    "        if token.pos_ in (\"PROPN\", \"NOUN\"):\n",
    "            if token.text in cna_noun_dict.keys():\n",
    "                cna_noun_dict[token.text] += 1\n",
    "            else:\n",
    "                cna_noun_dict[token.text] = 1\n",
    "\n",
    "cna_noun_dict_sorted = sorted(cna_noun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in cna_noun_dict_sorted:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bd70c1",
   "metadata": {},
   "source": [
    "##### Good grammar?\n",
    "1. Subject-verb agreement\n",
    "2. Tense matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c1755",
   "metadata": {},
   "source": [
    "#### Subject-verb agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "51e26177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "def is_passive(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dict1 = {'DEP': 'nsubjpass'}\n",
    "    dict2 = {'DEP': 'aux', 'OP': '*'}\n",
    "    dict3 = {'DEP': 'auxpass'}\n",
    "    dict4 = {'TAG': 'VBN'}\n",
    "    passive_rule = [dict1, dict2, dict3, dict4]\n",
    "    matcher.add(\"Passive\", [passive_rule])\n",
    "    matches = matcher(doc)\n",
    "    if matches:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b7b94",
   "metadata": {},
   "source": [
    "https://github.com/armsp/active_or_passive/blob/master/spacy_voices.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f927c",
   "metadata": {},
   "source": [
    "### Short break to count the % of passive sentences in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cbd79fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def passive_sents(text):\n",
    "    passivecounts = 0\n",
    "    for sent in text:\n",
    "        if is_passive(sent):\n",
    "            passivecounts += 1\n",
    "    return passivecounts/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bff1f7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of passive sentences in sof_text1:  0.0\n",
      "% of passive sentences in sof_text2:  0.0\n",
      "% of passive sentences in hwz_text1:  0.041666666666666664\n",
      "% of passive sentences in hwz_text2:  0.125\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16468/3890446297.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"% of passive sentences in hwz_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwz_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"% of passive sentences in hwz_text2: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwz_text2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"% of passive sentences in cna_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"% of passive sentences in cna_text2: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16468/361697252.py\u001b[0m in \u001b[0;36mpassive_sents\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_passive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mpassivecounts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpassivecounts\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print(\"% of passive sentences in sof_text1: \", passive_sents(sof_text1))\n",
    "print(\"% of passive sentences in sof_text2: \", passive_sents(sof_text2))\n",
    "print(\"% of passive sentences in hwz_text1: \", passive_sents(hwz_text1))\n",
    "print(\"% of passive sentences in hwz_text2: \", passive_sents(hwz_text2))\n",
    "print(\"% of passive sentences in cna_text1: \", passive_sents(cna_text1))\n",
    "print(\"% of passive sentences in cna_text2: \", passive_sents(cna_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5792fa",
   "metadata": {},
   "source": [
    "# Ok back to business"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d00df47a",
   "metadata": {},
   "source": [
    "for sent in sof_text1:\n",
    "    if not is_passive(sent):\n",
    "        sent_tagged = nlp(sent)\n",
    "        for word, tag in sent_tagged:\n",
    "            if tag == 'NNS' or tag == 'AUX':\n",
    "                \n",
    "        \n",
    "for sent in sof_text2:\n",
    "    if is_passive(sent):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f9be7",
   "metadata": {},
   "source": [
    "# I'm gonna see the percentage of short sentences first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008daebf",
   "metadata": {},
   "source": [
    "## 1. Avg length of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e9f2c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sent_len(text):\n",
    "    total_words = 0\n",
    "    for sent in text:\n",
    "        total_words += len(sent)\n",
    "    return total_words/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f11ad3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of sentences in sof_text1:  61.714285714285715\n",
      "Average length of sentences in sof_text2:  87.54545454545455\n",
      "Average length of sentences in hwz_text1:  233.45833333333334\n",
      "Average length of sentences in hwz_text2:  168.625\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16468/4107319672.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average length of sentences in hwz_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_sent_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwz_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average length of sentences in hwz_text2: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_sent_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwz_text2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average length of sentences in cna_text1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_sent_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average length of sentences in cna_text2: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_sent_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcna_text2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16468/802424031.py\u001b[0m in \u001b[0;36mavg_sent_len\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtotal_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print(\"Average length of sentences in sof_text1: \", avg_sent_len(sof_text1))\n",
    "print(\"Average length of sentences in sof_text2: \", avg_sent_len(sof_text2))\n",
    "print(\"Average length of sentences in hwz_text1: \", avg_sent_len(hwz_text1))\n",
    "print(\"Average length of sentences in hwz_text2: \", avg_sent_len(hwz_text2))\n",
    "print(\"Average length of sentences in cna_text1: \", avg_sent_len(cna_text1))\n",
    "print(\"Average length of sentences in cna_text2: \", avg_sent_len(cna_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f11c99",
   "metadata": {},
   "source": [
    "2. Short sentences in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7b2cebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_short_sents(text):\n",
    "    count = 0\n",
    "    for sent in text:\n",
    "        if len(sent) < 20:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "91ec8430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of short sentences in sof_text1:  4 out of 21 sentences\n",
      "Count of short sentences in sof_text2:  0 out of 11 sentences\n",
      "Count of short sentences in hwz_text1:  7 out of 24 sentences\n",
      "Count of short sentences in hwz_text2:  17 out of 32 sentences\n",
      "Count of short sentences in cna_text1:  0 out of 0 sentences\n",
      "Count of short sentences in cna_text2:  0 out of 0 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of short sentences in sof_text1: \", num_short_sents(sof_text1), \"out of\", len(sof_text1), \"sentences\")\n",
    "print(\"Count of short sentences in sof_text2: \", num_short_sents(sof_text2), \"out of\", len(sof_text2), \"sentences\")\n",
    "print(\"Count of short sentences in hwz_text1: \", num_short_sents(hwz_text1), \"out of\", len(hwz_text1), \"sentences\")\n",
    "print(\"Count of short sentences in hwz_text2: \", num_short_sents(hwz_text2), \"out of\", len(hwz_text2), \"sentences\")\n",
    "print(\"Count of short sentences in cna_text1: \", num_short_sents(cna_text1), \"out of\", len(cna_text1), \"sentences\")\n",
    "print(\"Count of short sentences in cna_text2: \", num_short_sents(cna_text2), \"out of\", len(cna_text2), \"sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111fe278",
   "metadata": {},
   "source": [
    "#### Trying this package (language_tool_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d8ff583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-BR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "881064ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Match({'ruleId': 'UPPERCASE_SENTENCE_START', 'message': 'This sentence does not start with an uppercase letter.', 'replacements': ['But'], 'offsetInContext': 0, 'context': 'but not exactly the same, because i dont wa...', 'offset': 0, 'errorLength': 3, 'category': 'CASING', 'ruleIssueType': 'typographical', 'sentence': 'but not exactly the same, because i dont want to add description, i want to add \"alt\" attribute to img.'}), Match({'ruleId': 'EN_CONTRACTION_SPELLING', 'message': 'Possible spelling mistake found', 'replacements': [\"don't\"], 'offsetInContext': 36, 'context': 'but not exactly the same, because i dont want to add description, i want to add ...', 'offset': 36, 'errorLength': 4, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'but not exactly the same, because i dont want to add description, i want to add \"alt\" attribute to img.'}), Match({'ruleId': 'I_LOWERCASE', 'message': 'The personal pronoun “I” should be uppercase.', 'replacements': ['I'], 'offsetInContext': 43, 'context': '...because i dont want to add description, i want to add \"alt\" attribute to img.', 'offset': 66, 'errorLength': 1, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'but not exactly the same, because i dont want to add description, i want to add \"alt\" attribute to img.'})]\n"
     ]
    }
   ],
   "source": [
    "matches_sof = []\n",
    "for sent in sof_text1:\n",
    "    matches_sof += tool.check(sent)\n",
    "print(matches_sof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bb395f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Match({'ruleId': 'DASH_RULE', 'message': 'Consider using an m-dash if you do not want to join two words.', 'replacements': ['—'], 'offsetInContext': 43, 'context': '...es for cycling have been pretty limited - functions like auto-pause and ride dete...', 'offset': 90, 'errorLength': 1, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': 'Up until now, tracking features on the Apple watches for cycling have been pretty limited - functions like auto-pause and ride detection that most of us cyclists take for granted on our Garmin or Wahoo computers are absent.'}), Match({'ruleId': 'DASH_RULE', 'message': 'Consider using an m-dash if you do not want to join two words.', 'replacements': ['—'], 'offsetInContext': 43, 'context': '...t analyse data from the watch’s sensors – the GPS, heart rate, accelerometer and ...', 'offset': 169, 'errorLength': 1, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': \"Now it can automatically detect when you begin cycling outside and remind you to start a workout based on advanced algorithms that analyse data from the watch's sensors – the GPS, heart rate, accelerometer and gyroscope – to understand when you're riding a bike.\"}), Match({'ruleId': 'DASH_RULE', 'message': 'Consider using an m-dash if you do not want to join two words.', 'replacements': ['—'], 'offsetInContext': 43, 'context': \"...heart rate, accelerometer and gyroscope – to understand when you're riding a bike...\", 'offset': 220, 'errorLength': 1, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': \"Now it can automatically detect when you begin cycling outside and remind you to start a workout based on advanced algorithms that analyse data from the watch's sensors – the GPS, heart rate, accelerometer and gyroscope – to understand when you're riding a bike.\"}), Match({'ruleId': 'DASH_RULE', 'message': 'Consider using an m-dash if you do not want to join two words.', 'replacements': ['—'], 'offsetInContext': 43, 'context': '...planatory auto pause and resume feature – a blessing for OCD cyclists who must co...', 'offset': 60, 'errorLength': 1, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': \"Then there's self-explanatory auto pause and resume feature – a blessing for OCD cyclists who must constantly pause their workout and watch out for traffic at the same time.\"}), Match({'ruleId': 'COMMA_COMPOUND_SENTENCE', 'message': 'Use a comma before ‘but’ if it connects two independent clauses (unless they are closely connected and short).', 'replacements': [', but'], 'offsetInContext': 39, 'context': 'It’s not ground-breaking, to be certain but it’s a handy feature for anyone who bik...', 'offset': 39, 'errorLength': 4, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': \"It's not ground-breaking, to be certain but it's a handy feature for anyone who bikes in dense cities like Singapore.\"}), Match({'ruleId': 'COMMA_COMPOUND_SENTENCE', 'message': 'Use a comma before ‘and’ if it connects two independent clauses (unless they are closely connected and short).', 'replacements': [', and'], 'offsetInContext': 43, 'context': '...ection three years ago for their watches and it quickly became an impactful feature ...', 'offset': 65, 'errorLength': 4, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': 'Apple introduced fall detection three years ago for their watches and it quickly became an impactful feature for many users, especially the elderlies.'}), Match({'ruleId': 'DASH_RULE', 'message': 'Consider using an m-dash if you do not want to join two words.', 'replacements': ['—'], 'offsetInContext': 43, 'context': '...e your Garmin or Wahoo cycling computer – yet.', 'offset': 89, 'errorLength': 1, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': 'That said, watchOS 8 is still not ready to replace your Garmin or Wahoo cycling computer – yet.'}), Match({'ruleId': 'COMMA_COMPOUND_SENTENCE', 'message': 'Use a comma before ‘and’ if it connects two independent clauses (unless they are closely connected and short).', 'replacements': [', and'], 'offsetInContext': 43, 'context': '...lists are motivated by their Apple Watch and we hope that this will just be the begi...', 'offset': 74, 'errorLength': 4, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': \"But it's exciting that so many cyclists are motivated by their Apple Watch and we hope that this will just be the beginning of what we could do for people who love to ride their bike,” Julz said, as we finished off our video call.\"})]\n"
     ]
    }
   ],
   "source": [
    "matches_hwz = []\n",
    "for sent in hwz_text1:\n",
    "    matches_hwz += tool.check(sent)\n",
    "print(matches_hwz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_cna = []\n",
    "for sent in cna_text1:\n",
    "    matches_cna += tool.check(sent)\n",
    "print(matches_cna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c19991",
   "metadata": {},
   "source": [
    "# Formality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e3eca",
   "metadata": {},
   "source": [
    "#### No of second-person pronoun 'you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "349b16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_sec_pronoun(text):\n",
    "    count = 0 \n",
    "    for sent in text:\n",
    "        if 'you' in sent:\n",
    "            count += 1 \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5e9d63dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of second person pronoun in sof_text1:  7\n",
      "Count of second person pronoun in sof_text2:  0\n",
      "Count of second person pronoun in hwz_text1:  8\n",
      "Count of second person pronoun in hwz_text2:  5\n",
      "Count of second person pronoun in cna_text1:  0\n",
      "Count of second person pronoun in cna_text2:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of second person pronoun in sof_text1: \", num_sec_pronoun(sof_text1))\n",
    "print(\"Count of second person pronoun in sof_text2: \", num_sec_pronoun(sof_text2))\n",
    "print(\"Count of second person pronoun in hwz_text1: \", num_sec_pronoun(hwz_text1))\n",
    "print(\"Count of second person pronoun in hwz_text2: \", num_sec_pronoun(hwz_text2))\n",
    "print(\"Count of second person pronoun in cna_text1: \", num_sec_pronoun(cna_text1))\n",
    "print(\"Count of second person pronoun in cna_text2: \", num_sec_pronoun(cna_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bf440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbf0c018",
   "metadata": {},
   "source": [
    "# So i learnt that this way is not good. i will attempt to try with spacy dependency tracker and then combine with nltk pos tagger to check if there is SVA. thx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa41b27",
   "metadata": {},
   "source": [
    "# this is what i will do\n",
    "# 3. continue with the two parts for checking for good grammar (any more ideas?)\n",
    "# 4. think n implement more things to analyse for writing style\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad0f1f",
   "metadata": {},
   "source": [
    "Findings:\n",
    "1. Length of articles\n",
    "    - SOF consistently the shortest, HWZ and CNA are higher by a big margin (+30 sentences on avg)\n",
    "2. Proper nouns\n",
    "    - SOF has the lowest counts of capitalisation for proper nouns (expected)\n",
    "3. Kind of proper nouns used \n",
    "    - SOF always uses language from programming/coding domain\n",
    "    - HWZ always writes about either telco/pop culture (movies?)\n",
    "    - CNA \n",
    "4. Length of sentences (CHECK THIS!!)\n",
    "    - SOF always has the shortest sentences\n",
    "    - HWZ and CNA has much longer sentences (approx 100 words)\n",
    "5. Counts of extremely short sentences\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88d3e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get counts of extremely short sentences\n",
    "THRESHOLD = 6\n",
    "extremely_short = [sentences for sentences in hwz_text1 if len(word_tokenize(sentences)) < THRESHOLD]\n",
    "\n",
    "len(extremely_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0a4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3994804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93b5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91e2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba8422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb6575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f3b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a16a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c4271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2c17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa663e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0d15f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
