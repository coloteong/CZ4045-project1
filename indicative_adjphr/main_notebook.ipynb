{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import requests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reviews = pd.read_json('reviewSelected100.json', encoding='ISO-8859-1', lines=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.3 Extraction of Indicative Adjective Phrases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_adjective_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    phrases = []\n",
    "    for token in doc:\n",
    "        phrase = ''\n",
    "        if (token.pos_ == 'ADJ') and (token.dep_ in ['dobj','pobj','nsubj','nsubjpass']):\n",
    "            for subtoken in token.children:\n",
    "                if (subtoken.pos_ == 'ADJ') or (subtoken.pos_ == 'ADV') or (subtoken.dep_ == 'compound'):\n",
    "                    phrase += subtoken.text + ' '\n",
    "            if len(phrase)!=0:\n",
    "                phrase += token.text\n",
    "        if  len(phrase)!=0:\n",
    "            phrases.append(phrase)\n",
    "    return phrases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adjective_phrases = get_adjective_phrases(clean_review)\n",
    "adjective_phrases[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "indicative_phrases = Counter(adjective_phrases).most_common(10)\n",
    "indicative_phrases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Looking at the Data\n",
    "\n",
    "I think instead of looking at the data and just blindly applying some ML techniques. The first thing that we can do is to consider three things namely:\n",
    "\n",
    "- Length of the review\n",
    "- Grammar\n",
    "- References, particularly inter and intra sentences\n",
    "\n",
    "## How to do?\n",
    "\n",
    "Firstly, we need to come up with a pipeline in which i think we should do the following:\n",
    "\n",
    "1. Cleaning of data\n",
    "2. Sentence segmentation\n",
    "3. Fixing the references.. hmm we can do either de-referencing or we can do some coreferencing based on the dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pair Ranking\n",
    "\n",
    "In order to do the pair ranking, we can look the top most frequent pairs -> potential downside is that useless adjective phrases will be generated which does not actually tell us anything about the shop\n",
    "\n",
    "TF-IDF: this is commonly used for text summarization but we can use it to rank too. IDF is how common a particular phrase is in the entire corpus. Thus, a high TF-IDF would mean that this is particular to this shop\n",
    "\n",
    "LDA: which is used for topic modelling, it can generate topics -> may be tried to see if an adjective phrase is particular\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Clustering may be an interesting thing that we can try, especially since I doubt the things we see above will give us good results.\n",
    "\n",
    "To do clustering, we need a continuous representation of the phrase. This can be done by generating word embeddings (word2vec, GloVe, BERT) or any other methods\n",
    "\n",
    "We can then do K-Means clustering (benchmark comparison to the discrete methods above)\n",
    "\n",
    "We can also do other clustering algos e.g. Hierarchical\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}